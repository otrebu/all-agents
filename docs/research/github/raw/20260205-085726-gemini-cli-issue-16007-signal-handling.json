{
  "files": [
    {
      "content": "Prefect of the Dicastery for Bishops, Former Prior General of the Augustinians\n69\nAppointed by Current Pope, International Experience Might Mitigate US Factor\nPierbattista Pizzaballa\nSoft Conservative\nItalian, Significant Presence in the Middle East\nLatin Patriarch of Jerusalem, Former Custos of the Holy Land\n60\nRespected, Bridge-Builder, Neutral Stance\n\n2. EVALUATION: Assessing Cardinal Suitability for Current Church Challenges\nBeyond the realm of electoral politics, the next Pope will face a Catholic Church navigating a complex landscape of demographic shifts, governance reforms, doctrinal questions, and the imperative of evangelization in the modern world. Evaluating which of the aforementioned cardinals would be most effective in addressing these specific challenges, independent of their electoral prospects, provides a crucial perspective on the future direction of the Church.\n\nDemographic Shifts:\nThe Catholic Church is currently experiencing significant demographic shifts, with a notable increase in the Catholic population in Africa and Asia, while Europe and the Americas face stagnation or decline.121 This reality presents both opportunities for growth and challenges in terms of resource allocation, pastoral care, and ensuring the Church's leadership reflects its increasingly global composition. The future vitality of the Catholic Church is intrinsically linked to its ability to effectively engage with and support the burgeoning Catholic communities in the Global South.\nIn this context, Cardinal Luis Antonio Tagle, as a Filipino cardinal, possesses a unique advantage in understanding and connecting with the Asian context, where Catholicism is experiencing dynamic growth.1 His current role as Pro-Prefect of the Dicastery for Evangelisation further underscores his focus on these regions and their specific needs.4 A pontificate under Cardinal Tagle could signal a significant embrace of the Church's growing presence in Asia.\nCardinal Robert Francis Prevost's extensive missionary work and leadership in Peru and Latin America also provide him with valuable firsthand experience in another region with a substantial and growing Catholic population.4 His deep engagement with the Catholic communities in Latin America equips him with insights into their specific challenges and opportunities.\nWhile the growth is concentrated in the Global South, the challenges of maintaining faith in increasingly secularized societies in Europe and the Americas remain significant. Cardinal Péter Erdő's leadership within the Council of European Episcopal Conferences has exposed him to the complexities of secularization and the need for renewed evangelization efforts in Europe.4 His experience could be beneficial in addressing these specific regional challenges.\nThe possibility of the first African or Asian Pope (beyond Cardinal Tagle, figures like Cardinal Turkson have been mentioned 1) underscores the potential for a historic shift in the papacy, reflecting the changing demographics of the global Catholic community. Such an election would carry immense symbolic weight, signaling the Church's universality and its commitment to its growing communities in the Global South.\n\nGovernance Reform:\nThe Catholic Church has been engaged in ongoing efforts to reform the Roman Curia, aiming for greater transparency, accountability, and efficiency, particularly in areas concerning finance and the handling of abuse cases.130 The next Pope will need to continue these reforms and foster a culture of good governance within the Vatican.\nCardinal Pietro Parolin's extensive tenure as Secretary of State provides him with an intimate understanding of the Curia and its internal workings.4 Having been part of Pope Francis' Council of Cardinals advising on Church reform, he possesses firsthand knowledge of the reforms already implemented and the challenges that remain.14 His experience could be instrumental in navigating the complexities of further Curial reform, though his approach might be characterized by a degree of caution given his long-standing role within the existing system.\nCardinal Robert Francis Prevost, as the current Prefect of the Dicastery for Bishops, holds a significant administrative position within the Vatican.4 His leadership of this key dicastery, along with his previous experience as the head of a religious order, demonstrates his administrative capabilities and his potential to lead further governance reforms within the Church.\nCardinal Péter Erdő's experience as the Archbishop of a major archdiocese and his leadership of the Council of European Episcopal Conferences showcase his capacity for governance at different levels of the Church.4 This broad experience could be valuable in implementing and overseeing governance reforms at both the Vatican and local levels.\n\nDoctrinal Questions:\nThe Catholic Church in the 21st century grapples with a range of complex doctrinal questions, including the interpretation and implementation of Vatican II, the evolving roles of women in the Church, the inclusion of LGBTQ+ Catholics, and ethical considerations arising from advancements in science and technology.6 The next Pope will need to provide theological leadership and pastoral guidance in navigating these often sensitive and divisive issues.\nCardinal Erdő, representing a more conservative theological perspective, would likely prioritize upholding traditional doctrines and liturgical practices.5 His firm stance on issues such as communion for divorced and remarried Catholics reflects this commitment to traditional teachings.5\nCardinal Tagle, with his liberal leanings, would likely approach these issues with a greater emphasis on compassion and inclusion.1 His background suggests a willingness to engage with contemporary challenges in a way that seeks to be both faithful to the Gospel and responsive to the needs of the modern world.\nCardinal Parolin, positioned as a moderate, might adopt a more centrist approach, seeking to maintain a degree of continuity with Pope Francis' reforms while also being mindful of the concerns of more traditional elements within the Church.5\nCardinal Pierbattista Pizzaballa, while considered soft conservative, brings a unique perspective from his role as the Latin Patriarch of Jerusalem.5 His experience in a religiously diverse region might inform a nuanced approach to doctrinal questions, emphasizing dialogue and understanding, particularly in interreligious contexts.\nCardinal Prevost, with his background as a canon lawyer, might approach doctrinal questions with a focus on Church law and established theological principles.66\n\nEvangelization:\nThe imperative of evangelization remains a central mission of the Catholic Church in an increasingly secular and diverse world. Declining church attendance and disaffiliation, particularly among younger generations, highlight the urgent need for effective strategies to reach out to both active and disengaged Catholics.152 The next Pope will need to inspire and guide the Church in its efforts to share the Gospel in contemporary society.\nCardinal Tagle's current role as Pro-Prefect of the Dicastery for Evangelisation directly positions him at the forefront of the Church's missionary efforts.4 His reputation as a charismatic communicator with a strong connection to the poor and youth further enhances his suitability to lead the Church's evangelization initiatives.8\nCardinal Erdő has also recognized the importance of the New Evangelization and the need to engage with younger generations.26 His participation in city missions in Europe demonstrates a commitment to reaching out beyond the traditional parish setting.32\nCardinal Pizzaballa's extensive experience in the Holy Land, interacting with pilgrims from diverse backgrounds and engaging deeply with the Scriptures, could inform an approach to evangelization that emphasizes encounter, dialogue, and a profound understanding of the foundational texts of the faith.5\n\nConclusion: Navigating the Future of the Catholic Church\nThe upcoming papal conclave holds immense significance for the Catholic Church as it navigates a rapidly changing global landscape. The choice of the next Pope will not only shape the immediate future of the Church but will also have long-term implications for its engagement with the world. Considering both the electoral dynamics and the suitability of potential candidates to address the Church's current challenges provides a comprehensive perspective on this pivotal moment.\nWhile the prediction of a specific successor remains inherently uncertain, the analysis suggests that Cardinals Pietro Parolin, Péter Erdő, Luis Antonio Tagle, Robert Francis Prevost, and Pierbattista Pizzaballa are among the most prominent figures whose electoral viability warrants close attention. Their respective theological positions, regional influence, administrative experience, age, and standing within the College of Cardinals will all play a role in the deliberations within the conclave.\n\n---\n\nTo reform Catholic Church, honor Christ's authority, accessed April 26, 2025, https://www.ncronline.org/opinion/guest-voices/reform-catholic-church-honor-christs-authority\nHow Pope Francis Radically Reshaped the Catholic Church - Newsweek, accessed April 26, 2025, https://www.newsweek.com/how-francis-radically-reshaped-catholic-church-2035465\nUnderstanding the Reform of the Roman Curia: A Conversation With Monsignor Anthony Ekpo - Berkley Center for Religion, Peace and World Affairs, accessed April 26, 2025, https://berkleycenter.georgetown.edu/features/understanding-the-reform-of-the-roman-curia-a-conversation-with-monsignor-anthony-ekpo\nPope Francis and the Reform of the Roman Curia: Challenges and Opportunities, accessed April 26, 2025, https://berkleycenter.georgetown.edu/events/pope-francis-and-the-reform-of-the-roman-curia-challenges-and-opportunities\nThe Main Challenges Facing the Catholic Church in 2024, accessed April 26, 2025, https://catholicchurchreformintl.org/the-main-challenges-facing-the-catholic-church-in-2024/\nU.S. Church Challenges to Francis' reform - ROCC, accessed April 26, 2025, https://www.jrconnolly.net/us-church-challenges-to-francis-reform.html\nPope Francis' reforms to church governance are unlike any since Vatican II, accessed April 26, 2025, https://www.ncronline.org/news/opinion/pope-francis-reforms-church-governance-are-unlike-any-vatican-ii\nContemporary Challenges for Global Catholicism - LA CIVILTÀ CATTOLICA, accessed April 26, 2025, https://www.laciviltacattolica.com/contemporary-challenges-for-global-catholicism/\nWho governs the Catholic Church? It's an open question. - America Magazine, accessed April 26, 2025, https://www.americamagazine.org/faith/2020/10/16/who-governs-catholic-church-its-open-question\nReform Your Expectations of Church Reform | Catholic Answers Magazine, accessed April 26, 2025, https://www.catholic.com/magazine/online-edition/reform-your-expectations-of-church-reform\nHow Pope Francis changed the Catholic Church | PBS News, accessed April 26, 2025, https://www.pbs.org/newshour/world/how-pope-francis-changed-the-catholic-church\ncollegeofcardinalsreport.com, accessed April 26, 2025, https://collegeofcardinalsreport.com/cardinals/pietro-parolin/#:~:text=In%202009%2C%20he%20was%20ordained,advise%20him%20on%20Church%20reform.\nPAROLIN Card. Pietro - The Holy See, accessed April 26, 2025, https://press.vatican.va/content/salastampa/en/documentation/cardinali_biografie/cardinali_bio_parolin_p.html\n'Jesus Wept' author chronicles the debates roiling the Catholic church | NSPR, accessed April 26, 2025, https://www.mynspr.org/2025-02-24/jesus-wept-author-chronicles-the-debates-roiling-the-catholic-church\nRome is not the Answer to the Ailments of Protestantism: The need for a Reformation in Ecclesiology in 2025 - Kuyperian Commentary, accessed April 26, 2025, https://kuyperian.com/rome-is-not-the-answer-to-the-ailments-of-protestantism-the-need-for-a-reformation-in-ecclesiology-in-2025/\nYear of Jubilee | Catholic Answers Guide to Jubilee 2025, accessed April 26, 2025, https://www.catholic.com/tract/year-of-jubilee-catholic-answers-guide-to-jubilee-2025\nWhat might 2025 hold for the Church? - Catholic World Report, accessed April 26, 2025, https://www.catholicworldreport.com/2025/01/03/what-might-2025-hold-for-the-church/\nThe Biggest Challenge for Christianity in 2025 - Josh.org, accessed April 26, 2025, https://www.josh.org/christianity-challenge-2025/\nThe Hope in the Jubilee Year 2025 - Modern Diplomacy, accessed April 26, 2025, https://moderndiplomacy.eu/2025/01/19/the-hope-in-the-jubilee-year-2025/\nTuesday, April 22, 2025 - AlbertMohler.com, accessed April 26, 2025, https://albertmohler.com/2025/04/22/briefing-4-22-25/\nVatican puts the brakes on Synod on Synodality, pushes 'controversial' topics to 2025, accessed April 26, 2025, https://religionnews.com/2024/03/14/vatican-pulls-the-break-on-synod-on-synodality-pushes-hot-topics-to-2025/\nWhat is your biggest issue with the Catholic faith? : r/Christianity - Reddit, accessed April 26, 2025, https://www.reddit.com/r/Christianity/comments/p2kbkm/what_is_your_biggest_issue_with_the_catholic_faith/\n5 Obstacles to Evangelization We Can Overcome - Benedictine College Media & Culture, accessed April 26, 2025, https://media.benedictine.edu/5-obstacles-to-evangelization-we-can-overcome\n5 Reasons Catholics Fail to Evangelize Modern People, accessed April 26, 2025, https://catholicmissionarydisciples.com/news/5-reasons\nThe 'new evangelization' has problems, but a synodal approach can help, accessed April 26, 2025, https://www.ncronline.org/news/opinion/new-evangelization-has-problems-synodal-approach-can-help\nChallenges to Evangelization in our Culture: Obstacles Can Be Opportunities and Open Doors - Community in Mission, accessed April 26, 2025, https://blog.adw.org/2010/09/challenges-to-evangelization-in-our-culture/\nThe Strange Myths of the New Evangelization | Church Life Journal, accessed April 26, 2025, https://churchlifejournal.nd.edu/articles/the-strange-myths-of-the-new-evangelization/\nEvangelization Challenges - CHARIS International, accessed April 26, 2025, https://www.charis.international/en/evangelization-challenges/\nThe Challenge of Evangelization | EWTN, accessed April 26, 2025, https://www.ewtn.com/catholicism/library/challenge-of-evangelization-7930\nThe Problem with the New Evangelization – IGNITUM TODAY, accessed April 26, 2025, https://ignitumtoday.com/2013/11/24/the-problem-with-the-new-evangelization/\nProblems with the New Evangelization (Guest: Msgr. Charles Pope) - Crisis Magazine, accessed April 26, 2025, https://crisismagazine.com/podcast/problems-with-the-new-evangelization-guest-msgr-charles-pope\n\n\n\nSource: https://g.co/gemini/share/0fce85b69101",
      "language": "pope/gemini (deep research)",
      "lines": 81,
      "path": "Pope/Gemini (Deep Research)",
      "rank": 8,
      "repository": "jaldps/ai-tests",
      "stars": 57,
      "url": "https://github.com/jaldps/ai-tests/blob/b293470791fc799f742eb0186dd4c0ea93fb2c9a/Pope/Gemini%20(Deep%20Research)"
    },
    {
      "content": "{'arxiv_id': 'arXiv:2507.16656', 'title': 'P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs', 'authors': 'Dongjun Jang, Youngchae Ahn, Hyopil Shin', 'link': 'https://arxiv.org/abs/2507.16656', 'abstract': 'This study explores the potential of phonological reasoning within text-based large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess tasks like rhyme word generation, g2p conversion, and syllable counting. Our evaluations across 12 LLMs reveal that while few-shot learning offers inconsistent gains, the introduction of a novel Pedagogically-motivated Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational theories like scaffolding and discovery learning, consistently enhances performance. This method leverages structured guidance to activate latent phonological abilities, achieving up to 52% improvement and even surpassing human baselines in certain tasks. Future work could aim to optimize P-CoT prompts for specific models or explore their application across different linguistic domains.'}\n{'arxiv_id': 'arXiv:2507.16642', 'title': 'Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models', 'authors': 'Armin Berger, Lars Hillebrand, David Leonhard, Tobias Deußer, Thiago Bell Felix de Oliveira, Tim Dilmaghani, Mohamed Khaled, Bernd Kliem, Rüdiger Loitz, Christian Bauckhage, Rafet Sifa', 'link': 'https://arxiv.org/abs/2507.16642', 'abstract': \"The auditing of financial documents, historically a labor-intensive process, stands on the precipice of transformation. AI-driven solutions have made inroads into streamlining this process by recommending pertinent text passages from financial reports to align with the legal requirements of accounting standards. However, a glaring limitation remains: these systems commonly fall short in verifying if the recommended excerpts indeed comply with the specific legal mandates. Hence, in this paper, we probe the efficiency of publicly available Large Language Models (LLMs) in the realm of regulatory compliance across different model configurations. We place particular emphasis on comparing cutting-edge open-source LLMs, such as Llama-2, with their proprietary counterparts like OpenAI's GPT models. This comparative analysis leverages two custom datasets provided by our partner PricewaterhouseCoopers (PwC) Germany. We find that the open-source Llama-2 70 billion model demonstrates outstanding performance in detecting non-compliance or true negative occurrences, beating all their proprietary counterparts. Nevertheless, proprietary models such as GPT-4 perform the best in a broad variety of scenarios, particularly in non-English contexts.\"}\n{'arxiv_id': 'arXiv:2507.16632', 'title': 'Step-Audio 2 Technical Report', 'authors': 'Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang', 'link': 'https://arxiv.org/abs/2507.16632', 'abstract': 'This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit this https URL for more information.'}\n{'arxiv_id': 'arXiv:2507.16572', 'title': 'Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models', 'authors': 'Mohamad Ballout, Serwan Jassim, Elia Bruni', 'link': 'https://arxiv.org/abs/2507.16572', 'abstract': 'This paper presents a systematic evaluation of state-of-the-art multimodal large language models (MLLMs) on intuitive physics tasks using the GRASP and IntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL, LLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that even the latest models struggle to reliably distinguish physically plausible from implausible scenarios. To go beyond performance metrics, we conduct a probing analysis of model embeddings, extracting intermediate representations at key processing stages to examine how well task-relevant information is preserved. Our results show that, depending on task difficulty, a critical vision-language misalignment can emerge: vision encoders successfully capture physical plausibility cues, but this information is not effectively utilized by the language model, leading to failures in reasoning. This misalignment suggests that the primary limitation of MLLMs in intuitive physics tasks is not the vision component but the ineffective integration of visual and linguistic information. Our findings highlight vision-language alignment as a key area for improvement, offering insights for future MLLMs development.'}\n{'arxiv_id': 'arXiv:2507.16557', 'title': 'Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language', 'authors': 'Kristin Gnadt, David Thulke, Simone Kopeinik, Ralf Schlüter', 'link': 'https://arxiv.org/abs/2507.16557', 'abstract': 'In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated with gender bias in German, including the ambiguous interpretation of male occupational terms and the influence of seemingly neutral nouns on gender perception. This work contributes to the understanding of gender bias in LLMs across languages and underscores the necessity for tailored evaluation frameworks.'}\n{'arxiv_id': 'arXiv:2507.16530', 'title': 'Learning Text Styles: A Study on Transfer, Attribution, and Verification', 'authors': 'Zhiqiang Hu', 'link': 'https://arxiv.org/abs/2507.16530', 'abstract': 'This thesis advances the computational understanding and manipulation of text styles through three interconnected pillars: (1) Text Style Transfer (TST), which alters stylistic properties (e.g., sentiment, formality) while preserving content; (2)Authorship Attribution (AA), identifying the author of a text via stylistic fingerprints; and (3) Authorship Verification (AV), determining whether two texts share the same authorship. We address critical challenges in these areas by leveraging parameter-efficient adaptation of large language models (LLMs), contrastive disentanglement of stylistic features, and instruction-based fine-tuning for explainable verification.'}\n{'arxiv_id': 'arXiv:2507.16515', 'title': 'Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness', 'authors': 'Siqi Liu, Guangrong Dai, Dechao Li', 'link': 'https://arxiv.org/abs/2507.16515', 'abstract': \"This preliminary study investigates the usefulness of sentence-level Quality Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE), focusing on its impact on post-editing speed and student translators' perceptions. It also explores the interaction effects between QE and MT quality, as well as between QE and translation expertise. The findings reveal that QE significantly reduces post-editing time. The examined interaction effects were not significant, suggesting that QE consistently improves MTPE efficiency across medium- and high-quality MT outputs and among student translators with varying levels of expertise. In addition to indicating potentially problematic segments, QE serves multiple functions in MTPE, such as validating translators' evaluations of MT quality and enabling them to double-check translation outputs. However, interview data suggest that inaccurate QE may hinder post-editing processes. This research provides new insights into the strengths and limitations of QE, facilitating its more effective integration into MTPE workflows to enhance translators' productivity.\"}\n{'arxiv_id': 'arXiv:2507.16514', 'title': 'The Ever-Evolving Science Exam', 'authors': 'Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai', 'link': 'https://arxiv.org/abs/2507.16514', 'abstract': 'As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad **Range**, wide **Reach**, and high **Rigor**, yet they often face two major challenges: **data leakage risks** that compromise benchmarking validity, and **evaluation inefficiency** due to large-scale testing. To address these issues, we introduce the **Ever-Evolving Science Exam (EESE)**, a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public **EESE-Pool** with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**, and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: this https URL.'}\n{'arxiv_id': 'arXiv:2507.16490', 'title': 'Combining Language and Topic Models for Hierarchical Text Classification', 'authors': 'Jaco du Toit, Marcel Dunaiski', 'link': 'https://arxiv.org/abs/2507.16490', 'abstract': 'Hierarchical text classification (HTC) is a natural language processing task which has the objective of categorising text documents into a set of classes from a predefined structured class hierarchy. Recent HTC approaches use various techniques to incorporate the hierarchical class structure information with the natural language understanding capabilities of pre-trained language models (PLMs) to improve classification performance. Furthermore, using topic models along with PLMs to extract features from text documents has been shown to be an effective approach for multi-label text classification tasks. The rationale behind the combination of these feature extractor models is that the PLM captures the finer-grained contextual and semantic information while the topic model obtains high-level representations which consider the corpus of documents as a whole. In this paper, we use a HTC approach which uses a PLM and a topic model to extract features from text documents which are used to train a classification model. Our objective is to determine whether the combination of the features extracted from the two models is beneficial to HTC performance in general. In our approach, the extracted features are passed through separate convolutional layers whose outputs are combined and passed to a label-wise attention mechanisms which obtains label-specific document representations by weighing the most important features for each class separately. We perform comprehensive experiments on three HTC benchmark datasets and show that using the features extracted from the topic model generally decreases classification performance compared to only using the features obtained by the PLM. In contrast to previous work, this shows that the incorporation of features extracted from topic models for text classification tasks should not be assumed beneficial.'}\n{'arxiv_id': 'arXiv:2507.16488', 'title': 'ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs', 'authors': 'Zhenliang Zhang, Xinyu Hu, Huixuan Zhang, Junzhe Zhang, Xiaojun Wan', 'link': 'https://arxiv.org/abs/2507.16488', 'abstract': \"Large language models (LLMs) excel at various natural language processing tasks, but their tendency to generate hallucinations undermines their reliability. Existing hallucination detection methods leveraging hidden states predominantly focus on static and isolated representations, overlooking their dynamic evolution across layers, which limits efficacy. To address this limitation, we shift the focus to the hidden state update process and introduce a novel metric, the ICR Score (Information Contribution to Residual Stream), which quantifies the contribution of modules to the hidden states' update. We empirically validate that the ICR Score is effective and reliable in distinguishing hallucinations. Building on these insights, we propose a hallucination detection method, the ICR Probe, which captures the cross-layer evolution of hidden states. Experimental results show that the ICR Probe achieves superior performance with significantly fewer parameters. Furthermore, ablation studies and case analyses offer deeper insights into the underlying mechanism of this method, improving its interpretability.\"}\n{'arxiv_id': 'arXiv:2507.16459', 'title': 'Towards Enforcing Company Policy Adherence in Agentic Workflows', 'authors': 'Naama Zwerdling, David Boaz, Ella Rabinovich, Guy Uziel, David Amid, Ateret Anaby-Tavor', 'link': 'https://arxiv.org/abs/2507.16459', 'abstract': 'Large Language Model (LLM) agents hold promise for a flexible and scalable alternative to traditional business process automation, but struggle to reliably follow complex company policies. In this study we introduce a deterministic, transparent, and modular framework for enforcing business policy adherence in agentic workflows. Our method operates in two phases: (1) an offline buildtime stage that compiles policy documents into verifiable guard code associated with tool use, and (2) a runtime integration where these guards ensure compliance before each agent action. We demonstrate our approach on the challenging $\\\\tau$-bench Airlines domain, showing encouraging preliminary results in policy enforcement, and further outline key challenges for real-world deployments.'}\n{'arxiv_id': 'arXiv:2507.16442', 'title': 'Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch', 'authors': 'Elza Strazda, Gerasimos Spanakis', 'link': 'https://arxiv.org/abs/2507.16442', 'abstract': 'Warning: This paper contains explicit statements of offensive stereotypes which might be upsetting.\\nLanguage models are prone to exhibiting biases, further amplifying unfair and harmful stereotypes. Given the fast-growing popularity and wide application of these models, it is necessary to ensure safe and fair language models. As of recent considerable attention has been paid to measuring bias in language models, yet the majority of studies have focused only on English language. A Dutch version of the US-specific CrowS-Pairs dataset for measuring bias in Dutch language models is introduced. The resulting dataset consists of 1463 sentence pairs that cover bias in 9 categories, such as Sexual orientation, Gender and Disability. The sentence pairs are composed of contrasting sentences, where one of the sentences concerns disadvantaged groups and the other advantaged groups. Using the Dutch CrowS-Pairs dataset, we show that various language models, BERTje, RobBERT, multilingual BERT, GEITje and Mistral-7B exhibit substantial bias across the various bias categories. Using the English and French versions of the CrowS-Pairs dataset, bias was evaluated in English (BERT and RoBERTa) and French (FlauBERT and CamemBERT) language models, and it was shown that English models exhibit the most bias, whereas Dutch models the least amount of bias. Additionally, results also indicate that assigning a persona to a language model changes the level of bias it exhibits. These findings highlight the variability of bias across languages and contexts, suggesting that cultural and linguistic factors play a significant role in shaping model biases.'}\n{'arxiv_id': 'arXiv:2507.16424', 'title': 'PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning', 'authors': 'Hui Xiang, Jinqiao Shi, Ting Zhang, Xiaojie Zhao, Yong Liu, Yong Ma', 'link': 'https://arxiv.org/abs/2507.16424', 'abstract': \"Active learning (AL) aims to optimize model training and reduce annotation costs by selecting the most informative samples for labeling. Typically, AL methods rely on the empirical distribution of labeled data to define the decision boundary and perform uncertainty or diversity estimation, subsequently identifying potential high-quality samples. In few-shot scenarios, the empirical distribution often diverges significantly from the target distribution, causing the decision boundary to shift away from its optimal position. However, existing methods overlook the role of unlabeled samples in enhancing the empirical distribution to better align with the target distribution, resulting in a suboptimal decision boundary and the selection of samples that inadequately represent the target distribution. To address this, we propose a hybrid AL framework, termed \\\\textbf{PromptAL} (Sample-Aware Dynamic Soft \\\\textbf{Prompts} for Few-Shot \\\\textbf{A}ctive \\\\textbf{L}earning). This framework accounts for the contribution of each unlabeled data point in aligning the current empirical distribution with the target distribution, thereby optimizing the decision boundary. Specifically, PromptAL first leverages unlabeled data to construct sample-aware dynamic soft prompts that adjust the model's predictive distribution and decision boundary. Subsequently, based on the adjusted decision boundary, it integrates uncertainty estimation with both global and local diversity to select high-quality samples that more accurately represent the target distribution. Experimental results on six in-domain and three out-of-domain datasets show that PromptAL achieves superior performance over nine baselines. Our codebase is openly accessible.\"}\n{'arxiv_id': 'arXiv:2507.16410', 'title': 'GG-BBQ: German Gender Bias Benchmark for Question Answering', 'authors': 'Shalaka Satheesh, Katrin Klug, Katharina Beckh, Héctor Allende-Cid, Sebastian Houben, Teena Hassan', 'link': 'https://arxiv.org/abs/2507.16410', 'abstract': \"Within the context of Natural Language Processing (NLP), fairness evaluation is often associated with the assessment of bias and reduction of associated harm. In this regard, the evaluation is usually carried out by using a benchmark dataset, for a task such as Question Answering, created for the measurement of bias in the model's predictions along various dimensions, including gender identity. In our work, we evaluate gender bias in German Large Language Models (LLMs) using the Bias Benchmark for Question Answering by Parrish et al. (2022) as a reference. Specifically, the templates in the gender identity subset of this English dataset were machine translated into German. The errors in the machine translated templates were then manually reviewed and corrected with the help of a language expert. We find that manual revision of the translation is crucial when creating datasets for gender bias evaluation because of the limitations of machine translation from English to a language such as German with grammatical gender. Our final dataset is comprised of two subsets: Subset-I, which consists of group terms related to gender identity, and Subset-II, where group terms are replaced with proper names. We evaluate several LLMs used for German NLP on this newly created dataset and report the accuracy and bias scores. The results show that all models exhibit bias, both along and against existing social stereotypes.\"}\n{'arxiv_id': 'arXiv:2507.16331', 'title': 'Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny', 'authors': 'Chuanhao Yan, Fengdi Che, Xuhan Huang, Xu Xu, Xin Li, Yizhi Li, Xingwei Qu, Jingzhe Shi, Zhuangzhuang He, Chenghua Lin, Yaodong Yang, Binhang Yuan, Hang Zhao, Yu Qiao, Bowen Zhou, Jie Fu', 'link': 'https://arxiv.org/abs/2507.16331', 'abstract': 'Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark.'}\n{'arxiv_id': 'arXiv:2507.16323', 'title': 'SpeLLM: Character-Level Multi-Head Decoding', 'authors': 'Amit Ben-Artzy, Roy Schwartz', 'link': 'https://arxiv.org/abs/2507.16323', 'abstract': \"Scaling LLM vocabulary is often used to reduce input sequence length and alleviate attention's quadratic cost. Yet, current LLM architectures impose a critical bottleneck to this procedure: the output projection layer scales linearly with vocabulary size, rendering substantial expansion impractical. We propose SpeLLM, a method that decouples input and output vocabularies by predicting character-level strings through multiple output heads. In SpeLLM, each of the $k$ linear heads predicts a single character simultaneously, enabling the model to represent a much larger output space using smaller, independent linear heads. We present a self-distillation approach for converting a standard LLM to a SpeLLM. Our experiments with four pre-trained LLMs show their SpeLLM variants achieve competitive performance on downstream tasks while reducing runtime by 5.1% on average across models. Our approach provides a potential avenue for reducing LLM costs, while increasing support for underrepresented languages and domains.\"}\n{'arxiv_id': 'arXiv:2507.16284', 'title': 'Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis', 'authors': 'Paul-Andrei Pogăcean, Sanda-Maria Avram', 'link': 'https://arxiv.org/abs/2507.16284', 'abstract': 'The debate surrounding language identification has gained renewed attention in recent years, especially with the rapid evolution of AI-powered language models. However, the non-AI-based approaches to language identification have been overshadowed. This research explores a mathematical implementation of an algorithm for language determinism by leveraging monograms and bigrams frequency rankings derived from established linguistic research. The datasets used comprise texts varying in length, historical period, and genre, including short stories, fairy tales, and poems. Despite these variations, the method achieves over 80\\\\% accuracy on texts shorter than 150 characters and reaches 100\\\\% accuracy for longer texts and older writings. These results demonstrate that classical frequency-based approaches remain effective and scalable alternatives to AI-driven models for language detection.'}\n{'arxiv_id': 'arXiv:2507.16271', 'title': 'Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction', 'authors': 'Tianyun Zhong, Guozhao Mo, Yanjiang Liu, Yihan Chen, Lingdi Kong, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Le Sun', 'link': 'https://arxiv.org/abs/2507.16271', 'abstract': 'With the emergence of large language models (LLMs), there is an expectation that LLMs can effectively extract explicit information from complex real-world documents (e.g., papers, reports). However, most LLMs generate paragraph-style answers that are chaotic, disorganized, and untraceable. To bridge this gap, we introduce the Arranged and Organized Extraction Benchmark (AOE), a new bilingual benchmark with data and documents of varying lengths designed to systematically evaluate the ability of LLMs to comprehend fragmented documents and reconstruct isolated information into one organized table. Unlike conventional text-to-table tasks, which rely on fixed schema and narrow task domains, AOE includes 11 carefully crafted tasks across three diverse domains, requiring models to generate context-specific schema tailored to varied input queries. In the experiment, we evaluated both open-source and closed-source state-of-the-art LLMs. The results show that even the most advanced models struggled significantly. The benchmark is available at this https URL.'}\n{'arxiv_id': 'arXiv:2507.16263', 'title': 'iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss', 'authors': 'Yujian Sun, Tian Li', 'link': 'https://arxiv.org/abs/2507.16263', 'abstract': 'As the Large Language Model (LLM) gains widespread adoption, increasing attention has been given to the challenge of making LLM forget non-compliant data memorized during its pre-training. Machine Unlearning focuses on efficiently erasing sensitive information from LLM under limited computational resources. To advance research in this area, SemEval 2025 Task 4: \"Unlearning Sensitive Content from Large Language Models\" introduces three unlearning datasets and establishes a benchmark by evaluating both forgetting effectiveness and the preservation of standard capabilities. In this work, we propose a more controllable forgetting loss, Effective Unlearning Loss, and explore its integration with various techniques to achieve more efficient and controlled unlearning. Our system ultimately ranked 5th on the competition leaderboard.'}\n{'arxiv_id': 'arXiv:2507.16252', 'title': 'Efficient RL for optimizing conversation level outcomes with an LLM-based tutor', 'authors': 'Hyunji Nam, Omer Gottesman, Amy Zhang, Dean Foster, Emma Brunskill, Lyle Ungar', 'link': 'https://arxiv.org/abs/2507.16252', 'abstract': \"Large language models (LLMs) built on existing reinforcement learning with human feedback (RLHF) frameworks typically optimize responses based on immediate turn-level human preferences. However, this approach falls short in multi-turn dialogue settings, such as online math tutoring. We propose a method to enhance LLM-based tutors by representing the dialogue history with a lower-dimensional latent state representation of a student and optimizing a long-term policy to determine high-level actions based on the latent state. The goal is to better align the tutor's behavior with the long-term objective of guiding the student towards solving a target math problem on their own. Our model is lightweight, requiring less computational resources than prior work of training the tutor policy end-to-end to directly output the tutor's next utterance. Our experiment results demonstrate that these modifications lead to improved long-term outcomes compared to prompting in LLM-simulated tutoring tasks.\"}\n{'arxiv_id': 'arXiv:2507.16248', 'title': 'FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents', 'authors': 'Run Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, Zhengwen Qiu', 'link': 'https://arxiv.org/abs/2507.16248', 'abstract': 'Recently, AI agents are rapidly evolving in intelligence and widely used in professional research applications, such as STEM, software development, finance, etc. Among these AI agents, deep research agent is a key category as it can perform long-horizon tasks and solve problems of greater complexity. However, there are few evaluation frameworks and benchmarks that systematically and automatically investigate the capabilities of these research agents. Furthermore, financial research problems have distinct complexity and subtlety. To fill in the gap, we propose FinResearchBench, which is a logic tree based Agent-as-a-Judge and targets specifically for the financial research agents. It provides a comprehensive and automatic assessment of the research agents across 7 key types of tasks in the financial research domain. The contributions of this work are two-folded: (1) the first and innovative Agent-as-a-Judge system that extracts the logic tree of the research outcome and uses it as the intermediate information to present a comprehensive, reliable and robust evaluation; (2) finance oriented that it covers 70 typical financial research questions, spreading across 7 frequently encountered types of tasks in the domain.'}\n{'arxiv_id': 'arXiv:2507.16217', 'title': 'Towards Compute-Optimal Many-Shot In-Context Learning', 'authors': 'Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister', 'link': 'https://arxiv.org/abs/2507.16217', 'abstract': 'Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.'}\n{'arxiv_id': 'arXiv:2507.16199', 'title': 'WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability', 'authors': 'Zipeng Ling, Yuehao Tang, Shuliang Liu, Junqi Yang, Shenghong Fu, Yao Wan, Kejia Huang, Zhichao Hou, Xuming Hu', 'link': 'https://arxiv.org/abs/2507.16199', 'abstract': 'Large Language Models (LLMs) frequently output the label \\\\emph{Unknown}, yet current evaluations focus almost exclusively on whether such answers are \\\\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an input that is genuinely indeterminate and (ii) a solvable problem that the model fails to resolve. We call this phenomenon \\\\emph{Vague Perception}. And thus we introduce a framework that quantifies the proportion of \\\\emph{Unknown} responses attributable to model incapacity and tests whether guided stimulation can convert them into either correct (\\\\emph{Known}) or intrinsically indeterminate outcomes. By separating these sources of uncertainty, our method provides a clearer picture of LLM reasoning limits and their potential for improvement. As we get a theoretical accuracy of reasoning task on different LLMs, we apply different methods to test whether the model can reach the accuracy given a baseline framework. Our work is meaningful in exploring the true reasoning ability of LLMs and providing a new perspective on solving the \\\\emph{Vague Perception} phenomenon.'}\n{'arxiv_id': 'arXiv:2507.16196', 'title': 'Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task', 'authors': 'Jared Moore, Ned Cooper, Rasmus Overmark, Beba Cibralic, Nick Haber, Cameron R. Jones', 'link': 'https://arxiv.org/abs/2507.16196', 'abstract': \"Recent evidence suggests Large Language Models (LLMs) display Theory of Mind (ToM) abilities. Most ToM experiments place participants in a spectatorial role, wherein they predict and interpret other agents' behavior. However, human ToM also contributes to dynamically planning action and strategically intervening on others' mental states. We present MindGames: a novel `planning theory of mind' (PToM) task which requires agents to infer an interlocutor's beliefs and desires to persuade them to alter their behavior. Unlike previous evaluations, we explicitly evaluate use cases of ToM. We find that humans significantly outperform o1-preview (an LLM) at our PToM task (11% higher; $p=0.006$). We hypothesize this is because humans have an implicit causal model of other agents (e.g., they know, as our task requires, to ask about people's preferences). In contrast, o1-preview outperforms humans in a baseline condition which requires a similar amount of planning but minimal mental state inferences (e.g., o1-preview is better than humans at planning when already given someone's preferences). These results suggest a significant gap between human-like social reasoning and LLM abilities.\"}\n{'arxiv_id': 'arXiv:2507.16183', 'title': 'BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset', 'authors': 'Azizul Hakim Fayaz, MD. Shorif Uddin, Rayhan Uddin Bhuiyan, Zakia Sultana, Md. Samiul Islam, Bidyarthi Paul, Tashreef Muhammad, Shahriar Manzoor', 'link': 'https://arxiv.org/abs/2507.16183', 'abstract': 'Hate speech on digital platforms has become a growing concern globally, especially in linguistically diverse countries like Bangladesh, where regional dialects play a major role in everyday communication. Despite progress in hate speech detection for standard Bangla, Existing datasets and systems fail to address the informal and culturally rich expressions found in dialects such as Barishal, Noakhali, and Chittagong. This oversight results in limited detection capability and biased moderation, leaving large sections of harmful content unaccounted for. To address this gap, this study introduces BIDWESH, the first multi-dialectal Bangla hate speech dataset, constructed by translating and annotating 9,183 instances from the BD-SHS corpus into three major regional dialects. Each entry was manually verified and labeled for hate presence, type (slander, gender, religion, call to violence), and target group (individual, male, female, group), ensuring linguistic and contextual accuracy. The resulting dataset provides a linguistically rich, balanced, and inclusive resource for advancing hate speech detection in Bangla. BIDWESH lays the groundwork for the development of dialect-sensitive NLP tools and contributes significantly to equitable and context-aware content moderation in low-resource language settings.'}\n{'arxiv_id': 'arXiv:2507.16083', 'title': 'Efficient Compositional Multi-tasking for On-device Large Language Models', 'authors': 'Ondrej Bohdal, Mete Ozay, Jijoong Moon, Kyeng-Hun Lee, Hyeonmok Ko, Umberto Michieli', 'link': 'https://arxiv.org/abs/2507.16083', 'abstract': 'Adapter parameters provide a mechanism to modify the behavior of machine learning models and have gained significant popularity in the context of large language models (LLMs) and generative AI. These parameters can be merged to support multiple tasks via a process known as task merging. However, prior work on merging in LLMs, particularly in natural language processing, has been limited to scenarios where each test example addresses only a single task. In this paper, we focus on on-device settings and study the problem of text-based compositional multi-tasking, where each test example involves the simultaneous execution of multiple tasks. For instance, generating a translated summary of a long text requires solving both translation and summarization tasks concurrently. To facilitate research in this setting, we propose a benchmark comprising four practically relevant compositional tasks. We also present an efficient method (Learnable Calibration) tailored for on-device applications, where computational resources are limited, emphasizing the need for solutions that are both resource-efficient and high-performing. Our contributions lay the groundwork for advancing the capabilities of LLMs in real-world multi-tasking scenarios, expanding their applicability to complex, resource-constrained use cases.'}\n{'arxiv_id': 'arXiv:2507.16076', 'title': 'The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models', 'authors': 'Marlene Lutz, Indira Sen, Georg Ahnert, Elisa Rogers, Markus Strohmaier', 'link': 'https://arxiv.org/abs/2507.16076', 'abstract': 'Persona prompting is increasingly used in large language models (LLMs) to simulate views of various sociodemographic groups. However, how a persona prompt is formulated can significantly affect outcomes, raising concerns about the fidelity of such simulations. Using five open-source LLMs, we systematically examine how different persona prompt strategies, specifically role adoption formats and demographic priming strategies, influence LLM simulations across 15 intersectional demographic groups in both open- and closed-ended tasks. Our findings show that LLMs struggle to simulate marginalized groups, particularly nonbinary, Hispanic, and Middle Eastern identities, but that the choice of demographic priming and role adoption strategy significantly impacts their portrayal. Specifically, we find that prompting in an interview-style format and name-based priming can help reduce stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B outperform larger ones such as Llama-3.3-70B. Our findings offer actionable guidance for designing sociodemographic persona prompts in LLM-based simulation studies.'}\n{'arxiv_id': 'arXiv:2507.16075', 'title': 'Deep Researcher with Test-Time Diffusion', 'authors': 'Rujun Han, Yanfei Chen, Zoey CuiZhu, Lesly Miculicich, Guan Sun, Yuanjun Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, George Lee, Vishy Tirumalashetty, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, Chen-Yu Lee', 'link': 'https://arxiv.org/abs/2507.16075', 'abstract': 'Deep research agents, powered by Large Language Models (LLMs), are rapidly advancing; yet, their performance often plateaus when generating complex, long-form research reports using generic test-time scaling algorithms. Drawing inspiration from the iterative nature of human research, which involves cycles of searching, reasoning, and revision, we propose the Test-Time Diffusion Deep Researcher (TTD-DR). This novel framework conceptualizes research report generation as a diffusion process. TTD-DR initiates this process with a preliminary draft, an updatable skeleton that serves as an evolving foundation to guide the research direction. The draft is then iteratively refined through a \"denoising\" process, which is dynamically informed by a retrieval mechanism that incorporates external information at each step. The core process is further enhanced by a self-evolutionary algorithm applied to each component of the agentic workflow, ensuring the generation of high-quality context for the diffusion process. This draft-centric design makes the report writing process more timely and coherent while reducing information loss during the iterative search process. We demonstrate that our TTD-DR achieves state-of-the-art results on a wide array of benchmarks that require intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.'}\n{'arxiv_id': 'arXiv:2507.16054', 'title': 'AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering', 'authors': 'Simon Baeuerle, Max Radyschevski, Ulrike Pado', 'link': 'https://arxiv.org/abs/2507.16054', 'abstract': 'In large organisations, knowledge is mainly shared in meetings, which takes up significant amounts of work time. Additionally, frequent in-person meetings produce inconsistent documentation -- official minutes, personal notes, presentations may or may not exist. Shared information therefore becomes hard to retrieve outside of the meeting, necessitating lengthy updates and high-frequency meeting schedules.\\nGenerative Artificial Intelligence (genAI) models like Large Language Models (LLMs) exhibit an impressive performance on spoken and written language processing. This motivates a practical usage of genAI for knowledge management in engineering departments: using genAI for transcribing meetings and integrating heterogeneous additional information sources into an easily usable format for ad-hoc searches.\\nWe implement an end-to-end pipeline to automate the entire meeting documentation workflow in a proof-of-concept state: meetings are recorded and minutes are created by genAI. These are further made easily searchable through a chatbot interface. The core of our work is to test this genAI-based software tooling in a real-world engineering department and collect extensive survey data on both ethical and technical aspects. Direct feedback from this real-world setup points out both opportunities and risks: a) users agree that the effort for meetings could be significantly reduced with the help of genAI models, b) technical aspects are largely solved already, c) organizational aspects are crucial for a successful ethical usage of such a system.'}\n{'arxiv_id': 'arXiv:2507.16011', 'title': 'mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages', 'authors': 'Hellina Hailu Nigatu, Min Li, Maartje ter Hoeve, Saloni Potdar, Sarah Chasins', 'link': 'https://arxiv.org/abs/2507.16011', 'abstract': 'Knowledge Graphs represent real-world entities and the relationships between them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of automatically constructing or predicting missing entities and links for knowledge graphs in a multilingual setting. In this work, we reformulate the mKGC task as a Question Answering (QA) task and introduce mRAKL: a Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve this by using the head entity and linking relation in a question, and having our model predict the tail entity as an answer. Our experiments focus primarily on two low-resourced languages: Tigrinya and Amharic. We experiment with using higher-resourced languages Arabic and English for cross-lingual transfer. With a BM25 retriever, we find that the RAG-based approach improves performance over a no-context setting. Further, our ablation studies show that with an idealized retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points for Tigrinya and Amharic, respectively.'}\n{'arxiv_id': 'arXiv:2507.16007', 'title': \"Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback\", 'authors': 'Hannah Rashkin, Elizabeth Clark, Fantine Huot, Mirella Lapata', 'link': 'https://arxiv.org/abs/2507.16007', 'abstract': 'Can LLMs provide support to creative writers by giving meaningful writing feedback? In this paper, we explore the challenges and limitations of model-generated writing feedback by defining a new task, dataset, and evaluation frameworks. To study model performance in a controlled manner, we present a novel test set of 1,300 stories that we corrupted to intentionally introduce writing issues. We study the performance of commonly used LLMs in this task with both automatic and human evaluation metrics. Our analysis shows that current models have strong out-of-the-box behavior in many respects -- providing specific and mostly accurate writing feedback. However, models often fail to identify the biggest writing issue in the story and to correctly decide when to offer critical vs. positive feedback.'}\n{'arxiv_id': 'arXiv:2507.16003', 'title': 'Learning without training: The implicit dynamics of in-context learning', 'authors': 'Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, Javier Gonzalvo', 'link': 'https://arxiv.org/abs/2507.16003', 'abstract': 'One of the most striking features of Large Language Models (LLM) is their ability to learn in context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in context and not only during training. Specifically, we show under mild simplifying assumptions how a transformer block implicitly transforms a context into a low-rank weight-update of the MLP layer.'}\n{'arxiv_id': 'arXiv:2507.16002', 'title': 'Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation', 'authors': 'Sumit Singh, Rohit Mishra, Uma Shanker Tiwary', 'link': 'https://arxiv.org/abs/2507.16002', 'abstract': \"One major challenge in natural language processing is named entity recognition (NER), which identifies and categorises named entities in textual input. In order to improve NER, this study investigates a Hindi NER technique that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf (Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments the data with retrieved data from external relevant contexts, notably from Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA. However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER generation. Our investigation shows that the mentioned language models (LMs) with Retrieval Augmentation (RA) outperform baseline methods that don't incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69 and 0.495, respectively, without RA and increase to 0.70 and 0.71, respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B by a significant margin. On the other hand the generative models which are not fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval context. The findings show that RA significantly improves performance, especially for low-context data. This study adds significant knowledge about how best to use data augmentation methods and pretrained models to enhance NER performance, particularly in languages with limited resources.\"}\n{'arxiv_id': 'arXiv:2507.15868', 'title': 'Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models', 'authors': 'Altynbek Ismailov, Salia Asanova', 'link': 'https://arxiv.org/abs/2507.15868', 'abstract': 'Large language models (LLMs) now write code in settings where misreading a single word can break safety or cost money, yet we still expect them to overlook stray typos. To probe where useful robustness ends and harmful insensitivity begins, we compile 50 LeetCode problems and craft three minimal prompt perturbations that should vary in importance: (i) progressive underspecification deleting 10 % of words per step; (ii) lexical flip swapping a pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a common noun with an obscure technical synonym. Six frontier models, including three \"reasoning-tuned\" versions, solve each mutated prompt, and their Python outputs are checked against the original test suites to reveal whether they reused the baseline solution or adapted. Among 11 853 generations we observe a sharp double asymmetry. Models remain correct in 85 % of cases even after 90 % of the prompt is missing, showing over-robustness to underspecification, yet only 54 % react to a single quantifier flip that reverses the task, with reasoning-tuned variants even less sensitive than their bases. Jargon edits lie in between, passing through 56 %. Current LLMs thus blur the line between harmless noise and meaning - changing edits, often treating both as ignorable. Masking salient anchors such as function names can force re - evaluation. We advocate evaluation and training protocols that reward differential sensitivity: stay steady under benign noise but adapt - or refuse - when semantics truly change.'}\n{'arxiv_id': 'arXiv:2507.15864', 'title': 'Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity', 'authors': 'Guowen Yuan, Tien-Hsuan Wu, Lianghao Xia, Ben Kao', 'link': 'https://arxiv.org/abs/2507.15864', 'abstract': \"We study the problem of named entity recognition (NER) based on demonstration learning in low-resource scenarios. We identify two issues in demonstration construction and model training. Firstly, existing methods for selecting demonstration examples primarily rely on semantic similarity; We show that feature similarity can provide significant performance improvement. Secondly, we show that the NER tagger's ability to reference demonstration examples is generally inadequate. We propose a demonstration and training approach that effectively addresses these issues. For the first issue, we propose to select examples by dual similarity, which comprises both semantic similarity and feature similarity. For the second issue, we propose to train an NER model with adversarial demonstration such that the model is forced to refer to the demonstrations when performing the tagging task. We conduct comprehensive experiments in low-resource NER tasks, and the results demonstrate that our method outperforms a range of methods.\"}\n{'arxiv_id': 'arXiv:2507.15863', 'title': \"eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs\", 'authors': 'Isaac Shi, Zeyuan Li, Fan Liu, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi', 'link': 'https://arxiv.org/abs/2507.15863', 'abstract': 'We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge) Module, a secure and scalable Retrieval-Augmented Generation pipeline designed specifically for enterprise document question answering. Designed and implemented by eSapiens, the system ingests heterogeneous content (PDF, Office, web), splits it into 1,000-token overlapping chunks, and indexes them in a hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via combined vector+BM25 search, reranked with Cohere, and answered by an LLM using CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap, regenerating answers until every claim is grounded. On four LegalBench subsets, 1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank boosts Precision@10 by approximately 7 pp; the verifier raises TRACe Utilization above 0.50 and limits unsupported statements to less than 3%. All components run in containers, enforce end-to-end TLS 1.3 and AES-256. These results demonstrate that the DEREK module delivers accurate, traceable, and production-ready document QA with minimal operational overhead. The module is designed to meet enterprise demands for secure, auditable, and context-faithful retrieval, providing a reliable baseline for high-stakes domains such as legal and finance.'}\n{'arxiv_id': 'arXiv:2507.16806', 'title': 'Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty', 'authors': 'Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas', 'link': 'https://arxiv.org/abs/2507.16806', 'abstract': 'When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models.'}\n{'arxiv_id': 'arXiv:2507.16795', 'title': 'Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning', 'authors': 'Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda', 'link': 'https://arxiv.org/abs/2507.16795', 'abstract': \"Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.\"}\n{'arxiv_id': 'arXiv:2507.16746', 'title': 'Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning', 'authors': 'Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum', 'link': 'https://arxiv.org/abs/2507.16746', 'abstract': \"Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce $\\\\textbf{Zebra-CoT}$, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.\"}\n{'arxiv_id': 'arXiv:2507.16713', 'title': 'Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory', 'authors': 'Guowei Lan, Kaixian Qu, René Zurbrügg, Changan Chen, Christopher E. Mower, Haitham Bou-Ammar, Marco Hutter', 'link': 'https://arxiv.org/abs/2507.16713', 'abstract': 'Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge. This paper presents ExpTeach, a framework that grounds VLMs to physical robots by building a self-generated memory of real-world experiences. In ExpTeach, the VLM autonomously plans actions, verifies outcomes, reflects on failures, and adapts robot behaviors in a closed loop. The self-generated experiences during this process are then summarized into a long-term memory, enabling retrieval of learned knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12 real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of ExpTeach.'}\n{'arxiv_id': 'arXiv:2507.16577', 'title': 'Scaling Linear Attention with Sparse State Expansion', 'authors': 'Yuqi Pan, Yongqi An, Zheng Li, Yuhong Chou, Ruijie Zhu, Xiaohui Wang, Mingxuan Wang, Jinqiao Wang, Guoqi Li', 'link': 'https://arxiv.org/abs/2507.16577', 'abstract': 'The Transformer architecture, despite its widespread success, struggles with long-context scenarios due to quadratic computation and linear memory growth. While various linear attention variants mitigate these efficiency constraints by compressing context into fixed-size states, they often degrade performance in tasks such as in-context retrieval and reasoning. To address this limitation and achieve more effective context compression, we propose two key innovations. First, we introduce a row-sparse update formulation for linear attention by conceptualizing state updating as information classification. This enables sparse state updates via softmax-based top-$k$ hard classification, thereby extending receptive fields and reducing inter-class interference. Second, we present Sparse State Expansion (SSE) within the sparse framework, which expands the contextual state into multiple partitions, effectively decoupling parameter size from state capacity while maintaining the sparse classification paradigm. Our design, supported by efficient parallelized implementations, yields effective classification and discriminative state representations. We extensively validate SSE in both pure linear and hybrid (SSE-H) architectures across language modeling, in-context retrieval, and mathematical reasoning benchmarks. SSE demonstrates strong retrieval performance and scales favorably with state size. Moreover, after reinforcement learning (RL) training, our 2B SSE-H model achieves state-of-the-art mathematical reasoning performance among small reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25, significantly outperforming similarly sized open-source Transformers. These results highlight SSE as a promising and efficient architecture for long-context modeling.'}\n{'arxiv_id': 'arXiv:2507.16534', 'title': 'Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report', 'authors': 'Shanghai AI Lab, Xiaoyang Chen, Yunhao Chen, Zeren Chen, Zhiyun Chen, Hanyun Cui, Yawen Duan, Jiaxuan Guo, Qi Guo, Xuhao Hu, Hong Huang, Lige Huang, Chunxiao Li, Juncheng Li, Qihao Lin, Dongrui Liu, Xinmin Liu, Zicheng Liu, Chaochao Lu, Xiaoya Lu, Jingjing Qu, Qibing Ren, Jing Shao, Jingwei Shi, Jingwei Sun, Peng Wang, Weibing Wang, Jia Xu, Lewen Yan, Xiao Yu, Yi Yu, Boxuan Zhang, Jie Zhang, Weichen Zhang, Zhijie Zheng, Tianyi Zhou, Bowen Zhou', 'link': 'https://arxiv.org/abs/2507.16534', 'abstract': 'To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, this report presents a comprehensive assessment of their frontier risks. Drawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks in seven areas: cyber offense, biological and chemical risks, persuasion and manipulation, uncontrolled autonomous AI R\\\\&D, strategic deception and scheming, self-replication, and collusion. Guided by the \"AI-$45^\\\\circ$ Law,\" we evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow lines\" (early warning indicators) to define risk zones: green (manageable risk for routine deployment and continuous monitoring), yellow (requiring strengthened mitigations and controlled deployment), and red (necessitating suspension of development and/or deployment). Experimental results show that all recent frontier AI models reside in green and yellow zones, without crossing red lines. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\\\\&D risks. For self-replication, and strategic deception and scheming, most models remain in the green zone, except for certain reasoning models in the yellow zone. In persuasion and manipulation, most models are in the yellow zone due to their effective influence on humans. For biological and chemical risks, we are unable to rule out the possibility of most models residing in the yellow zone, although detailed threat modeling and in-depth assessment are required to make further claims. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.'}\n{'arxiv_id': 'arXiv:2507.16518', 'title': 'C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning', 'authors': 'Xiuwei Chen, Wentao Hu, Hanhui Li, Jun Zhou, Zisheng Chen, Meng Cao, Yihan Zeng, Kui Zhang, Yu-Jie Yuan, Jianhua Han, Hang Xu, Xiaodan Liang', 'link': 'https://arxiv.org/abs/2507.16518', 'abstract': 'Recent advances in multimodal large language models (MLLMs) have shown impressive reasoning capabilities. However, further enhancing existing MLLMs necessitates high-quality vision-language datasets with carefully curated task complexities, which are both costly and challenging to scale. Although recent self-improving models that iteratively refine themselves offer a feasible solution, they still suffer from two core challenges: (i) most existing methods augment visual or textual data separately, resulting in discrepancies in data complexity (e.g., over-simplified diagrams paired with redundant textual descriptions); and (ii) the evolution of data and models is also separated, leading to scenarios where models are exposed to tasks with mismatched difficulty levels. To address these issues, we propose C2-Evo, an automatic, closed-loop self-improving framework that jointly evolves both training data and model capabilities. Specifically, given a base dataset and a base model, C2-Evo enhances them by a cross-modal data evolution loop and a data-model evolution loop. The former loop expands the base dataset by generating complex multimodal problems that combine structured textual sub-problems with iteratively specified geometric diagrams, while the latter loop adaptively selects the generated problems based on the performance of the base model, to conduct supervised fine-tuning and reinforcement learning alternately. Consequently, our method continuously refines its model and training data, and consistently obtains considerable performance gains across multiple mathematical reasoning benchmarks. Our code, models, and datasets will be released.'}\n\n---\n\n{'arxiv_id': 'arXiv:2507.16054', 'title': 'AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering', 'authors': 'Simon Baeuerle, Max Radyschevski, Ulrike Pado', 'link': 'https://arxiv.org/abs/2507.16054', 'abstract': 'In large organisations, knowledge is mainly shared in meetings, which takes up significant amounts of work time. Additionally, frequent in-person meetings produce inconsistent documentation -- official minutes, personal notes, presentations may or may not exist. Shared information therefore becomes hard to retrieve outside of the meeting, necessitating lengthy updates and high-frequency meeting schedules.\\nGenerative Artificial Intelligence (genAI) models like Large Language Models (LLMs) exhibit an impressive performance on spoken and written language processing. This motivates a practical usage of genAI for knowledge management in engineering departments: using genAI for transcribing meetings and integrating heterogeneous additional information sources into an easily usable format for ad-hoc searches.\\nWe implement an end-to-end pipeline to automate the entire meeting documentation workflow in a proof-of-concept state: meetings are recorded and minutes are created by genAI. These are further made easily searchable through a chatbot interface. The core of our work is to test this genAI-based software tooling in a real-world engineering department and collect extensive survey data on both ethical and technical aspects. Direct feedback from this real-world setup points out both opportunities and risks: a) users agree that the effort for meetings could be significantly reduced with the help of genAI models, b) technical aspects are largely solved already, c) organizational aspects are crucial for a successful ethical usage of such a system.'}\n{'arxiv_id': 'arXiv:2507.16011', 'title': 'mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages', 'authors': 'Hellina Hailu Nigatu, Min Li, Maartje ter Hoeve, Saloni Potdar, Sarah Chasins', 'link': 'https://arxiv.org/abs/2507.16011', 'abstract': 'Knowledge Graphs represent real-world entities and the relationships between them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of automatically constructing or predicting missing entities and links for knowledge graphs in a multilingual setting. In this work, we reformulate the mKGC task as a Question Answering (QA) task and introduce mRAKL: a Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve this by using the head entity and linking relation in a question, and having our model predict the tail entity as an answer. Our experiments focus primarily on two low-resourced languages: Tigrinya and Amharic. We experiment with using higher-resourced languages Arabic and English for cross-lingual transfer. With a BM25 retriever, we find that the RAG-based approach improves performance over a no-context setting. Further, our ablation studies show that with an idealized retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points for Tigrinya and Amharic, respectively.'}\n{'arxiv_id': 'arXiv:2507.16007', 'title': \"Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback\", 'authors': 'Hannah Rashkin, Elizabeth Clark, Fantine Huot, Mirella Lapata', 'link': 'https://arxiv.org/abs/2507.16007', 'abstract': 'Can LLMs provide support to creative writers by giving meaningful writing feedback? In this paper, we explore the challenges and limitations of model-generated writing feedback by defining a new task, dataset, and evaluation frameworks. To study model performance in a controlled manner, we present a novel test set of 1,300 stories that we corrupted to intentionally introduce writing issues. We study the performance of commonly used LLMs in this task with both automatic and human evaluation metrics. Our analysis shows that current models have strong out-of-the-box behavior in many respects -- providing specific and mostly accurate writing feedback. However, models often fail to identify the biggest writing issue in the story and to correctly decide when to offer critical vs. positive feedback.'}\n{'arxiv_id': 'arXiv:2507.16003', 'title': 'Learning without training: The implicit dynamics of in-context learning', 'authors': 'Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, Javier Gonzalvo', 'link': 'https://arxiv.org/abs/2507.16003', 'abstract': 'One of the most striking features of Large Language Models (LLM) is their ability to learn in context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in context and not only during training. Specifically, we show under mild simplifying assumptions how a transformer block implicitly transforms a context into a low-rank weight-update of the MLP layer.'}\n{'arxiv_id': 'arXiv:2507.16002', 'title': 'Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation', 'authors': 'Sumit Singh, Rohit Mishra, Uma Shanker Tiwary', 'link': 'https://arxiv.org/abs/2507.16002', 'abstract': \"One major challenge in natural language processing is named entity recognition (NER), which identifies and categorises named entities in textual input. In order to improve NER, this study investigates a Hindi NER technique that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf (Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments the data with retrieved data from external relevant contexts, notably from Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA. However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER generation. Our investigation shows that the mentioned language models (LMs) with Retrieval Augmentation (RA) outperform baseline methods that don't incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69 and 0.495, respectively, without RA and increase to 0.70 and 0.71, respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B by a significant margin. On the other hand the generative models which are not fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval context. The findings show that RA significantly improves performance, especially for low-context data. This study adds significant knowledge about how best to use data augmentation methods and pretrained models to enhance NER performance, particularly in languages with limited resources.\"}\n{'arxiv_id': 'arXiv:2507.15868', 'title': 'Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models', 'authors': 'Altynbek Ismailov, Salia Asanova', 'link': 'https://arxiv.org/abs/2507.15868', 'abstract': 'Large language models (LLMs) now write code in settings where misreading a single word can break safety or cost money, yet we still expect them to overlook stray typos. To probe where useful robustness ends and harmful insensitivity begins, we compile 50 LeetCode problems and craft three minimal prompt perturbations that should vary in importance: (i) progressive underspecification deleting 10 % of words per step; (ii) lexical flip swapping a pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a common noun with an obscure technical synonym. Six frontier models, including three \"reasoning-tuned\" versions, solve each mutated prompt, and their Python outputs are checked against the original test suites to reveal whether they reused the baseline solution or adapted. Among 11 853 generations we observe a sharp double asymmetry. Models remain correct in 85 % of cases even after 90 % of the prompt is missing, showing over-robustness to underspecification, yet only 54 % react to a single quantifier flip that reverses the task, with reasoning-tuned variants even less sensitive than their bases. Jargon edits lie in between, passing through 56 %. Current LLMs thus blur the line between harmless noise and meaning - changing edits, often treating both as ignorable. Masking salient anchors such as function names can force re - evaluation. We advocate evaluation and training protocols that reward differential sensitivity: stay steady under benign noise but adapt - or refuse - when semantics truly change.'}\n{'arxiv_id': 'arXiv:2507.15864', 'title': 'Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity', 'authors': 'Guowen Yuan, Tien-Hsuan Wu, Lianghao Xia, Ben Kao', 'link': 'https://arxiv.org/abs/2507.15864', 'abstract': \"We study the problem of named entity recognition (NER) based on demonstration learning in low-resource scenarios. We identify two issues in demonstration construction and model training. Firstly, existing methods for selecting demonstration examples primarily rely on semantic similarity; We show that feature similarity can provide significant performance improvement. Secondly, we show that the NER tagger's ability to reference demonstration examples is generally inadequate. We propose a demonstration and training approach that effectively addresses these issues. For the first issue, we propose to select examples by dual similarity, which comprises both semantic similarity and feature similarity. For the second issue, we propose to train an NER model with adversarial demonstration such that the model is forced to refer to the demonstrations when performing the tagging task. We conduct comprehensive experiments in low-resource NER tasks, and the results demonstrate that our method outperforms a range of methods.\"}\n{'arxiv_id': 'arXiv:2507.15863', 'title': \"eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs\", 'authors': 'Isaac Shi, Zeyuan Li, Fan Liu, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi', 'link': 'https://arxiv.org/abs/2507.15863', 'abstract': 'We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge) Module, a secure and scalable Retrieval-Augmented Generation pipeline designed specifically for enterprise document question answering. Designed and implemented by eSapiens, the system ingests heterogeneous content (PDF, Office, web), splits it into 1,000-token overlapping chunks, and indexes them in a hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via combined vector+BM25 search, reranked with Cohere, and answered by an LLM using CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap, regenerating answers until every claim is grounded. On four LegalBench subsets, 1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank boosts Precision@10 by approximately 7 pp; the verifier raises TRACe Utilization above 0.50 and limits unsupported statements to less than 3%. All components run in containers, enforce end-to-end TLS 1.3 and AES-256. These results demonstrate that the DEREK module delivers accurate, traceable, and production-ready document QA with minimal operational overhead. The module is designed to meet enterprise demands for secure, auditable, and context-faithful retrieval, providing a reliable baseline for high-stakes domains such as legal and finance.'}\n{'arxiv_id': 'arXiv:2507.16806', 'title': 'Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty', 'authors': 'Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas', 'link': 'https://arxiv.org/abs/2507.16806', 'abstract': 'When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models.'}\n{'arxiv_id': 'arXiv:2507.16795', 'title': 'Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning', 'authors': 'Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda', 'link': 'https://arxiv.org/abs/2507.16795', 'abstract': \"Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.\"}\n{'arxiv_id': 'arXiv:2507.16746', 'title': 'Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning', 'authors': 'Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum', 'link': 'https://arxiv.org/abs/2507.16746', 'abstract': \"Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce $\\\\textbf{Zebra-CoT}$, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.\"}\n{'arxiv_id': 'arXiv:2507.16713', 'title': 'Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory', 'authors': 'Guowei Lan, Kaixian Qu, René Zurbrügg, Changan Chen, Christopher E. Mower, Haitham Bou-Ammar, Marco Hutter', 'link': 'https://arxiv.org/abs/2507.16713', 'abstract': 'Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge. This paper presents ExpTeach, a framework that grounds VLMs to physical robots by building a self-generated memory of real-world experiences. In ExpTeach, the VLM autonomously plans actions, verifies outcomes, reflects on failures, and adapts robot behaviors in a closed loop. The self-generated experiences during this process are then summarized into a long-term memory, enabling retrieval of learned knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12 real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of ExpTeach.'}\n{'arxiv_id': 'arXiv:2507.16577', 'title': 'Scaling Linear Attention with Sparse State Expansion', 'authors': 'Yuqi Pan, Yongqi An, Zheng Li, Yuhong Chou, Ruijie Zhu, Xiaohui Wang, Mingxuan Wang, Jinqiao Wang, Guoqi Li', 'link': 'https://arxiv.org/abs/2507.16577', 'abstract': 'The Transformer architecture, despite its widespread success, struggles with long-context scenarios due to quadratic computation and linear memory growth. While various linear attention variants mitigate these efficiency constraints by compressing context into fixed-size states, they often degrade performance in tasks such as in-context retrieval and reasoning. To address this limitation and achieve more effective context compression, we propose two key innovations. First, we introduce a row-sparse update formulation for linear attention by conceptualizing state updating as information classification. This enables sparse state updates via softmax-based top-$k$ hard classification, thereby extending receptive fields and reducing inter-class interference. Second, we present Sparse State Expansion (SSE) within the sparse framework, which expands the contextual state into multiple partitions, effectively decoupling parameter size from state capacity while maintaining the sparse classification paradigm. Our design, supported by efficient parallelized implementations, yields effective classification and discriminative state representations. We extensively validate SSE in both pure linear and hybrid (SSE-H) architectures across language modeling, in-context retrieval, and mathematical reasoning benchmarks. SSE demonstrates strong retrieval performance and scales favorably with state size. Moreover, after reinforcement learning (RL) training, our 2B SSE-H model achieves state-of-the-art mathematical reasoning performance among small reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25, significantly outperforming similarly sized open-source Transformers. These results highlight SSE as a promising and efficient architecture for long-context modeling.'}\n{'arxiv_id': 'arXiv:2507.16534', 'title': 'Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report', 'authors': 'Shanghai AI Lab, Xiaoyang Chen, Yunhao Chen, Zeren Chen, Zhiyun Chen, Hanyun Cui, Yawen Duan, Jiaxuan Guo, Qi Guo, Xuhao Hu, Hong Huang, Lige Huang, Chunxiao Li, Juncheng Li, Qihao Lin, Dongrui Liu, Xinmin Liu, Zicheng Liu, Chaochao Lu, Xiaoya Lu, Jingjing Qu, Qibing Ren, Jing Shao, Jingwei Shi, Jingwei Sun, Peng Wang, Weibing Wang, Jia Xu, Lewen Yan, Xiao Yu, Yi Yu, Boxuan Zhang, Jie Zhang, Weichen Zhang, Zhijie Zheng, Tianyi Zhou, Bowen Zhou', 'link': 'https://arxiv.org/abs/2507.16534', 'abstract': 'To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, this report presents a comprehensive assessment of their frontier risks. Drawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks in seven areas: cyber offense, biological and chemical risks, persuasion and manipulation, uncontrolled autonomous AI R\\\\&D, strategic deception and scheming, self-replication, and collusion. Guided by the \"AI-$45^\\\\circ$ Law,\" we evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow lines\" (early warning indicators) to define risk zones: green (manageable risk for routine deployment and continuous monitoring), yellow (requiring strengthened mitigations and controlled deployment), and red (necessitating suspension of development and/or deployment). Experimental results show that all recent frontier AI models reside in green and yellow zones, without crossing red lines. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\\\\&D risks. For self-replication, and strategic deception and scheming, most models remain in the green zone, except for certain reasoning models in the yellow zone. In persuasion and manipulation, most models are in the yellow zone due to their effective influence on humans. For biological and chemical risks, we are unable to rule out the possibility of most models residing in the yellow zone, although detailed threat modeling and in-depth assessment are required to make further claims. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.'}\n{'arxiv_id': 'arXiv:2507.16518', 'title': 'C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning', 'authors': 'Xiuwei Chen, Wentao Hu, Hanhui Li, Jun Zhou, Zisheng Chen, Meng Cao, Yihan Zeng, Kui Zhang, Yu-Jie Yuan, Jianhua Han, Hang Xu, Xiaodan Liang', 'link': 'https://arxiv.org/abs/2507.16518', 'abstract': 'Recent advances in multimodal large language models (MLLMs) have shown impressive reasoning capabilities. However, further enhancing existing MLLMs necessitates high-quality vision-language datasets with carefully curated task complexities, which are both costly and challenging to scale. Although recent self-improving models that iteratively refine themselves offer a feasible solution, they still suffer from two core challenges: (i) most existing methods augment visual or textual data separately, resulting in discrepancies in data complexity (e.g., over-simplified diagrams paired with redundant textual descriptions); and (ii) the evolution of data and models is also separated, leading to scenarios where models are exposed to tasks with mismatched difficulty levels. To address these issues, we propose C2-Evo, an automatic, closed-loop self-improving framework that jointly evolves both training data and model capabilities. Specifically, given a base dataset and a base model, C2-Evo enhances them by a cross-modal data evolution loop and a data-model evolution loop. The former loop expands the base dataset by generating complex multimodal problems that combine structured textual sub-problems with iteratively specified geometric diagrams, while the latter loop adaptively selects the generated problems based on the performance of the base model, to conduct supervised fine-tuning and reinforcement learning alternately. Consequently, our method continuously refines its model and training data, and consistently obtains considerable performance gains across multiple mathematical reasoning benchmarks. Our code, models, and datasets will be released.'}\n{'arxiv_id': 'arXiv:2507.16463', 'title': 'MMS Player: an open source software for parametric data-driven animation of Sign Language avatars', 'authors': 'Fabrizio Nunnari, Shailesh Mishra, Patrick Gebhard', 'link': 'https://arxiv.org/abs/2507.16463', 'abstract': 'This paper describes the MMS-Player, an open source software able to synthesise sign language animations from a novel sign language representation format called MMS (MultiModal Signstream). The MMS enhances gloss-based representations by adding information on parallel execution of signs, timing, and inflections. The implementation consists of Python scripts for the popular Blender 3D authoring tool and can be invoked via command line or HTTP API. Animations can be rendered as videos or exported in other popular 3D animation exchange formats. The software is freely available under GPL-3.0 license at this https URL.'}\n{'arxiv_id': 'arXiv:2507.16298', 'title': 'WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections', 'authors': 'Gautam Kishore Shahi, Scot A. Hale', 'link': 'https://arxiv.org/abs/2507.16298', 'abstract': \"WhatsApp tiplines, first launched in 2019 to combat misinformation, enable users to interact with fact-checkers to verify misleading content. This study analyzes 580 unique claims (tips) from 451 users, covering both high-resource languages (English, Hindi) and a low-resource language (Telugu) during the 2021 Indian assembly elections using a mixed-method approach. We categorize the claims into three categories, election, COVID-19, and others, and observe variations across languages. We compare content similarity through frequent word analysis and clustering of neural sentence embeddings. We also investigate user overlap across languages and fact-checking organizations. We measure the average time required to debunk claims and inform tipline users. Results reveal similarities in claims across languages, with some users submitting tips in multiple languages to the same fact-checkers. Fact-checkers generally require a couple of days to debunk a new claim and share the results with users. Notably, no user submits claims to multiple fact-checking organizations, indicating that each organization maintains a unique audience. We provide practical recommendations for using tiplines during elections with ethical consideration of users' information.\"}\n{'arxiv_id': 'arXiv:2507.16185', 'title': 'Characterizing Online Activities Contributing to Suicide Mortality among Youth', 'authors': 'Aparna Ananthasubramaniam, Elyse J. Thulin, Viktoryia Kalesnikava, Silas Falde, Jonathan Kertawidjaja, Lily Johns, Alejandro Rodríguez-Putnam, Emma Spring, Kara Zivin, Briana Mezuk', 'link': 'https://arxiv.org/abs/2507.16185', 'abstract': 'The recent rise in youth suicide highlights the urgent need to understand how online experiences contribute to this public health issue. Our mixed-methods approach responds to this challenge by developing a set of themes focused on risk factors for suicide mortality in online spaces among youth ages 10-24, and a framework to model these themes at scale. Using 29,124 open text summaries of death investigations between 2013-2022, we conducted a thematic analysis to identify 12 types of online activities that were considered by investigators or next of kin to be relevant in contextualizing a given suicide death. We then develop a zero-shot learning framework to model these 12 themes at scale, and analyze variation in these themes by decedent characteristics and over time. Our work uncovers several online activities related to harm to self, harm to others, interpersonal interactions, activity levels online, and life events, which correspond to different phases of suicide risk from two prominent suicide theories. We find an association between these themes and decedent characteristics like age, means of death, and interpersonal problems, and many themes became more prevalent during the 2020 COVID-19 lockdowns. While digital spaces have taken some steps to address expressions of suicidality online, our work illustrates the opportunities for developing interventions related to less explicit indicators of suicide risk by combining suicide theories with computational research.'}\n{'arxiv_id': 'arXiv:2507.16145', 'title': 'SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting', 'authors': 'Shuhao Mei, Yongchao Long, Shan Cao, Xiaobo Han, Shijia Geng, Jinbo Sun, Yuxi Zhou, Shenda Hong', 'link': 'https://arxiv.org/abs/2507.16145', 'abstract': 'Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory disease with persistent airflow limitation, is a leading global cause of disability and mortality. Respiratory spirogram time series, routinely collected during pulmonary function tests (PFTs), play a critical role in the early detection of repsiratory diseases and in monitoring lung function over time. However, most current AI models for COPD diagnosis are limited to outputting classification results without providing a rationale for their diagnostic process, while current Large Language Models (LLMs) cannot understand spirograms yet, which severely limits their clinical trust and adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large language model that can understand spirogram. The model extracts morphological features from respiratory curves via a SpiroEncoder and aligns them with PFT numerical values in a unified latent space using a SpiroProjector, ultimately empowering a large language model to generate a comprehensive diagnostic report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC of 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data, it maintained a 100% valid response rate, far surpassing the 13.4% of a text-only model and showcasing the superiority of its multimodal design. This work demonstrates the substantial potential of deeply fusing physiological signals with large language models, establishing a new paradigm for the next generation of interpretable and reliable clinical decision support tools.'}\n{'arxiv_id': 'arXiv:2507.15887', 'title': 'AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?', 'authors': 'Ori Press, Brandon Amos, Haoyu Zhao, Yikai Wu, Samuel K. Ainsworth, Dominik Krupke, Patrick Kidger, Touqir Sajed, Bartolomeo Stellato, Jisun Park, Nathanael Bosch, Eli Meril, Albert Steppi, Arman Zharmagambetov, Fangzhao Zhang, David Perez-Pineiro, Alberto Mercurio, Ni Zhan, Talor Abramovich, Kilian Lieret, Hanlin Zhang, Shirley Huang, Matthias Bethge, Ofir Press', 'link': 'https://arxiv.org/abs/2507.15887', 'abstract': \"Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.\"}\n{'arxiv_id': 'arXiv:2507.15882', 'title': 'Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark', 'authors': 'Goeric Huybrechts, Srikanth Ronanki, Sai Muralidhar Jayanthi, Jack Fitzgerald, Srinivasan Veeravanallur', 'link': 'https://arxiv.org/abs/2507.15882', 'abstract': 'The proliferation of multimodal Large Language Models has significantly advanced the ability to analyze and understand complex data inputs from different modalities. However, the processing of long documents remains under-explored, largely due to a lack of suitable benchmarks. To address this, we introduce Document Haystack, a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. Document Haystack features documents ranging from 5 to 200 pages and strategically inserts pure text or multimodal text+image \"needles\" at various depths within the documents to challenge VLMs\\' retrieval capabilities. Comprising 400 document variants and a total of 8,250 questions, it is supported by an objective, automated evaluation framework. We detail the construction and characteristics of the Document Haystack dataset, present results from prominent VLMs and discuss potential research avenues in this area.'}\n{'arxiv_id': 'arXiv:2507.15874', 'title': 'Why Braking? Scenario Extraction and Reasoning Utilizing LLM', 'authors': 'Yin Wu, Daniel Slieter, Vivek Subramanian, Ahmed Abouelazm, Robin Bohn, J. Marius Zöllner', 'link': 'https://arxiv.org/abs/2507.15874', 'abstract': 'The growing number of ADAS-equipped vehicles has led to a dramatic increase in driving data, yet most of them capture routine driving behavior. Identifying and understanding safety-critical corner cases within this vast dataset remains a significant challenge. Braking events are particularly indicative of potentially hazardous situations, motivating the central question of our research: Why does a vehicle brake? Existing approaches primarily rely on rule-based heuristics to retrieve target scenarios using predefined condition filters. While effective in simple environments such as highways, these methods lack generalization in complex urban settings. In this paper, we propose a novel framework that leverages Large Language Model (LLM) for scenario understanding and reasoning. Our method bridges the gap between low-level numerical signals and natural language descriptions, enabling LLM to interpret and classify driving scenarios. We propose a dual-path scenario retrieval that supports both category-based search for known scenarios and embedding-based retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset. Experimental results show that our method outperforms rule-based baselines and generalizes well to OOD scenarios.'}\n{'arxiv_id': 'arXiv:2507.15867', 'title': 'RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems', 'authors': 'John Wu, Adam Cross, Jimeng Sun', 'link': 'https://arxiv.org/abs/2507.15867', 'abstract': 'Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail to capture these conditions in electronic health records (EHR), leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities. We present Rare Disease Mining Agents (RDMA), a framework that mirrors how medical experts identify rare disease patterns in EHR. RDMA connects scattered clinical observations that together suggest specific rare conditions. By handling clinical abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware, RDMA reduces privacy risks while improving F1 performance by upwards of 30\\\\% and decreasing inferences costs 10-fold. This approach helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients. Available at this https URL.'}",
      "language": "jsonl",
      "lines": 69,
      "path": "23-Jul-2025/NLP/papers.jsonl",
      "rank": 10,
      "repository": "CSQianDong/Awesome-arXiv-Daily-Reporter",
      "stars": 42,
      "url": "https://github.com/CSQianDong/Awesome-arXiv-Daily-Reporter/blob/0d10ba6301b7a83c4207a0535e67a33054f9ba42/23-Jul-2025/NLP/papers.jsonl"
    }
  ],
  "ranked": [
    {
      "path": "wordWeights.txt",
      "repository": "callum-oakley/gotta-go-fast",
      "score": 1,
      "stars": 299,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/97883596/contents/wordWeights.txt?ref=195b736b4a54261f01353150df9ff5dd5b3ad21f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "names\t1191\nissue\t1191\norders\t1190",
          "matches": [
            {
              "indices": [
                11,
                16
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/97883596/contents/wordWeights.txt?ref=195b736b4a54261f01353150df9ff5dd5b3ad21f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "holland\t93\ngemini\t93\ngaines\t93",
          "matches": [
            {
              "indices": [
                11,
                17
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/callum-oakley/gotta-go-fast/blob/195b736b4a54261f01353150df9ff5dd5b3ad21f/wordWeights.txt",
      "qualityScore": 0.3052121254719663,
      "rank": 1
    },
    {
      "path": "clark_clusters_wnut_and_hege/clark_clusters.32.txt",
      "repository": "napsternxg/TwitterNER",
      "score": 1,
      "stars": 140,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/68426556/contents/clark_clusters_wnut_and_hege/clark_clusters.32.txt?ref=60f4cc81cb0afcc36d86b6f02dbf44daef284fa9",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "bay 11 0.00265428\nissue 11 0.00265428\nwireless 19 0.00319489",
          "matches": [
            {
              "indices": [
                18,
                23
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/68426556/contents/clark_clusters_wnut_and_hege/clark_clusters.32.txt?ref=60f4cc81cb0afcc36d86b6f02dbf44daef284fa9",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "kern 31 0.00010686\ngemini 31 0.00010686\nemirates 31 0.00010686",
          "matches": [
            {
              "indices": [
                19,
                25
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/napsternxg/TwitterNER/blob/60f4cc81cb0afcc36d86b6f02dbf44daef284fa9/clark_clusters_wnut_and_hege/clark_clusters.32.txt",
      "qualityScore": 0.272421911265538,
      "rank": 2
    },
    {
      "path": "relevant_lower.txt",
      "repository": "pry0cc/relevant-wordlist",
      "score": 1,
      "stars": 121,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/359999612/contents/relevant_lower.txt?ref=a10b8272741390451619bac5a90dcd87a1753884",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "gem\ngemini\ngemitaiz",
          "matches": [
            {
              "indices": [
                4,
                10
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/359999612/contents/relevant_lower.txt?ref=a10b8272741390451619bac5a90dcd87a1753884",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "sign-up\nsignal\nsignaling",
          "matches": [
            {
              "indices": [
                8,
                14
              ],
              "text": "signal"
            },
            {
              "indices": [
                15,
                21
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/pry0cc/relevant-wordlist/blob/a10b8272741390451619bac5a90dcd87a1753884/relevant_lower.txt",
      "qualityScore": 0.2661359830674749,
      "rank": 3
    },
    {
      "path": "datasets/nsf/nsfvocab.txt",
      "repository": "ericproffitt/TopicModelsVB.jl",
      "score": 1,
      "stars": 83,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/61924430/contents/datasets/nsf/nsfvocab.txt?ref=a91ae62af7fac40d27eb4776be8c79c59bac96b7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "9167\tgeman\n9168\tgemini\n9169\tgemstone",
          "matches": [
            {
              "indices": [
                16,
                22
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/61924430/contents/datasets/nsf/nsfvocab.txt?ref=a91ae62af7fac40d27eb4776be8c79c59bac96b7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "20679\tsign\n20680\tsignal\n20681\tsignaled",
          "matches": [
            {
              "indices": [
                17,
                23
              ],
              "text": "signal"
            },
            {
              "indices": [
                30,
                36
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/ericproffitt/TopicModelsVB.jl/blob/a91ae62af7fac40d27eb4776be8c79c59bac96b7/datasets/nsf/nsfvocab.txt",
      "qualityScore": 0.2599279286061882,
      "rank": 4
    },
    {
      "path": "Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp",
      "repository": "ddmcdonald/sparser",
      "score": 1,
      "stars": 62,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/41226934/contents/Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp?ref=bd784c216aa1f8984c7ed40e3ede87dc139dcff8",
          "object_type": "FileContent",
          "property": "content",
          "fragment": " \"karyopherin alpha6\" \"Karyopherin-α1\" \"KC734549\" \"kcal/mol\" \"KCT0001803\" \"kg/day\" \"KH2PO4\"\n \"kidney14\" \"kidneys14\" \"killed/inactivated\" \"Kimura-2\" \"kinase/signal\" \"Kirschner-wire\"\n \"Kishino-Hasegawa\" \"Kit v3-cBot-HS\" \"KJ660346.1\" \"KJ660347.1\" \"KJ660348.1\" \"Klenow-fill\"",
          "matches": [
            {
              "indices": [
                157,
                163
              ],
              "text": "ignal\""
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/41226934/contents/Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp?ref=bd784c216aa1f8984c7ed40e3ede87dc139dcff8",
          "object_type": "FileContent",
          "property": "content",
          "fragment": " \"gravity\" \"grinding\" \"grooming\" \"grossly\" \"grounded\" \"grouped\" \"haemolysis\" \"haemopoietic\"\n \"haemorrhagic\" \"hampered\" \"handling\" \"handwashing\" \"happened\" \"harnessing\" \"harvested\" \"hatching\"\n \"hazardous\" \"healing\" \"heightened\" \"helical\" \"hellenic\" \"hemadsorption\" \"hemagglutination\"",
          "matches": [
            {
              "indices": [
                120,
                128
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/ddmcdonald/sparser/blob/bd784c216aa1f8984c7ed40e3ede87dc139dcff8/Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp",
      "qualityScore": 0.24743405494535817,
      "rank": 5
    },
    {
      "path": "output/ncbi/word_vocabulary.dict",
      "repository": "tigerchen52/Biomedical-Entity-Linking",
      "score": 1,
      "stars": 53,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/289109219/contents/output/ncbi/word_vocabulary.dict?ref=462085a663e8e0b0198eb75dba1805dd1658a5a4",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "15243\twinegrad\n15244\thandling\n15245\tidms",
          "matches": [
            {
              "indices": [
                21,
                29
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/289109219/contents/output/ncbi/word_vocabulary.dict?ref=462085a663e8e0b0198eb75dba1805dd1658a5a4",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "16006\ttsutsugamushi\n16007\tsegment\n16008\tmantle",
          "matches": [
            {
              "indices": [
                20,
                25
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/tigerchen52/Biomedical-Entity-Linking/blob/462085a663e8e0b0198eb75dba1805dd1658a5a4/output/ncbi/word_vocabulary.dict",
      "qualityScore": 0.24073937598229686,
      "rank": 6
    },
    {
      "path": "nanogenmo-2018/tvwl.txt",
      "repository": "enkiv2/misc",
      "score": 1,
      "stars": 63,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/58768663/contents/nanogenmo-2018/tvwl.txt?ref=825690f1ed14591a8f808fe9fe2c3201e20d0a66",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1616       names              1191\n1617       issue              1191\n1618       orders             1190",
          "matches": [
            {
              "indices": [
                46,
                51
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/58768663/contents/nanogenmo-2018/tvwl.txt?ref=825690f1ed14591a8f808fe9fe2c3201e20d0a66",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "8886 holland          93\n8887 gemini           93\n8888 gaines           93",
          "matches": [
            {
              "indices": [
                30,
                36
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/enkiv2/misc/blob/825690f1ed14591a8f808fe9fe2c3201e20d0a66/nanogenmo-2018/tvwl.txt",
      "qualityScore": 0.23811799739838874,
      "rank": 7
    },
    {
      "path": "Pope/Gemini (Deep Research)",
      "repository": "jaldps/ai-tests",
      "score": 1,
      "stars": 57,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/939048262/contents/Pope/Gemini%20(Deep%20Research)?ref=b293470791fc799f742eb0186dd4c0ea93fb2c9a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Governance Reform:\nThe Catholic Church has been engaged in ongoing efforts to reform the Roman Curia, aiming for greater transparency, accountability, and efficiency, particularly in areas concerning finance and the handling of abuse cases.130 The next Pope will need to continue these reforms and foster a culture of good governance within the Vatican.\nCardinal Pietro Parolin's extensive tenure as Secretary of State provides him with an intimate understanding of the Curia and its internal workings.4 Having been part of Pope Francis' Council of Cardinals advising on Church reform, he possesses firsthand knowledge of the reforms already implemented and the challenges that remain.14 His experience could be instrumental in navigating the complexities of further Curial reform, though his approach might be characterized by a degree of caution given his long-standing role within the existing system.",
          "matches": [
            {
              "indices": [
                216,
                224
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/939048262/contents/Pope/Gemini%20(Deep%20Research)?ref=b293470791fc799f742eb0186dd4c0ea93fb2c9a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Vatican puts the brakes on Synod on Synodality, pushes 'controversial' topics to 2025, accessed April 26, 2025, https://religionnews.com/2024/03/14/vatican-pulls-the-break-on-synod-on-synodality-pushes-hot-topics-to-2025/\nWhat is your biggest issue with the Catholic faith? : r/Christianity - Reddit, accessed April 26, 2025, https://www.reddit.com/r/Christianity/comments/p2kbkm/what_is_your_biggest_issue_with_the_catholic_faith/\n5 Obstacles to Evangelization We Can Overcome - Benedictine College Media & Culture, accessed April 26, 2025, https://media.benedictine.edu/5-obstacles-to-evangelization-we-can-overcome",
          "matches": [
            {
              "indices": [
                243,
                248
              ],
              "text": "issue"
            },
            {
              "indices": [
                401,
                406
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/jaldps/ai-tests/blob/b293470791fc799f742eb0186dd4c0ea93fb2c9a/Pope/Gemini%20(Deep%20Research)",
      "qualityScore": 0.23384279935629376,
      "rank": 8
    },
    {
      "path": "packages/c/chromium/.rev",
      "repository": "bmwiedemann/openSUSE",
      "score": 1,
      "stars": 43,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/207833170/contents/packages/c/chromium/.rev?ref=0c712b289cf686e81809f6cdea26b8e6993c2ac2",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "  * CVE-2020-6567: Insufficient validation of untrusted input in command line handling.\r\n  * CVE-2020-6568: Insufficient policy enforcement in intent handling.\r\n  * CVE-2020-6569: Integer overflow in WebUSB.\r",
          "matches": [
            {
              "indices": [
                78,
                86
              ],
              "text": "handling"
            },
            {
              "indices": [
                150,
                158
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/207833170/contents/packages/c/chromium/.rev?ref=0c712b289cf686e81809f6cdea26b8e6993c2ac2",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "  * CVE-2025-8578: Use after free in Cast\n  * CVE-2025-8579: Inappropriate implementation in Gemini Live in Chrome\n  * CVE-2025-8580: Inappropriate implementation in Filesystems",
          "matches": [
            {
              "indices": [
                93,
                99
              ],
              "text": "Gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/bmwiedemann/openSUSE/blob/0c712b289cf686e81809f6cdea26b8e6993c2ac2/packages/c/chromium/.rev",
      "qualityScore": 0.23184526764861874,
      "rank": 9
    },
    {
      "path": "23-Jul-2025/NLP/papers.jsonl",
      "repository": "CSQianDong/Awesome-arXiv-Daily-Reporter",
      "score": 1,
      "stars": 42,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/905516268/contents/23-Jul-2025/NLP/papers.jsonl?ref=0d10ba6301b7a83c4207a0535e67a33054f9ba42",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "{'arxiv_id': 'arXiv:2507.16248', 'title': 'FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents', 'authors': 'Run Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, Zhengwen Qiu', 'link': 'https://arxiv.org/abs/2507.16248', 'abstract': 'Recently, AI agents are rapidly evolving in intelligence and widely used in professional research applications, such as STEM, software development, finance, etc. Among these AI agents, deep research agent is a key category as it can perform long-horizon tasks and solve problems of greater complexity. However, there are few evaluation frameworks and benchmarks that systematically and automatically investigate the capabilities of these research agents. Furthermore, financial research problems have distinct complexity and subtlety. To fill in the gap, we propose FinResearchBench, which is a logic tree based Agent-as-a-Judge and targets specifically for the financial research agents. It provides a comprehensive and automatic assessment of the research agents across 7 key types of tasks in the financial research domain. The contributions of this work are two-folded: (1) the first and innovative Agent-as-a-Judge system that extracts the logic tree of the research outcome and uses it as the intermediate information to present a comprehensive, reliable and robust evaluation; (2) finance oriented that it covers 70 typical financial research questions, spreading across 7 frequently encountered types of tasks in the domain.'}\n{'arxiv_id': 'arXiv:2507.16217', 'title': 'Towards Compute-Optimal Many-Shot In-Context Learning', 'authors': 'Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister', 'link': 'https://arxiv.org/abs/2507.16217', 'abstract': 'Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.'}\n{'arxiv_id': 'arXiv:2507.16199', 'title': 'WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability', 'authors': 'Zipeng Ling, Yuehao Tang, Shuliang Liu, Junqi Yang, Shenghong Fu, Yao Wan, Kejia Huang, Zhichao Hou, Xuming Hu', 'link': 'https://arxiv.org/abs/2507.16199', 'abstract': 'Large Language Models (LLMs) frequently output the label \\\\emph{Unknown}, yet current evaluations focus almost exclusively on whether such answers are \\\\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an input that is genuinely indeterminate and (ii) a solvable problem that the model fails to resolve. We call this phenomenon \\\\emph{Vague Perception}. And thus we introduce a framework that quantifies the proportion of \\\\emph{Unknown} responses attributable to model incapacity and tests whether guided stimulation can convert them into either correct (\\\\emph{Known}) or intrinsically indeterminate outcomes. By separating these sources of uncertainty, our method provides a clearer picture of LLM reasoning limits and their potential for improvement. As we get a theoretical accuracy of reasoning task on different LLMs, we apply different methods to test whether the model can reach the accuracy given a baseline framework. Our work is meaningful in exploring the true reasoning ability of LLMs and providing a new perspective on solving the \\\\emph{Vague Perception} phenomenon.'}",
          "matches": [
            {
              "indices": [
                2933,
                2939
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/905516268/contents/23-Jul-2025/NLP/papers.jsonl?ref=0d10ba6301b7a83c4207a0535e67a33054f9ba42",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "{'arxiv_id': 'arXiv:2507.15882', 'title': 'Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark', 'authors': 'Goeric Huybrechts, Srikanth Ronanki, Sai Muralidhar Jayanthi, Jack Fitzgerald, Srinivasan Veeravanallur', 'link': 'https://arxiv.org/abs/2507.15882', 'abstract': 'The proliferation of multimodal Large Language Models has significantly advanced the ability to analyze and understand complex data inputs from different modalities. However, the processing of long documents remains under-explored, largely due to a lack of suitable benchmarks. To address this, we introduce Document Haystack, a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. Document Haystack features documents ranging from 5 to 200 pages and strategically inserts pure text or multimodal text+image \"needles\" at various depths within the documents to challenge VLMs\\' retrieval capabilities. Comprising 400 document variants and a total of 8,250 questions, it is supported by an objective, automated evaluation framework. We detail the construction and characteristics of the Document Haystack dataset, present results from prominent VLMs and discuss potential research avenues in this area.'}\n{'arxiv_id': 'arXiv:2507.15874', 'title': 'Why Braking? Scenario Extraction and Reasoning Utilizing LLM', 'authors': 'Yin Wu, Daniel Slieter, Vivek Subramanian, Ahmed Abouelazm, Robin Bohn, J. Marius Zöllner', 'link': 'https://arxiv.org/abs/2507.15874', 'abstract': 'The growing number of ADAS-equipped vehicles has led to a dramatic increase in driving data, yet most of them capture routine driving behavior. Identifying and understanding safety-critical corner cases within this vast dataset remains a significant challenge. Braking events are particularly indicative of potentially hazardous situations, motivating the central question of our research: Why does a vehicle brake? Existing approaches primarily rely on rule-based heuristics to retrieve target scenarios using predefined condition filters. While effective in simple environments such as highways, these methods lack generalization in complex urban settings. In this paper, we propose a novel framework that leverages Large Language Model (LLM) for scenario understanding and reasoning. Our method bridges the gap between low-level numerical signals and natural language descriptions, enabling LLM to interpret and classify driving scenarios. We propose a dual-path scenario retrieval that supports both category-based search for known scenarios and embedding-based retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset. Experimental results show that our method outperforms rule-based baselines and generalizes well to OOD scenarios.'}\n{'arxiv_id': 'arXiv:2507.15867', 'title': 'RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems', 'authors': 'John Wu, Adam Cross, Jimeng Sun', 'link': 'https://arxiv.org/abs/2507.15867', 'abstract': 'Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail to capture these conditions in electronic health records (EHR), leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities. We present Rare Disease Mining Agents (RDMA), a framework that mirrors how medical experts identify rare disease patterns in EHR. RDMA connects scattered clinical observations that together suggest specific rare conditions. By handling clinical abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware, RDMA reduces privacy risks while improving F1 performance by upwards of 30\\\\% and decreasing inferences costs 10-fold. This approach helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients. Available at this https URL.'}",
          "matches": [
            {
              "indices": [
                2406,
                2412
              ],
              "text": "ignals"
            },
            {
              "indices": [
                3321,
                3324
              ],
              "text": "lin"
            },
            {
              "indices": [
                3480,
                3483
              ],
              "text": "lin"
            },
            {
              "indices": [
                3664,
                3667
              ],
              "text": "lin"
            },
            {
              "indices": [
                3737,
                3745
              ],
              "text": "andling "
            },
            {
              "indices": [
                3746,
                3749
              ],
              "text": "lin"
            },
            {
              "indices": [
                4012,
                4015
              ],
              "text": "lin"
            }
          ]
        }
      ],
      "url": "https://github.com/CSQianDong/Awesome-arXiv-Daily-Reporter/blob/0d10ba6301b7a83c4207a0535e67a33054f9ba42/23-Jul-2025/NLP/papers.jsonl",
      "qualityScore": 0.23084684555795867,
      "rank": 10
    }
  ],
  "results": [
    {
      "path": "src/content/issues/24-12-11-ainews-google-wakes-up-gemini-20-et-al.md",
      "repository": "smol-ai/ainews-web-2025",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/971669456/contents/src/content/issues/24-12-11-ainews-google-wakes-up-gemini-20-et-al.md?ref=f02123e0cfadd1d72ea9ea21cb389a85333bbd17",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\n- **Google's Gemini 2.0 Launch**: Google DeepMind unveiled **Gemini 2.0**, introducing an experimental version called **Gemini 2.0 Flash**, featuring enhanced multimodal output and improved performance, as highlighted in their [official announcement](https://x.com/GoogleDeepMind/status/1866869343570608557).\n   - Gemini 2.0 aims to pave the way for new **agentic experiences** with its **tool use capabilities**.",
          "matches": [
            {
              "indices": [
                14,
                20
              ],
              "text": "Gemini"
            },
            {
              "indices": [
                62,
                68
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/971669456/contents/src/content/issues/24-12-11-ainews-google-wakes-up-gemini-20-et-al.md?ref=f02123e0cfadd1d72ea9ea21cb389a85333bbd17",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "- **Backtrack_to Attribute Error Troubleshooting**: One member shared an error related to **'backtrack_to'** not being an attribute of **Settings** in DSPy and sought help on resolving it.\n   - Another user indicated that the issue had been **resolved earlier** and was likely tied to some **async usage**.\n- **Discussion on Video and Audio IO**: A user posed a question regarding opinions on **video and audio IO**, prompting a discussion among members.",
          "matches": [
            {
              "indices": [
                226,
                231
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/smol-ai/ainews-web-2025/blob/f02123e0cfadd1d72ea9ea21cb389a85333bbd17/src/content/issues/24-12-11-ainews-google-wakes-up-gemini-20-et-al.md"
    },
    {
      "path": "Pope/Gemini (Deep Research)",
      "repository": "jaldps/ai-tests",
      "score": 1,
      "stars": 57,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/939048262/contents/Pope/Gemini%20(Deep%20Research)?ref=b293470791fc799f742eb0186dd4c0ea93fb2c9a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Governance Reform:\nThe Catholic Church has been engaged in ongoing efforts to reform the Roman Curia, aiming for greater transparency, accountability, and efficiency, particularly in areas concerning finance and the handling of abuse cases.130 The next Pope will need to continue these reforms and foster a culture of good governance within the Vatican.\nCardinal Pietro Parolin's extensive tenure as Secretary of State provides him with an intimate understanding of the Curia and its internal workings.4 Having been part of Pope Francis' Council of Cardinals advising on Church reform, he possesses firsthand knowledge of the reforms already implemented and the challenges that remain.14 His experience could be instrumental in navigating the complexities of further Curial reform, though his approach might be characterized by a degree of caution given his long-standing role within the existing system.",
          "matches": [
            {
              "indices": [
                216,
                224
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/939048262/contents/Pope/Gemini%20(Deep%20Research)?ref=b293470791fc799f742eb0186dd4c0ea93fb2c9a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Vatican puts the brakes on Synod on Synodality, pushes 'controversial' topics to 2025, accessed April 26, 2025, https://religionnews.com/2024/03/14/vatican-pulls-the-break-on-synod-on-synodality-pushes-hot-topics-to-2025/\nWhat is your biggest issue with the Catholic faith? : r/Christianity - Reddit, accessed April 26, 2025, https://www.reddit.com/r/Christianity/comments/p2kbkm/what_is_your_biggest_issue_with_the_catholic_faith/\n5 Obstacles to Evangelization We Can Overcome - Benedictine College Media & Culture, accessed April 26, 2025, https://media.benedictine.edu/5-obstacles-to-evangelization-we-can-overcome",
          "matches": [
            {
              "indices": [
                243,
                248
              ],
              "text": "issue"
            },
            {
              "indices": [
                401,
                406
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/jaldps/ai-tests/blob/b293470791fc799f742eb0186dd4c0ea93fb2c9a/Pope/Gemini%20(Deep%20Research)"
    },
    {
      "path": "wordWeights.txt",
      "repository": "callum-oakley/gotta-go-fast",
      "score": 1,
      "stars": 299,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/97883596/contents/wordWeights.txt?ref=195b736b4a54261f01353150df9ff5dd5b3ad21f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "names\t1191\nissue\t1191\norders\t1190",
          "matches": [
            {
              "indices": [
                11,
                16
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/97883596/contents/wordWeights.txt?ref=195b736b4a54261f01353150df9ff5dd5b3ad21f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "holland\t93\ngemini\t93\ngaines\t93",
          "matches": [
            {
              "indices": [
                11,
                17
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/callum-oakley/gotta-go-fast/blob/195b736b4a54261f01353150df9ff5dd5b3ad21f/wordWeights.txt"
    },
    {
      "path": "nanogenmo-2018/tvwl.txt",
      "repository": "enkiv2/misc",
      "score": 1,
      "stars": 63,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/58768663/contents/nanogenmo-2018/tvwl.txt?ref=825690f1ed14591a8f808fe9fe2c3201e20d0a66",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1616       names              1191\n1617       issue              1191\n1618       orders             1190",
          "matches": [
            {
              "indices": [
                46,
                51
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/58768663/contents/nanogenmo-2018/tvwl.txt?ref=825690f1ed14591a8f808fe9fe2c3201e20d0a66",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "8886 holland          93\n8887 gemini           93\n8888 gaines           93",
          "matches": [
            {
              "indices": [
                30,
                36
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/enkiv2/misc/blob/825690f1ed14591a8f808fe9fe2c3201e20d0a66/nanogenmo-2018/tvwl.txt"
    },
    {
      "path": "datasets/nsf/nsfvocab.txt",
      "repository": "ericproffitt/TopicModelsVB.jl",
      "score": 1,
      "stars": 83,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/61924430/contents/datasets/nsf/nsfvocab.txt?ref=a91ae62af7fac40d27eb4776be8c79c59bac96b7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "9167\tgeman\n9168\tgemini\n9169\tgemstone",
          "matches": [
            {
              "indices": [
                16,
                22
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/61924430/contents/datasets/nsf/nsfvocab.txt?ref=a91ae62af7fac40d27eb4776be8c79c59bac96b7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "20679\tsign\n20680\tsignal\n20681\tsignaled",
          "matches": [
            {
              "indices": [
                17,
                23
              ],
              "text": "signal"
            },
            {
              "indices": [
                30,
                36
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/ericproffitt/TopicModelsVB.jl/blob/a91ae62af7fac40d27eb4776be8c79c59bac96b7/datasets/nsf/nsfvocab.txt"
    },
    {
      "path": "logs/text-format/gemini-25-base-1/bayes_cal.out",
      "repository": "Christine8888/replicationbench-release",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/1080132609/contents/logs/text-format/gemini-25-base-1/bayes_cal.out?ref=a87284ced5652645d81b82dcace2f41a7a02d3ce",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "│ may underfit or overfit individual parameters and misidentify systematics or │\n│ information about the signal being measured.                                 │\n│                                                                              │",
          "matches": [
            {
              "indices": [
                111,
                117
              ],
              "text": " being"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/1080132609/contents/logs/text-format/gemini-25-base-1/bayes_cal.out?ref=a87284ced5652645d81b82dcace2f41a7a02d3ce",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "╭──────────────────────────────────────────────────────────────────────────────╮\n│bayes_cal (1 sample): google/gemini-2.5-pro                                   │\n╰──────────────────────────────────────────────────────────────────────────────╯",
          "matches": [
            {
              "indices": [
                273,
                279
              ],
              "text": null
            }
          ]
        }
      ],
      "url": "https://github.com/Christine8888/replicationbench-release/blob/a87284ced5652645d81b82dcace2f41a7a02d3ce/logs/text-format/gemini-25-base-1/bayes_cal.out"
    },
    {
      "path": "README.md",
      "repository": "tycloud97/awesome-stars",
      "score": 1,
      "stars": 32,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/485813895/contents/README.md?ref=1cdd0df9e38816ae09bb3be9d3478aaf2fa218a4",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "| 27 |  [AWS-Python-Scripts](https://github.com/Kerberos1864/AWS-Python-Scripts) | Useful scripts for automation of operations and costs. | Kerberos1864 | 1 |\n| 28 |  [SWE-agent](https://github.com/SWE-agent/SWE-agent) | SWE-agent takes a GitHub issue and tries to automatically fix it, using your LM of choice. It can also be employed for offensive cybersecurity or competitive coding challenges. [NeurIPS 2024] | SWE-agent | 18399 |\n| 29 |  [pdfplumber](https://github.com/jsvine/pdfplumber) | Plumb a PDF for detailed information about each char, rectangle, line, et cetera — and easily extract text and tables. | jsvine | 9660 |",
          "matches": [
            {
              "indices": [
                246,
                251
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/485813895/contents/README.md?ref=1cdd0df9e38816ae09bb3be9d3478aaf2fa218a4",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "| 152 |  [prowler](https://github.com/prowler-cloud/prowler) | Prowler is the world’s most widely used open-source cloud security platform that automates security and compliance across any cloud environment. | prowler-cloud | 12918 |\n| 153 |  [aws-sam-cli](https://github.com/aws/aws-sam-cli) | CLI tool to build, test, debug, and deploy Serverless applications using AWS SAM | aws | 6688 |\n| 154 |  [serverless-application-model](https://github.com/aws/serverless-application-model) | The AWS Serverless Application Model (AWS SAM) transform is a AWS CloudFormation macro that transforms SAM templates into CloudFormation templates. | aws | 9550 |",
          "matches": [
            {
              "indices": [
                254,
                257
              ],
              "text": "i]("
            },
            {
              "indices": [
                290,
                293
              ],
              "text": "i) "
            },
            {
              "indices": [
                297,
                300
              ],
              "text": "I t"
            }
          ]
        }
      ],
      "url": "https://github.com/tycloud97/awesome-stars/blob/1cdd0df9e38816ae09bb3be9d3478aaf2fa218a4/README.md"
    },
    {
      "path": "packages/c/chromium/.rev",
      "repository": "bmwiedemann/openSUSE",
      "score": 1,
      "stars": 43,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/207833170/contents/packages/c/chromium/.rev?ref=0c712b289cf686e81809f6cdea26b8e6993c2ac2",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "  * CVE-2020-6567: Insufficient validation of untrusted input in command line handling.\r\n  * CVE-2020-6568: Insufficient policy enforcement in intent handling.\r\n  * CVE-2020-6569: Integer overflow in WebUSB.\r",
          "matches": [
            {
              "indices": [
                78,
                86
              ],
              "text": "handling"
            },
            {
              "indices": [
                150,
                158
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/207833170/contents/packages/c/chromium/.rev?ref=0c712b289cf686e81809f6cdea26b8e6993c2ac2",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "  * CVE-2025-8578: Use after free in Cast\n  * CVE-2025-8579: Inappropriate implementation in Gemini Live in Chrome\n  * CVE-2025-8580: Inappropriate implementation in Filesystems",
          "matches": [
            {
              "indices": [
                93,
                99
              ],
              "text": "Gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/bmwiedemann/openSUSE/blob/0c712b289cf686e81809f6cdea26b8e6993c2ac2/packages/c/chromium/.rev"
    },
    {
      "path": "clark_clusters_wnut_and_hege/clark_clusters.32.txt",
      "repository": "napsternxg/TwitterNER",
      "score": 1,
      "stars": 140,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/68426556/contents/clark_clusters_wnut_and_hege/clark_clusters.32.txt?ref=60f4cc81cb0afcc36d86b6f02dbf44daef284fa9",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "bay 11 0.00265428\nissue 11 0.00265428\nwireless 19 0.00319489",
          "matches": [
            {
              "indices": [
                18,
                23
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/68426556/contents/clark_clusters_wnut_and_hege/clark_clusters.32.txt?ref=60f4cc81cb0afcc36d86b6f02dbf44daef284fa9",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "kern 31 0.00010686\ngemini 31 0.00010686\nemirates 31 0.00010686",
          "matches": [
            {
              "indices": [
                19,
                25
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/napsternxg/TwitterNER/blob/60f4cc81cb0afcc36d86b6f02dbf44daef284fa9/clark_clusters_wnut_and_hege/clark_clusters.32.txt"
    },
    {
      "path": "shared_vocab.txt",
      "repository": "huangjia2019/DeepBlue-LLM",
      "score": 1,
      "stars": 45,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/647747856/contents/shared_vocab.txt?ref=4f47c0980454fd620482ac3c93f719fe891d634f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "317 5901\nhandling 5902\njudgement 5903",
          "matches": [
            {
              "indices": [
                9,
                17
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/647747856/contents/shared_vocab.txt?ref=4f47c0980454fd620482ac3c93f719fe891d634f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "posthumous 16006\nSavage 16007\nevidently 16008",
          "matches": [
            {
              "indices": [
                24,
                29
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/huangjia2019/DeepBlue-LLM/blob/4f47c0980454fd620482ac3c93f719fe891d634f/shared_vocab.txt"
    },
    {
      "path": "data/2025-04-23.md",
      "repository": "xmkxabc/InsightArxiv",
      "score": 1,
      "stars": 13,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/1023062709/contents/data/2025-04-23.md?ref=adda0529ad648f28b96b58827bd8011fb71fe603",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "over multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
          "matches": [
            {
              "indices": [
                115,
                121
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/1023062709/contents/data/2025-04-23.md?ref=adda0529ad648f28b96b58827bd8011fb71fe603",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "recording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain",
          "matches": [
            {
              "indices": [
                147,
                153
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/xmkxabc/InsightArxiv/blob/adda0529ad648f28b96b58827bd8011fb71fe603/data/2025-04-23.md"
    },
    {
      "path": "data/dictionary.txt",
      "repository": "yaushian/CycleGAN-sentiment-transfer",
      "score": 1,
      "stars": 17,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/122983026/contents/data/dictionary.txt?ref=0ef4e91b5e0c6ce09d5cecac3f85f6f98a041a6e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "torchwood 10208\nhandling 10209\nchewed 10210",
          "matches": [
            {
              "indices": [
                16,
                24
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/122983026/contents/data/dictionary.txt?ref=0ef4e91b5e0c6ce09d5cecac3f85f6f98a041a6e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "smokes 10434\ngemini 10435\ncontracts 10436",
          "matches": [
            {
              "indices": [
                13,
                19
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/yaushian/CycleGAN-sentiment-transfer/blob/0ef4e91b5e0c6ce09d5cecac3f85f6f98a041a6e/data/dictionary.txt"
    },
    {
      "path": "data/word_index.txt",
      "repository": "UIUC-data-mining/KERT",
      "score": 1,
      "stars": 10,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/41654580/contents/data/word_index.txt?ref=8897564d853d9d5ca3874999f6f60b04e080a75d",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "404\tsequence\n405\thandling\n406\tmissing",
          "matches": [
            {
              "indices": [
                17,
                25
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/41654580/contents/data/word_index.txt?ref=8897564d853d9d5ca3874999f6f60b04e080a75d",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1878\tacoustic\n1879\tsignal\n1880\thelicoverpa",
          "matches": [
            {
              "indices": [
                19,
                25
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/UIUC-data-mining/KERT/blob/8897564d853d9d5ca3874999f6f60b04e080a75d/data/word_index.txt"
    },
    {
      "path": "pages/2025-04-23-cs-cl.html",
      "repository": "Luvata/arxive",
      "score": 1,
      "stars": 14,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/662095071/contents/pages/2025-04-23-cs-cl.html?ref=bae2c4360898b1f574cb5616f48de58eaec3ca7b",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "<p>Abstract: In this paper, we describe our participation in the RuTermEval competition devoted to extracting nested terms. We apply the Binder model, which was previously successfully applied to the recognition of nested named entities, to extract nested terms. We obtained the best results of term recognition in all three tracks of the RuTermEval competition. In addition, we study the new task of recognition of nested terms from flat training data annotated with terms without nestedness. We can conclude that several approaches we proposed in this work are viable enough to retrieve nested terms effectively without nested labeling of them.</p>\n<button class=\"copy-button\" onclick=\"copyToClipboard('https://arxiv.org/abs/2504.16007', 31)\">Copy Link</button>\n<div id=\"copy-message-31\" class=\"copy-message\"></div>",
          "matches": [
            {
              "indices": [
                681,
                684
              ],
              "text": "cli"
            },
            {
              "indices": [
                694,
                697
              ],
              "text": "Cli"
            },
            {
              "indices": [
                732,
                737
              ],
              "text": "16007"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/662095071/contents/pages/2025-04-23-cs-cl.html?ref=bae2c4360898b1f574cb5616f48de58eaec3ca7b",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "<p><b>Authors:</b> Dustin Wright, Isabelle Augenstein</p>\n<p>Abstract: Selecting an effective training signal for machine learning tasks is difficult: expert annotations are expensive, and crowd-sourced annotations may not be reliable. Recent work has demonstrated that learning from a distribution over labels acquired from crowd annotations can be effective both for performance and uncertainty estimation. However, this has mainly been studied using a limited set of soft-labeling methods in an in-domain setting. Additionally, no one method has been shown to consistently perform well across tasks, making it difficult to know a priori which to choose. To fill these gaps, this paper provides the first large-scale empirical study on learning from crowd labels in the out-of-domain setting, systematically analyzing 8 soft-labeling methods on 4 language and vision tasks. Additionally, we propose to aggregate soft-labels via a simple average in order to achieve consistent performance across tasks. We demonstrate that this yields classifiers with improved predictive uncertainty estimation in most settings while maintaining consistent raw performance compared to learning from individual soft-labeling methods or taking a majority vote of the annotations. We additionally highlight that in regimes with abundant or minimal training data, the selection of soft labeling method is less important, while for highly subjective labels and moderate amounts of training data, aggregation yields significant improvements in uncertainty estimation over individual methods. Code can be found at https://github.com/copenlu/aggregating-crowd-annotations-ood.</p>\n<p>URLs: <a href=\"https://github.com/copenlu/aggregating-crowd-annotations-ood.\">https://github.com/copenlu/aggregating-crowd-annotations-ood.</a></p>",
          "matches": [
            {
              "indices": [
                103,
                109
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/Luvata/arxive/blob/bae2c4360898b1f574cb5616f48de58eaec3ca7b/pages/2025-04-23-cs-cl.html"
    },
    {
      "path": "output/ncbi/word_vocabulary.dict",
      "repository": "tigerchen52/Biomedical-Entity-Linking",
      "score": 1,
      "stars": 53,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/289109219/contents/output/ncbi/word_vocabulary.dict?ref=462085a663e8e0b0198eb75dba1805dd1658a5a4",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "15243\twinegrad\n15244\thandling\n15245\tidms",
          "matches": [
            {
              "indices": [
                21,
                29
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/289109219/contents/output/ncbi/word_vocabulary.dict?ref=462085a663e8e0b0198eb75dba1805dd1658a5a4",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "16006\ttsutsugamushi\n16007\tsegment\n16008\tmantle",
          "matches": [
            {
              "indices": [
                20,
                25
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/tigerchen52/Biomedical-Entity-Linking/blob/462085a663e8e0b0198eb75dba1805dd1658a5a4/output/ncbi/word_vocabulary.dict"
    },
    {
      "path": "LLM_TRAINING_DATA_RESTRICTIONS_REPORT.md",
      "repository": "erichowens/some_claude_skills",
      "score": 1,
      "stars": 25,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/1097352193/contents/LLM_TRAINING_DATA_RESTRICTIONS_REPORT.md?ref=87853d6835a2d4dfbe22bd53a17e1d2ce58c99ed",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\n### 4.3 Paywalled Content Handling\n",
          "matches": [
            {
              "indices": [
                27,
                35
              ],
              "text": "Handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/1097352193/contents/LLM_TRAINING_DATA_RESTRICTIONS_REPORT.md?ref=87853d6835a2d4dfbe22bd53a17e1d2ce58c99ed",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "**Training Data Diversity Issues:**\n- Issue stems primarily from significant imbalances in training data across **linguistic, geographic, and cultural dimensions**\n- LLMs trained on high-resource languages struggle to generalize to low-resource languages",
          "matches": [
            {
              "indices": [
                26,
                31
              ],
              "text": "Issue"
            },
            {
              "indices": [
                38,
                43
              ],
              "text": "Issue"
            }
          ]
        }
      ],
      "url": "https://github.com/erichowens/some_claude_skills/blob/87853d6835a2d4dfbe22bd53a17e1d2ce58c99ed/LLM_TRAINING_DATA_RESTRICTIONS_REPORT.md"
    },
    {
      "path": "html/user_3/2025-05-23.html",
      "repository": "wwd29/arxiv-daily",
      "score": 1,
      "stars": 19,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/482004183/contents/html/user_3/2025-05-23.html?ref=21c1faea2083a7b57816d5d294082de9809547b7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "<li><strong>Keywords: </strong>generation</a></li>\n<li><strong>Abstract: </strong>With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: this https URL</li>\n<li><strong>摘要：</strong>随着视频游戏现在在娱乐业中获得最高收入，优化游戏开发工作流程对于该行业持续增长至关重要。视觉模型（VLMS）的最新进展为自动化和增强游戏开发的各个方面（尤其是质量保证（QA））提供了巨大的潜力，尤其是质量保证（QA），这仍然是该行业最劳动密集型的流程之一，自动化选项有限。为了准确评估VLM在视频游戏QA任务中的性能并确定其在处理实际场景中的有效性，因此明显需要标准化的基准测试，因为现有基准不足以满足该域的特定要求。为了弥合这一差距，我们介绍了VideoGameqa-Bench，这是一个全面的基准测试，涵盖了各种各样的游戏质量检查活动，包括视觉单元测试，视觉回归测试，核对面的针刺任务，小故障检测以及针对各种游戏的图像和视频的错误报告生成。代码和数据可用：此HTTPS URL</li>",
          "matches": [
            {
              "indices": [
                647,
                655
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/482004183/contents/html/user_3/2025-05-23.html?ref=21c1faea2083a7b57816d5d294082de9809547b7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "<li><strong>Keywords: </strong>generative</a></li>\n<li><strong>Abstract: </strong>Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: this https URL</li>\n<li><strong>摘要：</strong>Generative AI（Genai）具有自动化日常图像编辑任务的巨大希望，尤其是在2025年3月25日GPT-4O发行后，人们最经常想编辑哪些主题？他们想执行什么样的编辑操作（例如，删除或样式化主题）？人们是否喜欢具有可预测结果的精确编辑？通过了解实际请求的特征以及自由照片编辑向导进行的相应编辑，我们可以绘制用于改进基于AI的编辑的课程，并确定当前可以通过AI编辑成功处理哪些类型的请求？在本文中，我们提出了一项独特的研究，通过分析过去12年（2013  -  2025年）对Reddit社区的83K请求，该研究收集了305K PSR-Wizard编辑。根据人类评分，最佳的AI编辑者（包括GPT-4O，Gemini-2.0-Flash，Seededit）只能满足大约33％的请求。有趣的是，AI编辑者在需要精确编辑的低创造性请求方面的表现要差，而不是在开放式任务上。他们经常难以保留人和动物的身份，并经常进行无需进行修饰。在桌子的另一侧，VLM法官（例如O1）的表现不同于人类法官，并且可能更喜欢AI的编辑，而不是人类的编辑。代码和定性示例可提供：此HTTPS URL</li>",
          "matches": [
            {
              "indices": [
                1066,
                1072
              ],
              "text": "Gemini"
            },
            {
              "indices": [
                2323,
                2329
              ],
              "text": null
            }
          ]
        }
      ],
      "url": "https://github.com/wwd29/arxiv-daily/blob/21c1faea2083a7b57816d5d294082de9809547b7/html/user_3/2025-05-23.html"
    },
    {
      "path": "23-Jul-2025/NLP/papers.jsonl",
      "repository": "CSQianDong/Awesome-arXiv-Daily-Reporter",
      "score": 1,
      "stars": 42,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/905516268/contents/23-Jul-2025/NLP/papers.jsonl?ref=0d10ba6301b7a83c4207a0535e67a33054f9ba42",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "{'arxiv_id': 'arXiv:2507.16248', 'title': 'FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents', 'authors': 'Run Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, Zhengwen Qiu', 'link': 'https://arxiv.org/abs/2507.16248', 'abstract': 'Recently, AI agents are rapidly evolving in intelligence and widely used in professional research applications, such as STEM, software development, finance, etc. Among these AI agents, deep research agent is a key category as it can perform long-horizon tasks and solve problems of greater complexity. However, there are few evaluation frameworks and benchmarks that systematically and automatically investigate the capabilities of these research agents. Furthermore, financial research problems have distinct complexity and subtlety. To fill in the gap, we propose FinResearchBench, which is a logic tree based Agent-as-a-Judge and targets specifically for the financial research agents. It provides a comprehensive and automatic assessment of the research agents across 7 key types of tasks in the financial research domain. The contributions of this work are two-folded: (1) the first and innovative Agent-as-a-Judge system that extracts the logic tree of the research outcome and uses it as the intermediate information to present a comprehensive, reliable and robust evaluation; (2) finance oriented that it covers 70 typical financial research questions, spreading across 7 frequently encountered types of tasks in the domain.'}\n{'arxiv_id': 'arXiv:2507.16217', 'title': 'Towards Compute-Optimal Many-Shot In-Context Learning', 'authors': 'Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister', 'link': 'https://arxiv.org/abs/2507.16217', 'abstract': 'Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.'}\n{'arxiv_id': 'arXiv:2507.16199', 'title': 'WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability', 'authors': 'Zipeng Ling, Yuehao Tang, Shuliang Liu, Junqi Yang, Shenghong Fu, Yao Wan, Kejia Huang, Zhichao Hou, Xuming Hu', 'link': 'https://arxiv.org/abs/2507.16199', 'abstract': 'Large Language Models (LLMs) frequently output the label \\\\emph{Unknown}, yet current evaluations focus almost exclusively on whether such answers are \\\\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an input that is genuinely indeterminate and (ii) a solvable problem that the model fails to resolve. We call this phenomenon \\\\emph{Vague Perception}. And thus we introduce a framework that quantifies the proportion of \\\\emph{Unknown} responses attributable to model incapacity and tests whether guided stimulation can convert them into either correct (\\\\emph{Known}) or intrinsically indeterminate outcomes. By separating these sources of uncertainty, our method provides a clearer picture of LLM reasoning limits and their potential for improvement. As we get a theoretical accuracy of reasoning task on different LLMs, we apply different methods to test whether the model can reach the accuracy given a baseline framework. Our work is meaningful in exploring the true reasoning ability of LLMs and providing a new perspective on solving the \\\\emph{Vague Perception} phenomenon.'}",
          "matches": [
            {
              "indices": [
                2933,
                2939
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/905516268/contents/23-Jul-2025/NLP/papers.jsonl?ref=0d10ba6301b7a83c4207a0535e67a33054f9ba42",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "{'arxiv_id': 'arXiv:2507.15882', 'title': 'Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark', 'authors': 'Goeric Huybrechts, Srikanth Ronanki, Sai Muralidhar Jayanthi, Jack Fitzgerald, Srinivasan Veeravanallur', 'link': 'https://arxiv.org/abs/2507.15882', 'abstract': 'The proliferation of multimodal Large Language Models has significantly advanced the ability to analyze and understand complex data inputs from different modalities. However, the processing of long documents remains under-explored, largely due to a lack of suitable benchmarks. To address this, we introduce Document Haystack, a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. Document Haystack features documents ranging from 5 to 200 pages and strategically inserts pure text or multimodal text+image \"needles\" at various depths within the documents to challenge VLMs\\' retrieval capabilities. Comprising 400 document variants and a total of 8,250 questions, it is supported by an objective, automated evaluation framework. We detail the construction and characteristics of the Document Haystack dataset, present results from prominent VLMs and discuss potential research avenues in this area.'}\n{'arxiv_id': 'arXiv:2507.15874', 'title': 'Why Braking? Scenario Extraction and Reasoning Utilizing LLM', 'authors': 'Yin Wu, Daniel Slieter, Vivek Subramanian, Ahmed Abouelazm, Robin Bohn, J. Marius Zöllner', 'link': 'https://arxiv.org/abs/2507.15874', 'abstract': 'The growing number of ADAS-equipped vehicles has led to a dramatic increase in driving data, yet most of them capture routine driving behavior. Identifying and understanding safety-critical corner cases within this vast dataset remains a significant challenge. Braking events are particularly indicative of potentially hazardous situations, motivating the central question of our research: Why does a vehicle brake? Existing approaches primarily rely on rule-based heuristics to retrieve target scenarios using predefined condition filters. While effective in simple environments such as highways, these methods lack generalization in complex urban settings. In this paper, we propose a novel framework that leverages Large Language Model (LLM) for scenario understanding and reasoning. Our method bridges the gap between low-level numerical signals and natural language descriptions, enabling LLM to interpret and classify driving scenarios. We propose a dual-path scenario retrieval that supports both category-based search for known scenarios and embedding-based retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset. Experimental results show that our method outperforms rule-based baselines and generalizes well to OOD scenarios.'}\n{'arxiv_id': 'arXiv:2507.15867', 'title': 'RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems', 'authors': 'John Wu, Adam Cross, Jimeng Sun', 'link': 'https://arxiv.org/abs/2507.15867', 'abstract': 'Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail to capture these conditions in electronic health records (EHR), leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities. We present Rare Disease Mining Agents (RDMA), a framework that mirrors how medical experts identify rare disease patterns in EHR. RDMA connects scattered clinical observations that together suggest specific rare conditions. By handling clinical abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware, RDMA reduces privacy risks while improving F1 performance by upwards of 30\\\\% and decreasing inferences costs 10-fold. This approach helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients. Available at this https URL.'}",
          "matches": [
            {
              "indices": [
                2406,
                2412
              ],
              "text": "ignals"
            },
            {
              "indices": [
                3321,
                3324
              ],
              "text": "lin"
            },
            {
              "indices": [
                3480,
                3483
              ],
              "text": "lin"
            },
            {
              "indices": [
                3664,
                3667
              ],
              "text": "lin"
            },
            {
              "indices": [
                3737,
                3745
              ],
              "text": "andling "
            },
            {
              "indices": [
                3746,
                3749
              ],
              "text": "lin"
            },
            {
              "indices": [
                4012,
                4015
              ],
              "text": "lin"
            }
          ]
        }
      ],
      "url": "https://github.com/CSQianDong/Awesome-arXiv-Daily-Reporter/blob/0d10ba6301b7a83c4207a0535e67a33054f9ba42/23-Jul-2025/NLP/papers.jsonl"
    },
    {
      "path": "pr/all.tsv",
      "repository": "bioconda/bioconda-paper",
      "score": 1,
      "stars": 4,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/98338215/contents/pr/all.tsv?ref=3eb5e8d5976519f04f1ee7c6832a78fb303aabd3",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0\t60195122\t0 days 00:11:59.000000000\tadd a recipe for synapseclient\n1\t60189318\t0 days 00:22:15.000000000\tgemini 0.18.2\n0\t60183635\t0 days 00:07:20.000000000\tadd python2.7 build to backports.csv",
          "matches": [
            {
              "indices": [
                61,
                64
              ],
              "text": "cli"
            },
            {
              "indices": [
                105,
                111
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/98338215/contents/pr/all.tsv?ref=3eb5e8d5976519f04f1ee7c6832a78fb303aabd3",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "5\t52246264\t0 days 00:32:08.000000000\tNew recipes: Spades and MetaSV version of AGE\n1\t52227272\t0 days 00:04:22.000000000\tImprove error handling of anaconda upload.\n3\t52225668\t0 days 00:00:08.000000000\tAdd the R version of the great viridis colormap.",
          "matches": [
            {
              "indices": [
                134,
                142
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/bioconda/bioconda-paper/blob/3eb5e8d5976519f04f1ee7c6832a78fb303aabd3/pr/all.tsv"
    },
    {
      "path": "data/convai2_voacb_idf.txt",
      "repository": "vsharecodes/percvae",
      "score": 1,
      "stars": 29,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/188799166/contents/data/convai2_voacb_idf.txt?ref=2b2150c3be005c0d4095336af246a035064a0afa",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "presses\t0.221834\nhandling\t0.166003\nrogan\t0.286317",
          "matches": [
            {
              "indices": [
                17,
                25
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/188799166/contents/data/convai2_voacb_idf.txt?ref=2b2150c3be005c0d4095336af246a035064a0afa",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "trick\t0.186329\ngemini\t0.247975\nfreely\t0.187975",
          "matches": [
            {
              "indices": [
                15,
                21
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/vsharecodes/percvae/blob/2b2150c3be005c0d4095336af246a035064a0afa/data/convai2_voacb_idf.txt"
    },
    {
      "path": "daily_papers/20250723_Wed/text.md",
      "repository": "advanced-cs/arXiv_daily",
      "score": 1,
      "stars": 23,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/994558424/contents/daily_papers/20250723_Wed/text.md?ref=3cf00a9b1cfe065a3d790f16e7647835791fa448",
          "object_type": "FileContent",
          "property": "content",
          "fragment": ">\n> **摘要:** Can LLMs provide support to creative writers by giving meaningful writing feedback? In this paper, we explore the challenges and limitations of model-generated writing feedback by defining a new task, dataset, and evaluation frameworks. To study model performance in a controlled manner, we present a novel test set of 1,300 stories that we corrupted to intentionally introduce writing issues. We study the performance of commonly used LLMs in this task with both automatic and human evaluation metrics. Our analysis shows that current models have strong out-of-the-box behavior in many respects -- providing specific and mostly accurate writing feedback. However, models often fail to identify the biggest writing issue in the story and to correctly decide when to offer critical vs. positive feedback.\n>",
          "matches": [
            {
              "indices": [
                402,
                407
              ],
              "text": "es. W"
            },
            {
              "indices": [
                731,
                736
              ],
              "text": "e in "
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/994558424/contents/daily_papers/20250723_Wed/text.md?ref=3cf00a9b1cfe065a3d790f16e7647835791fa448",
          "object_type": "FileContent",
          "property": "content",
          "fragment": ">\n> **摘要:** This paper presents a systematic evaluation of state-of-the-art multimodal large language models (MLLMs) on intuitive physics tasks using the GRASP and IntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL, LLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that even the latest models struggle to reliably distinguish physically plausible from implausible scenarios. To go beyond performance metrics, we conduct a probing analysis of model embeddings, extracting intermediate representations at key processing stages to examine how well task-relevant information is preserved. Our results show that, depending on task difficulty, a critical vision-language misalignment can emerge: vision encoders successfully capture physical plausibility cues, but this information is not effectively utilized by the language model, leading to failures in reasoning. This misalignment suggests that the primary limitation of MLLMs in intuitive physics tasks is not the vision component but the ineffective integration of visual and linguistic information. Our findings highlight vision-language alignment as a key area for improvement, offering insights for future MLLMs development.\n>",
          "matches": [
            {
              "indices": [
                285,
                291
              ],
              "text": "ni 2.0"
            }
          ]
        }
      ],
      "url": "https://github.com/advanced-cs/arXiv_daily/blob/3cf00a9b1cfe065a3d790f16e7647835791fa448/daily_papers/20250723_Wed/text.md"
    },
    {
      "path": "daily_papers/20251223_Tue/text.md",
      "repository": "advanced-cs/arXiv_daily",
      "score": 1,
      "stars": 23,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/994558424/contents/daily_papers/20251223_Tue/text.md?ref=3cf00a9b1cfe065a3d790f16e7647835791fa448",
          "object_type": "FileContent",
          "property": "content",
          "fragment": ">\n> **摘要:** Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.\n>",
          "matches": [
            {
              "indices": [
                1013,
                1019
              ],
              "text": "i 2.5 "
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/994558424/contents/daily_papers/20251223_Tue/text.md?ref=3cf00a9b1cfe065a3d790f16e7647835791fa448",
          "object_type": "FileContent",
          "property": "content",
          "fragment": ">\n> **摘要:** Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting \"Lost in Translation,\" where translation steps lead to errors that would have been avoided by question's language reasoning.\n>",
          "matches": [
            {
              "indices": [
                338,
                346
              ],
              "text": "ling of "
            }
          ]
        }
      ],
      "url": "https://github.com/advanced-cs/arXiv_daily/blob/3cf00a9b1cfe065a3d790f16e7647835791fa448/daily_papers/20251223_Tue/text.md"
    },
    {
      "path": "skjul.csv",
      "repository": "mpdn/skjul",
      "score": 1,
      "stars": 16,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/177454769/contents/skjul.csv?ref=fb44b51966a11a1f62303d76ad747f612cc73365",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "absorbed,absorbing,0.26941156\r\nhandling,handled,0.2694407\r\nemperor,emperors,0.2694878\r",
          "matches": [
            {
              "indices": [
                31,
                39
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/177454769/contents/skjul.csv?ref=fb44b51966a11a1f62303d76ad747f612cc73365",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "pulse,bullet,0.51184547\r\nconstellation,gemini,0.51193476\r\npublication,proceedings,0.5119895\r",
          "matches": [
            {
              "indices": [
                39,
                45
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/mpdn/skjul/blob/fb44b51966a11a1f62303d76ad747f612cc73365/skjul.csv"
    },
    {
      "path": "content-org/o.org",
      "repository": "geekodour/o",
      "score": 1,
      "stars": 5,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/507525189/contents/content-org/o.org?ref=db20246c5883c796fd9673eb5c980ddb9df86471",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "- [[https://blog.geekodour.org][blog]] : The blog\n- [[https://cheats.geekodour.org/][cheats]] : CLI cheatsheets. Trust me, they look better on the terminal.\n",
          "matches": [
            {
              "indices": [
                96,
                99
              ],
              "text": "CLI"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/507525189/contents/content-org/o.org?ref=db20246c5883c796fd9673eb5c980ddb9df86471",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "#+begin_quote\n/A document is not necessarily a simulation of paper. In the most general sense, a document is a package of ideas created by human minds and addressed to human minds, intended for the furtherance of those ideas and those minds. Human ideas manifest as text, connections, diagrams and more: thus how to store them and present them is a crucial issue for civilization. ~ Ted Nelson/\n#+end_quote",
          "matches": [
            {
              "indices": [
                357,
                362
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/geekodour/o/blob/db20246c5883c796fd9673eb5c980ddb9df86471/content-org/o.org"
    },
    {
      "path": "A9_urls.txt",
      "repository": "tech234a/annotation-urls",
      "score": 1,
      "stars": 31,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/389307289/contents/A9_urls.txt?ref=5c0300471b87c6a5f3dd8883484f8155b5ff33d7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "https://play.google.com/store/apps/details?id=com.indiatv.livetv&pageId=104380829017797174094\nhttps://www.railstotrails.org/magazine/green-issue-2017/\nhttps://www.patreon.com/cmpuls3",
          "matches": [
            {
              "indices": [
                139,
                144
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/389307289/contents/A9_urls.txt?ref=5c0300471b87c6a5f3dd8883484f8155b5ff33d7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "http://kamogashira.com/hanashikata-school/\nhttp://www.lindyfishingtackle.com/lindy-fish-handling-glove-9359\nhttp://1vam.com/",
          "matches": [
            {
              "indices": [
                88,
                96
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/tech234a/annotation-urls/blob/5c0300471b87c6a5f3dd8883484f8155b5ff33d7/A9_urls.txt"
    },
    {
      "path": "Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp",
      "repository": "ddmcdonald/sparser",
      "score": 1,
      "stars": 62,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/41226934/contents/Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp?ref=bd784c216aa1f8984c7ed40e3ede87dc139dcff8",
          "object_type": "FileContent",
          "property": "content",
          "fragment": " \"karyopherin alpha6\" \"Karyopherin-α1\" \"KC734549\" \"kcal/mol\" \"KCT0001803\" \"kg/day\" \"KH2PO4\"\n \"kidney14\" \"kidneys14\" \"killed/inactivated\" \"Kimura-2\" \"kinase/signal\" \"Kirschner-wire\"\n \"Kishino-Hasegawa\" \"Kit v3-cBot-HS\" \"KJ660346.1\" \"KJ660347.1\" \"KJ660348.1\" \"Klenow-fill\"",
          "matches": [
            {
              "indices": [
                157,
                163
              ],
              "text": "ignal\""
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/41226934/contents/Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp?ref=bd784c216aa1f8984c7ed40e3ede87dc139dcff8",
          "object_type": "FileContent",
          "property": "content",
          "fragment": " \"gravity\" \"grinding\" \"grooming\" \"grossly\" \"grounded\" \"grouped\" \"haemolysis\" \"haemopoietic\"\n \"haemorrhagic\" \"hampered\" \"handling\" \"handwashing\" \"happened\" \"harnessing\" \"harvested\" \"hatching\"\n \"hazardous\" \"healing\" \"heightened\" \"helical\" \"hellenic\" \"hemadsorption\" \"hemagglutination\"",
          "matches": [
            {
              "indices": [
                120,
                128
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/ddmcdonald/sparser/blob/bd784c216aa1f8984c7ed40e3ede87dc139dcff8/Sparser/code/s/tools/ns-stuff/unknown-words-covid-2020-04-03-0403-com-pmc-6301-6400.lisp"
    },
    {
      "path": "relevant_lower.txt",
      "repository": "pry0cc/relevant-wordlist",
      "score": 1,
      "stars": 121,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/359999612/contents/relevant_lower.txt?ref=a10b8272741390451619bac5a90dcd87a1753884",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "gem\ngemini\ngemitaiz",
          "matches": [
            {
              "indices": [
                4,
                10
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/359999612/contents/relevant_lower.txt?ref=a10b8272741390451619bac5a90dcd87a1753884",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "sign-up\nsignal\nsignaling",
          "matches": [
            {
              "indices": [
                8,
                14
              ],
              "text": "signal"
            },
            {
              "indices": [
                15,
                21
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/pry0cc/relevant-wordlist/blob/a10b8272741390451619bac5a90dcd87a1753884/relevant_lower.txt"
    },
    {
      "path": "dev00/data/noutput.wlist.index",
      "repository": "KaosEngineer/TF-RNNLM-CUED",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/92298576/contents/dev00/data/noutput.wlist.index?ref=854676fb761e829a89a1cbb9fe6093cbd6cfb453",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "16006 HAZARDS\n16007 SMACKING\n16008 WHITEY",
          "matches": [
            {
              "indices": [
                14,
                19
              ],
              "text": "16007"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/92298576/contents/dev00/data/noutput.wlist.index?ref=854676fb761e829a89a1cbb9fe6093cbd6cfb453",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "17771 COMBING\n17772 GEMINI\n17773 WILDEST",
          "matches": [
            {
              "indices": [
                20,
                26
              ],
              "text": "GEMINI"
            }
          ]
        }
      ],
      "url": "https://github.com/KaosEngineer/TF-RNNLM-CUED/blob/854676fb761e829a89a1cbb9fe6093cbd6cfb453/dev00/data/noutput.wlist.index"
    },
    {
      "path": "vocab.en",
      "repository": "ddegenaro/ling_4466_hw1",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/749266673/contents/vocab.en?ref=f878004de01d66ee0cb8de377996fd6f0955de1f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "2209 Gemelli 1\n2210 Gemini 1\n2211 Gender 3",
          "matches": [
            {
              "indices": [
                20,
                26
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/749266673/contents/vocab.en?ref=f878004de01d66ee0cb8de377996fd6f0955de1f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "10786 handled 10\n10787 handling 4\n10788 handouts 2",
          "matches": [
            {
              "indices": [
                23,
                31
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/ddegenaro/ling_4466_hw1/blob/f878004de01d66ee0cb8de377996fd6f0955de1f/vocab.en"
    },
    {
      "path": "maliit-plugin-global/openautomata/dict/English.dic",
      "repository": "webosose/ime-manager",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/125007436/contents/maliit-plugin-global/openautomata/dict/English.dic?ref=46e66b09159974fa6657cbfeae105e2b051bd90e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Geller 143\ngemini 93\ngender 68",
          "matches": [
            {
              "indices": [
                11,
                17
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/125007436/contents/maliit-plugin-global/openautomata/dict/English.dic?ref=46e66b09159974fa6657cbfeae105e2b051bd90e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "washroom 28\nwasn't 16007\nwaste 1596",
          "matches": [
            {
              "indices": [
                19,
                24
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/webosose/ime-manager/blob/46e66b09159974fa6657cbfeae105e2b051bd90e/maliit-plugin-global/openautomata/dict/English.dic"
    },
    {
      "path": "vocab.txt",
      "repository": "Day-Aaron-661/AI-final-project-team69",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/982241567/contents/vocab.txt?ref=8190014f662dd280a82db498efe5240089e4b2d3",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "switchin\t8174\r\ngemini\t8175\r\nhittin\t8176\r",
          "matches": [
            {
              "indices": [
                15,
                21
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/982241567/contents/vocab.txt?ref=8190014f662dd280a82db498efe5240089e4b2d3",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "verse3\t16006\r\nhoneysuckle\t16007\r\nguesssed\t16008\r",
          "matches": [
            {
              "indices": [
                26,
                31
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/Day-Aaron-661/AI-final-project-team69/blob/8190014f662dd280a82db498efe5240089e4b2d3/vocab.txt"
    },
    {
      "path": "ranked_words.txt",
      "repository": "HarshithPancheru/Text-Prediction",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/901654261/contents/ranked_words.txt?ref=4792143d72588cdea08df336c5ae86bf1f4dc8e5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "gem 8764\r\ngemini 13177\r\ngems 10751\r",
          "matches": [
            {
              "indices": [
                10,
                16
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/901654261/contents/ranked_words.txt?ref=4792143d72588cdea08df336c5ae86bf1f4dc8e5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "slipped 15673\r\nslippers 16007\r\nslippery 18828\r",
          "matches": [
            {
              "indices": [
                24,
                29
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/HarshithPancheru/Text-Prediction/blob/4792143d72588cdea08df336c5ae86bf1f4dc8e5/ranked_words.txt"
    },
    {
      "path": "src/content/issues/24-02-08-ainews-gemini-ultra-is-out-to-mixed-reviews.md",
      "repository": "smol-ai/ainews-web-2025",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/971669456/contents/src/content/issues/24-02-08-ainews-gemini-ultra-is-out-to-mixed-reviews.md?ref=f02123e0cfadd1d72ea9ea21cb389a85333bbd17",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "- **NLSQLTableQueryEngine Troubleshooting**: User `@nzmrs7` suggested feeding tables when using `NLSQLTableQueryEngine` and proposed checking the query generated with `print(response.metadata)` to clarify the generated SQL query.\n- **Gemini Connection Issues**: User `@whitefang_jr` recommended checking the google console for moderation filters which might be causing issues when connecting to Gemini as faced by `@mowlidharan`.\n- **Reranker Inclusion Strategies**: In response to `@theoxd` inquiring about including a reranker, `@kapa.ai` provided detailed examples of setting up `LLMRerank` and `SentenceTransformerRerank` within a retriever using **LlamaIndex** libraries.",
          "matches": [
            {
              "indices": [
                234,
                240
              ],
              "text": "Gemini"
            },
            {
              "indices": [
                252,
                257
              ],
              "text": "Issue"
            },
            {
              "indices": [
                369,
                374
              ],
              "text": "issue"
            },
            {
              "indices": [
                395,
                401
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/971669456/contents/src/content/issues/24-02-08-ainews-gemini-ultra-is-out-to-mixed-reviews.md?ref=f02123e0cfadd1d72ea9ea21cb389a85333bbd17",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\n- **Documenting the RunPod Issue**: `@m4ttfl0` agreed to document the issue and proposed solutions on GitHub, aiming to help others understand the changes and reasoning. They offered to help test any new changes to ensure they work as expected.\n",
          "matches": [
            {
              "indices": [
                28,
                33
              ],
              "text": "Issue"
            },
            {
              "indices": [
                71,
                76
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/smol-ai/ainews-web-2025/blob/f02123e0cfadd1d72ea9ea21cb389a85333bbd17/src/content/issues/24-02-08-ainews-gemini-ultra-is-out-to-mixed-reviews.md"
    },
    {
      "path": "src/job.md",
      "repository": "ryan-mcclue/ryan-mcclue",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/533695108/contents/src/job.md?ref=e0fe341fad7323bc46f7260e06ef47099282aed8",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "        reconfig: user sends some config packets while system is running; how to handle that? do you restart threads? do you collect a set of changes and commit all at once or commit each small change as they are made?\n        resync: system losses wifi signal then reacquires it; what do you do? can this be detected? do you need a heartbeat or similar?\n        halt: perhaps the system is battery powered and needs to halt on its own as the battery gets low - what should it do? is it safe to broadcast a wifi msg saying \"i'm halted\"? how to reduce power consumption but prevent a reboot into operational mode again?",
          "matches": [
            {
              "indices": [
                254,
                260
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/533695108/contents/src/job.md?ref=e0fe341fad7323bc46f7260e06ef47099282aed8",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "GHD (27/06/24;)\nhttps://ejov.fa.ca2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/16007/?utm_medium=jobshare\n",
          "matches": [
            {
              "indices": [
                94,
                99
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/ryan-mcclue/ryan-mcclue/blob/e0fe341fad7323bc46f7260e06ef47099282aed8/src/job.md"
    },
    {
      "path": "data/2015_02_output/part-00017",
      "repository": "fanshi118/Time-Out-New-York-MLC",
      "score": 1,
      "stars": 8,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/56117590/contents/data/2015_02_output/part-00017?ref=3df17e9c83859d6742c1dec1b214a73fdaf77f13",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "25407310,Goodwin6 Actual,\"IndepenConservative. Wiccan. Husband is medic w/4-7 Cav, 1ABCT, 2nd ID (ROK). Tweets aren't views of DoD, DA, 3rd ID or 2nd ID command grps\",sv,\"I'm at NJ Turnpike Tollbooth in Pennsville, NJ https://t.co/gMg2rt684U\",2015-02-10 15:04:13.0,39.685966,-75.447685,\"New Jersey, USA\"\n21833864,nyc2theworld,HR/PR/IR for my company or clients do not approve any message on here.  Shaming hotels that have HDTV but no HD tv signal.,en,\"I'm at LaGuardia Airport (LGA) in East Elmhurst, NY https://t.co/6sUPdpucTV\",2015-02-10 15:04:53.0,40.773838,-73.87122,\"Queens, NY\"\n21833864,nyc2theworld,HR/PR/IR for my company or clients do not approve any message on here.  Shaming hotels that have HDTV but no HD tv signal.,en,\"I'm at Maple Leaf Lounge in East Elmhurst, NY https://t.co/UOthZFujsN\",2015-02-10 15:05:12.0,40.774117,-73.87002,\"Queens, NY\"",
          "matches": [
            {
              "indices": [
                353,
                356
              ],
              "text": "cli"
            },
            {
              "indices": [
                441,
                447
              ],
              "text": "signal"
            },
            {
              "indices": [
                634,
                637
              ],
              "text": "cli"
            },
            {
              "indices": [
                722,
                728
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/56117590/contents/data/2015_02_output/part-00017?ref=3df17e9c83859d6742c1dec1b214a73fdaf77f13",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "16910996,Jenga,Freak,en,\"I'm at Hale &amp; Hearty in New York, NY https://t.co/fZUDcEauqo\",2015-02-10 17:45:24.0,40.759033,-73.97044,\"Manhattan, NY\"\n435086282,Gökçعn,Yok öyle umutları yitirip karanlıkta savrulmak. Unutma aynı gökyüzü altında DİRENİŞtir yaşamak...[̲̅ə̲̅٨̲̅٥̲̅٦̲̅],en,\"I'm at Istanbul borek &amp; kebab in Cliffside Park, NJ https://t.co/MEe2Fv4NDw\",2015-02-10 17:45:24.0,40.827965,-73.98749,\"Cliffside Park, NJ\"\n18842485,MelissaRains,null,en,\"I'm at Newark Liberty International Airport (EWR) in Newark, NJ https://t.co/MuGePe4det\",2015-02-10 17:45:57.0,40.689686,-74.17939,\"Newark, NJ\"",
          "matches": [
            {
              "indices": [
                350,
                353
              ],
              "text": "co/"
            },
            {
              "indices": [
                437,
                440
              ],
              "text": "Mel"
            }
          ]
        }
      ],
      "url": "https://github.com/fanshi118/Time-Out-New-York-MLC/blob/3df17e9c83859d6742c1dec1b214a73fdaf77f13/data/2015_02_output/part-00017"
    },
    {
      "path": "2-grams/6 q.txt",
      "repository": "mortlach/google_ngrams_Version-20200217",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/529037648/contents/2-grams/6%20q.txt?ref=94d2331b435b9f7f8477317abda8fad9d1c0e173",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "fighting \" 623650\nsignal \" 621690\ncomedy \" 621225",
          "matches": [
            {
              "indices": [
                18,
                24
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/529037648/contents/2-grams/6%20q.txt?ref=94d2331b435b9f7f8477317abda8fad9d1c0e173",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "curzon \" 16010\nratify \" 16007\nmorass \" 16002",
          "matches": [
            {
              "indices": [
                24,
                29
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/mortlach/google_ngrams_Version-20200217/blob/94d2331b435b9f7f8477317abda8fad9d1c0e173/2-grams/6%20q.txt"
    },
    {
      "path": "exports/export_2024-03-16.csv",
      "repository": "donnamagi/hackernews-analysis",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/755817747/contents/exports/export_2024-03-16.csv?ref=94cec960f725bd2dd05c0c9ace0aa3fef20dd10a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "39476304,Facial recognition error message on vending machine sparks concern at university,\"['Smart vending machines', 'Facial recognition', 'Privacy concerns', 'University campus', 'the University of Waterloo', 'Technology']\"\n39478551,Intel Processor Instability Causing Oodle Decompression Failures,\"['Oodle Data compression', 'Hardware issue', 'System instability', 'BIOS', 'RAD Game Tools', 'Oodle Data', 'CineBench', 'Intel processors', 'Unreal Engine', 'RealBench', 'Intel']\"\n39479001,\"Thanks FedEx, this is why we keep getting phished\",\"['Online verification', 'Phishing', 'Scams', 'Australia Post', 'Parcel delivery', 'SMS']\"",
          "matches": [
            {
              "indices": [
                338,
                343
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/755817747/contents/exports/export_2024-03-16.csv?ref=94cec960f725bd2dd05c0c9ace0aa3fef20dd10a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "39481554,\"Gemma.cpp: lightweight, standalone C++ inference engine for Gemma models\",\"['Google', 'Inference', 'C++ inference engine', 'Minimal dependencies', 'Google's Open Source Community Guidelines', 'Research', 'Gemma']\"\n39481670,I Spent a Week with Gemini Pro 1.5–It's Fantastic,\"['Google', 'Model', 'Industries', 'Novels', 'AI', 'Context window size', 'Gemini Pro 1.5']\"\n39483482,\"Show HN: OK-Robot: open, modular home robot framework for pick-and-drop anywhere\",\"['Home automation', 'Machine learning', 'Navigation', 'Discord', 'GitHub', 'Robotics', 'Ok-Robot', 'Modular framework']\"",
          "matches": [
            {
              "indices": [
                253,
                259
              ],
              "text": "Gemini"
            },
            {
              "indices": [
                360,
                366
              ],
              "text": "mini P"
            }
          ]
        }
      ],
      "url": "https://github.com/donnamagi/hackernews-analysis/blob/94cec960f725bd2dd05c0c9ace0aa3fef20dd10a/exports/export_2024-03-16.csv"
    },
    {
      "path": "data/WD40k/word2id.txt",
      "repository": "yo0826jp/KGML",
      "score": 1,
      "stars": 9,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/155802528/contents/data/WD40k/word2id.txt?ref=81a5c9ca97d9fb8c305f40d80d63eb81b8f997a7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "nasa's\t7654\ngemini\t7655\nairborne\t7656",
          "matches": [
            {
              "indices": [
                12,
                18
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/155802528/contents/data/WD40k/word2id.txt?ref=81a5c9ca97d9fb8c305f40d80d63eb81b8f997a7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "subcontinent\t9627\nhandling\t9628\n1849–1936\t9629",
          "matches": [
            {
              "indices": [
                18,
                26
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/yo0826jp/KGML/blob/81a5c9ca97d9fb8c305f40d80d63eb81b8f997a7/data/WD40k/word2id.txt"
    },
    {
      "path": "vocab_updated.en",
      "repository": "ddegenaro/ling_4466_hw1",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/749266673/contents/vocab_updated.en?ref=f878004de01d66ee0cb8de377996fd6f0955de1f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "2305 Gemelli 1\n2306 Gemini 1\n2307 Gender 3",
          "matches": [
            {
              "indices": [
                20,
                26
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/749266673/contents/vocab_updated.en?ref=f878004de01d66ee0cb8de377996fd6f0955de1f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "11331 handles 1\n11332 handling 4\n11333 handouts 2",
          "matches": [
            {
              "indices": [
                22,
                30
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/ddegenaro/ling_4466_hw1/blob/f878004de01d66ee0cb8de377996fd6f0955de1f/vocab_updated.en"
    },
    {
      "path": "1000mostfreqwords.txt",
      "repository": "meg2208/DomainScrape",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/10389192/contents/1000mostfreqwords.txt?ref=534df151f6a5fd922c4a936bc97429b62cf051ec",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1616\tnames\t1191\n1617\tissue\t1191\n1618\torders\t1190",
          "matches": [
            {
              "indices": [
                21,
                26
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/10389192/contents/1000mostfreqwords.txt?ref=534df151f6a5fd922c4a936bc97429b62cf051ec",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "8886\tholland\t93\n8887\tgemini\t93\n8888\tgaines\t93",
          "matches": [
            {
              "indices": [
                21,
                27
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/meg2208/DomainScrape/blob/534df151f6a5fd922c4a936bc97429b62cf051ec/1000mostfreqwords.txt"
    },
    {
      "path": "priv/bookmarks.csv",
      "repository": "iagocavalcante/iago-cavalcante",
      "score": 1,
      "stars": 5,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/605836834/contents/priv/bookmarks.csv?ref=2efa006c03c1bd6899f506fcf15f1c2912722723",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Here’s a Git Branching Strategy for Better Team Collaboration | by Tapas Da,https://betterprogramming.pub/git-branching-strategy-for-better-team-collaboration-aacb5f235d05,1649104530,git,unread\nUsing docker to build nestjs and Mongo development environment - 编程知识,https://cdmana.com/2021/08/20210804175044788x.html,1647267167,node,unread\nStripe Checkout with Phoenix LiveView · stibbard.io,https://www.stibbard.io/learning/elixir-phoenix-stripe-checkout,1643829184,elixir,unread\nHandling any POST data in Express,https://codex.so/handling-any-post-data-in-express,1691595569,,unread\nReact Native implementation for Zendesk Chat using WebView & Modal componen,https://gist.github.com/hetmann/bda29c335da8bb51f8e2e2d520edf3b6,1644006075,react-native-hooks,unread\nFigma Illustrations | Figma Elements,https://figmaelements.com/utilities/illustrations,1647632433,design,unread\nClient get SSL internal error · Issue #5748 · Kong/kong,https://github.com/Kong/kong/issues/5748,1645034302,kong,unread",
          "matches": [
            {
              "indices": [
                490,
                498
              ],
              "text": "y POST d"
            },
            {
              "indices": [
                541,
                549
              ],
              "text": "y-post-d"
            },
            {
              "indices": [
                884,
                887
              ],
              "text": "SSL"
            },
            {
              "indices": [
                917,
                922
              ],
              "text": "· Kon"
            },
            {
              "indices": [
                971,
                976
              ],
              "text": "64503"
            }
          ]
        }
      ],
      "url": "https://github.com/iagocavalcante/iago-cavalcante/blob/2efa006c03c1bd6899f506fcf15f1c2912722723/priv/bookmarks.csv"
    },
    {
      "path": "src/Components/Data.js",
      "repository": "web4389/Find-News",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/859424837/contents/src/Components/Data.js?ref=9c0d850228cac7b0a9e447a2dd8cc4d9fb8005de",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "          \"“The Aurora he’s talking about is one the people of Colorado have never heard of,” Colorado Gov. Jared Polis said Friday.\",\n        url: \"https://nypost.com/2024/10/12/us-news/trump-slams-colorado-gov-jared-polis-over-handling-of-migrant-gangs-in-aurora-hes-chickens/\",\n        urlToImage:",
          "matches": [
            {
              "indices": [
                235,
                243
              ],
              "text": "ng-of-mi"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/859424837/contents/src/Components/Data.js?ref=9c0d850228cac7b0a9e447a2dd8cc4d9fb8005de",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "        title:\n          \"Wholesale prices flatten out and signal low inflation in guts of the economy - MarketWatch\",\n        description: \"PPI unchanged in September, core rate up 0.1%\",",
          "matches": [
            {
              "indices": [
                59,
                65
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/web4389/Find-News/blob/9c0d850228cac7b0a9e447a2dd8cc4d9fb8005de/src/Components/Data.js"
    },
    {
      "path": "info/2024-July/24/paper_info.json",
      "repository": "alexfanjn/arXiv-daily-graphMining",
      "score": 1,
      "stars": 24,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/373440689/contents/info/2024-July/24/paper_info.json?ref=7bb16324058708738f19d8faefcfaa3c578f5033",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "  {\n    \"id\": \"arXiv:2407.16357\",\n    \"title\": \"TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou\",\n    \"abstract\": \"           The significance of modeling long-term user interests for CTR prediction tasks in large-scale recommendation systems is progressively gaining attention among researchers and practitioners. Existing work, such as SIM and TWIN, typically employs a two-stage approach to model long-term user behavior sequences for efficiency concerns. The first stage rapidly retrieves a subset of sequences related to the target item from a long sequence using a search-based mechanism namely the General Search Unit (GSU), while the second stage calculates the interest scores using the Exact Search Unit (ESU) on the retrieved results. Given the extensive length of user behavior sequences spanning the entire life cycle, potentially reaching up to 10^6 in scale, there is currently no effective solution for fully modeling such expansive user interests. To overcome this issue, we introduced TWIN-V2, an enhancement of TWIN, where a divide-and-conquer approach is applied to compress life-cycle behaviors and uncover more accurate and diverse user interests. Specifically, a hierarchical clustering method groups items with similar characteristics in life-cycle behaviors into a single cluster during the offline phase. By limiting the size of clusters, we can compress behavior sequences well beyond the magnitude of 10^5 to a length manageable for online inference in GSU retrieval. Cluster-aware target attention extracts comprehensive and multi-faceted long-term interests of users, thereby making the final recommendation results more accurate and diverse. Extensive offline experiments on a multi-billion-scale industrial dataset and online A/B tests have demonstrated the effectiveness of TWIN-V2. Under an efficient deployment framework, TWIN-V2 has been successfully deployed to the primary traffic that serves hundreds of millions of daily active users at Kuaishou.         \",\n    \"url\": \"https://arxiv.org/abs/2407.16357\",\n    \"authors\": [\n      \"Zihua Si\",",
          "matches": [
            {
              "indices": [
                1034,
                1039
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/alexfanjn/arXiv-daily-graphMining/blob/7bb16324058708738f19d8faefcfaa3c578f5033/info/2024-July/24/paper_info.json"
    },
    {
      "path": "exports/export_2024-03-13.csv",
      "repository": "donnamagi/hackernews-analysis",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/755817747/contents/exports/export_2024-03-13.csv?ref=94cec960f725bd2dd05c0c9ace0aa3fef20dd10a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "39476304,\"['Smart vending machines', 'Facial recognition', 'Privacy concerns', 'University campus', 'the University of Waterloo', 'Technology']\",26-02-2024\n39478551,\"['Oodle Data compression', 'Hardware issue', 'System instability', 'BIOS', 'RAD Game Tools', 'Oodle Data', 'CineBench', 'Intel processors', 'Unreal Engine', 'RealBench', 'Intel']\",26-02-2024\n39479001,\"['Online verification', 'Phishing', 'Scams', 'Australia Post', 'Parcel delivery', 'SMS']\",26-02-2024",
          "matches": [
            {
              "indices": [
                203,
                208
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/755817747/contents/exports/export_2024-03-13.csv?ref=94cec960f725bd2dd05c0c9ace0aa3fef20dd10a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "39481554,\"['Google', 'Inference', 'C++ inference engine', 'Minimal dependencies', 'Google's Open Source Community Guidelines', 'Research', 'Gemma']\",26-02-2024\n39481670,\"['Google', 'Model', 'Industries', 'Novels', 'AI', 'Context window size', 'Gemini Pro 1.5']\",26-02-2024\n39483482,\"['Home automation', 'Machine learning', 'Navigation', 'Discord', 'GitHub', 'Robotics', 'Ok-Robot', 'Modular framework']\",26-02-2024",
          "matches": [
            {
              "indices": [
                244,
                250
              ],
              "text": "Gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/donnamagi/hackernews-analysis/blob/94cec960f725bd2dd05c0c9ace0aa3fef20dd10a/exports/export_2024-03-13.csv"
    },
    {
      "path": "pastes/pastes_20250620060838.csv",
      "repository": "osirislab/LeakyPastes-V2",
      "score": 1,
      "stars": 29,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/706296536/contents/pastes/pastes_20250620060838.csv?ref=72f9607bd91684ca7a52815ab73215cc4101ba27",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\r\n  - platform: wifi_signal    \r\n    id: wifi_signal_db\r",
          "matches": [
            {
              "indices": [
                21,
                27
              ],
              "text": "signal"
            },
            {
              "indices": [
                46,
                52
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/706296536/contents/pastes/pastes_20250620060838.csv?ref=72f9607bd91684ca7a52815ab73215cc4101ba27",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "INSERT IGNORE INTO character_achievement (guid, achievement, DATE) VALUES (7976386, 439, 1743465601);\r\nINSERT IGNORE INTO character_achievement (guid, achievement, DATE) VALUES (6160073, 439, 1743465601);\r\nINSERT IGNORE INTO character_achievement (guid, achievement, DATE) VALUES (3871712, 439, 1743465601);\r",
          "matches": [
            {
              "indices": [
                179,
                184
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/osirislab/LeakyPastes-V2/blob/72f9607bd91684ca7a52815ab73215cc4101ba27/pastes/pastes_20250620060838.csv"
    },
    {
      "path": "datasets/nsf/nsflex.txt",
      "repository": "JuliaPackageMirrors/TopicModelsVB.jl",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/67878255/contents/datasets/nsf/nsflex.txt?ref=1f03fb8170209c00fb071fc66fc8eb5d180debb2",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "9167\tgeman\n9168\tgemini\n9169\tgemstone",
          "matches": [
            {
              "indices": [
                16,
                22
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/67878255/contents/datasets/nsf/nsflex.txt?ref=1f03fb8170209c00fb071fc66fc8eb5d180debb2",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "20679\tsign\n20680\tsignal\n20681\tsignaled",
          "matches": [
            {
              "indices": [
                17,
                23
              ],
              "text": "signal"
            },
            {
              "indices": [
                30,
                36
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/JuliaPackageMirrors/TopicModelsVB.jl/blob/1f03fb8170209c00fb071fc66fc8eb5d180debb2/datasets/nsf/nsflex.txt"
    },
    {
      "path": "exports/export_2024-04-07.csv",
      "repository": "donnamagi/hackernews-analysis",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/755817747/contents/exports/export_2024-04-07.csv?ref=94cec960f725bd2dd05c0c9ace0aa3fef20dd10a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "39476304,Facial recognition error message on vending machine sparks concern at university,26-02-2024,\"['Smart vending machines', 'Facial recognition', 'Privacy concerns', 'University campus', 'the University of Waterloo', 'Technology']\"\n39478551,Intel Processor Instability Causing Oodle Decompression Failures,26-02-2024,\"['Oodle Data compression', 'Hardware issue', 'System instability', 'BIOS', 'RAD Game Tools', 'Oodle Data', 'CineBench', 'Intel processors', 'Unreal Engine', 'RealBench', 'Intel']\"\n39479001,\"Thanks FedEx, this is why we keep getting phished\",26-02-2024,\"['Online verification', 'Phishing', 'Scams', 'Australia Post', 'Parcel delivery', 'SMS']\"",
          "matches": [
            {
              "indices": [
                360,
                365
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/755817747/contents/exports/export_2024-04-07.csv?ref=94cec960f725bd2dd05c0c9ace0aa3fef20dd10a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "39499207,Hallucination is inevitable: An innate limitation of large language models,27-02-2024,\"['Language models', 'Mitigation', 'Inconsistencies', 'LLM', 'Hallucination', 'Computable functions']\"\n39501061,Marginalia: 3 Years,27-02-2024,\"['Marginalia Search', 'Keyword Handling', 'Upgrades', 'Search Engine', 'Indexed Documents']\"\n39501073,\"Show HN: Reverse-Engineering a Switch Lite with 1,917 wires\",27-02-2024,\"['Soldering', 'Electronics manufacturing', 'Boardview', 'Boardscans', 'PCB assembly']\"",
          "matches": [
            {
              "indices": [
                270,
                278
              ],
              "text": "Handling"
            }
          ]
        }
      ],
      "url": "https://github.com/donnamagi/hackernews-analysis/blob/94cec960f725bd2dd05c0c9ace0aa3fef20dd10a/exports/export_2024-04-07.csv"
    },
    {
      "path": "src/dictionaries/frequency/wiktionary/2006-tv.txt",
      "repository": "henge-tech/henge",
      "score": 1,
      "stars": 2,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/65596234/contents/src/dictionaries/frequency/wiktionary/2006-tv.txt?ref=23902709cb949d54fa522c573dd50b4ddcd3d164",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1616    names           1191\n1617    issue           1191\n1618    orders          1190",
          "matches": [
            {
              "indices": [
                37,
                42
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/65596234/contents/src/dictionaries/frequency/wiktionary/2006-tv.txt?ref=23902709cb949d54fa522c573dd50b4ddcd3d164",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "8886    holland                 93\n8887    gemini                  93\n8888    gaines                  93",
          "matches": [
            {
              "indices": [
                43,
                49
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/henge-tech/henge/blob/23902709cb949d54fa522c573dd50b4ddcd3d164/src/dictionaries/frequency/wiktionary/2006-tv.txt"
    },
    {
      "path": "word-weights.txt",
      "repository": "callum-oakley/nonsense",
      "score": 1,
      "stars": 9,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/362210764/contents/word-weights.txt?ref=20f79e3de4a9d794a78dbdac51cd9db4507850e6",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "names\t1191\nissue\t1191\norders\t1190",
          "matches": [
            {
              "indices": [
                11,
                16
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/362210764/contents/word-weights.txt?ref=20f79e3de4a9d794a78dbdac51cd9db4507850e6",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "holland\t93\ngemini\t93\ngaines\t93",
          "matches": [
            {
              "indices": [
                11,
                17
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/callum-oakley/nonsense/blob/20f79e3de4a9d794a78dbdac51cd9db4507850e6/word-weights.txt"
    },
    {
      "path": "android-app/src/main/res/raw/original_dictionary.txt",
      "repository": "NTNU-IE-IIR/excited-eeg",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/274499826/contents/android-app/src/main/res/raw/original_dictionary.txt?ref=3f7d8c0429df5f760676d93f05e82f0b2a8c4546",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "names 1191\nissue 1191\norders 1190",
          "matches": [
            {
              "indices": [
                11,
                16
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/274499826/contents/android-app/src/main/res/raw/original_dictionary.txt?ref=3f7d8c0429df5f760676d93f05e82f0b2a8c4546",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "holland 93\ngemini 93\ngaines 93",
          "matches": [
            {
              "indices": [
                11,
                17
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/NTNU-IE-IIR/excited-eeg/blob/3f7d8c0429df5f760676d93f05e82f0b2a8c4546/android-app/src/main/res/raw/original_dictionary.txt"
    },
    {
      "path": "001大语言模型/03AINews/20250131AINews.md",
      "repository": "dalong0514/dalong.ITstudy",
      "score": 1,
      "stars": 28,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/124549652/contents/001%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/03AINews/20250131AINews.md?ref=96174108504eb119999d48c1cbaa014e0e9677dc",
          "object_type": "FileContent",
          "property": "content",
          "fragment": " - Participants advised that API calls should be made from either a Node backend with Relay Request or through an Edge function to avoid such issues.\n- **SEO meta data handling in React apps**: A user is seeking advice on how to implement server-side SEO meta data for different pages in a React application, noting that the usual methods are not effective.\n - There was discussion about using alternatives, as the default helmet approach does not seem to be fetching the right metadata for social media sharing. **Links mentioned**: ",
          "matches": [
            {
              "indices": [
                142,
                147
              ],
              "text": "issue"
            },
            {
              "indices": [
                168,
                176
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/124549652/contents/001%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/03AINews/20250131AINews.md?ref=96174108504eb119999d48c1cbaa014e0e9677dc",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\n- **Support Ticket Created**: A member created a [support ticket](https://discord.com/channels/954421988141711382/1334344003994386474/1334344003994386474) for assistance, ensuring that the issue is noted and tracked.\n - This reinforces the importance of keeping communication clear in Discord channels for efficient problem-solving.",
          "matches": [
            {
              "indices": [
                190,
                195
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/dalong0514/dalong.ITstudy/blob/96174108504eb119999d48c1cbaa014e0e9677dc/001%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/03AINews/20250131AINews.md"
    },
    {
      "path": "models/vocab_jobs_train.txt",
      "repository": "Taranveer/JobSkillRecommendation",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/111638240/contents/models/vocab_jobs_train.txt?ref=dc21d5f45eae1b39af634fc1e120cc7231715278",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "hayden\nissue\nreco",
          "matches": [
            {
              "indices": [
                7,
                12
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/111638240/contents/models/vocab_jobs_train.txt?ref=dc21d5f45eae1b39af634fc1e120cc7231715278",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "mlc\nhandling\nrds",
          "matches": [
            {
              "indices": [
                4,
                12
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/Taranveer/JobSkillRecommendation/blob/dc21d5f45eae1b39af634fc1e120cc7231715278/models/vocab_jobs_train.txt"
    },
    {
      "path": "rss_data/cs.CR/2025-06-02_cs.CR.xml",
      "repository": "ehijano/rss_fetch",
      "score": 1,
      "stars": 10,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/767212066/contents/rss_data/cs.CR/2025-06-02_cs.CR.xml?ref=f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "      <description>arXiv:2402.02160v3 Announce Type: replace \nAbstract: In the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL's susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, demonstrate that ICL's performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.</description>\n      <guid isPermaLink=\"false\">oai:arXiv.org:2402.02160v3</guid>",
          "matches": [
            {
              "indices": [
                310,
                315
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/767212066/contents/rss_data/cs.CR/2025-06-02_cs.CR.xml?ref=f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "      <description>arXiv:2505.23821v2 Announce Type: replace \nAbstract: With the surge of social media, maliciously tampered public speeches, especially those from influential figures, have seriously affected social stability and public trust. Existing speech tampering detection methods remain insufficient: they either rely on external reference data or fail to be both sensitive to attacks and robust to benign operations, such as compression and resampling. To tackle these challenges, we introduce SpeechVerifer to proactively verify speech integrity using only the published speech itself, i.e., without requiring any external references. Inspired by audio fingerprinting and watermarking, SpeechVerifier can (i) effectively detect tampering attacks, (ii) be robust to benign operations and (iii) verify the integrity only based on published speeches. Briefly, SpeechVerifier utilizes multiscale feature extraction to capture speech features across different temporal resolutions. Then, it employs contrastive learning to generate fingerprints that can detect modifications at varying granularities. These fingerprints are designed to be robust to benign operations, but exhibit significant changes when malicious tampering occurs. To enable speech verification in a self-contained manner, the generated fingerprints are then embedded into the speech signal by segment-wise watermarking. Without external references, SpeechVerifier can retrieve the fingerprint from the published audio and check it with the embedded watermark to verify the integrity of the speech. Extensive experimental results demonstrate that the proposed SpeechVerifier is effective in detecting tampering attacks and robust to benign operations.</description>\n      <guid isPermaLink=\"false\">oai:arXiv.org:2505.23821v2</guid>",
          "matches": [
            {
              "indices": [
                1357,
                1363
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/ehijano/rss_fetch/blob/f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5/rss_data/cs.CR/2025-06-02_cs.CR.xml"
    },
    {
      "path": "Analysis/Tweets_Data/Data/Tweets_mashabletech.txt",
      "repository": "lunrongchen/Setrend",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_mashabletech.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "mashabletech\t706989828517326848\t2016-03-07T23:48:05\tPeeple, the 'Yelp for people,' has launched — and everyone still hates it \nmashabletech\t706986281927110656\t2016-03-07T23:33:59\tNissan concept video transforms cars into power plants of the future \nmashabletech\t706980005960687616\t2016-03-07T23:09:03\tGoogle has hired the founder of 4chan to save its struggling social network, Google+. \nmashabletech\t706967899592855553\t2016-03-07T22:20:56\tHere's how Apple's bat-signal united the tech industry to rush to court against the FBI. \nmashabletech\t706966695437586432\t2016-03-07T22:16:09\tGoogle has hired the founder of 4chan to save its struggling social network, Google+. \nmashabletech\t706939652142456833\t2016-03-07T20:28:42\tHulk Hogan is in court today, suing Gawker media over a sex tape that showed him making racial slurs. \nmashabletech\t706932366489612288\t2016-03-07T19:59:44\tApple's bat-signal called the whole tech industry to court to fight the FBI. ",
          "matches": [
            {
              "indices": [
                465,
                471
              ],
              "text": "gnal u"
            },
            {
              "indices": [
                890,
                896
              ],
              "text": "gnal c"
            }
          ]
        }
      ],
      "url": "https://github.com/lunrongchen/Setrend/blob/58fa3d0ff28dc92a83cb3f10d5c0637df51176bd/Analysis/Tweets_Data/Data/Tweets_mashabletech.txt"
    },
    {
      "path": "sitemapf/13220438.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/13220438.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b01icnoywq o artico a serie da natureza livro 4 portuguese edition\nb015rwvx1a the gospel of fury the gemini star english edition\nb01n3psntv el despertar del tercer ojo",
          "matches": [
            {
              "indices": [
                101,
                107
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/13220438.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b007az4p68 figaro economie le no 20581 du 02/10/2010 - lego se diversifie dans les jeux de societe - les entreprises francaises se bousculent au bresil - leurop flambe a nouveau - fmi leurope prete a faire plus de place aux pays emergents - safran boucle le rachat de snpe materiaux - fabricant du carburant de la fusee ariane - ikea le plus rentable du monde - leo apotheker prend les commandes de hp - un constructeur de maisons appate le client avec un jeu sur facebook -\n044401604x application of conjugate gradient method to electromagnetics and signal analysis\nb00lefs8k4 home preparation of moonshine english edition",
          "matches": [
            {
              "indices": [
                441,
                444
              ],
              "text": "cli"
            },
            {
              "indices": [
                551,
                557
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/13220438.txt"
    },
    {
      "path": "sitemapf/34481148.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/34481148.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0977806650 stuffed animals: a story in paper cutouts\nb00payma04 interfaces de linea de comandos en python con el modulo cli (spanish edition)\n3319442864 financial regulation in the eu: from resilience to growth",
          "matches": [
            {
              "indices": [
                120,
                123
              ],
              "text": "cli"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/34481148.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00kgmiwt8 i married the icepick killer: a poet in hollywood (english edition)\nb00b4z93ki bulletin, issue 44\nb00jmq8k8m excellency of christ (annotated and illustrated): how can jesus christ be the lamb and lion at the same time (english edition)",
          "matches": [
            {
              "indices": [
                100,
                105
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/34481148.txt"
    },
    {
      "path": "rss_data/cs.CL/2025-07-22_cs.CL.xml",
      "repository": "ehijano/rss_fetch",
      "score": 1,
      "stars": 10,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/767212066/contents/rss_data/cs.CL/2025-07-22_cs.CL.xml?ref=f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "      <description>arXiv:2507.16217v1 Announce Type: new \nAbstract: Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.</description>\n      <guid isPermaLink=\"false\">oai:arXiv.org:2507.16217v1</guid>",
          "matches": [
            {
              "indices": [
                1153,
                1159
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/767212066/contents/rss_data/cs.CL/2025-07-22_cs.CL.xml?ref=f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "Abstract: Discourse markers (DMs) like 'but' or 'then' are crucial for creating coherence in discourse, yet they are often replaced by or co-occur with non-DMs ('in the morning' can mean the same as 'then'), and both can be ambiguous ('since' can refer to time or cause). The interaction mechanism between such signals remains unclear but pivotal for their disambiguation. In this paper we investigate the relationship between DM polysemy and co-occurrence of non-DM signals in English, as well as the influence of genre on these patterns.\n  Using the framework of eRST, we propose a graded definition of DM polysemy, and conduct correlation and regression analyses to examine whether polysemous DMs are accompanied by more numerous and diverse non-DM signals. Our findings reveal that while polysemous DMs do co-occur with more diverse non-DMs, the total number of co-occurring signals does not necessarily increase. Moreover, genre plays a significant role in shaping DM-signal interactions.</description>\n      <guid isPermaLink=\"false\">oai:arXiv.org:2507.16748v1</guid>",
          "matches": [
            {
              "indices": [
                311,
                317
              ],
              "text": "signal"
            },
            {
              "indices": [
                467,
                473
              ],
              "text": "signal"
            },
            {
              "indices": [
                752,
                758
              ],
              "text": "signal"
            },
            {
              "indices": [
                879,
                885
              ],
              "text": "signal"
            },
            {
              "indices": [
                973,
                979
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/ehijano/rss_fetch/blob/f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5/rss_data/cs.CL/2025-07-22_cs.CL.xml"
    },
    {
      "path": "rss_data/cs.CL/2025-12-23_cs.CL.xml",
      "repository": "ehijano/rss_fetch",
      "score": 1,
      "stars": 10,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/767212066/contents/rss_data/cs.CL/2025-12-23_cs.CL.xml?ref=f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "      <description>arXiv:2512.19620v1 Announce Type: new \nAbstract: Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.</description>\n      <guid isPermaLink=\"false\">oai:arXiv.org:2512.19620v1</guid>",
          "matches": [
            {
              "indices": [
                735,
                740
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/767212066/contents/rss_data/cs.CL/2025-12-23_cs.CL.xml?ref=f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "      <title>Label Words as Local Task Vectors in In-Context Learning</title>\n      <link>https://arxiv.org/abs/2406.16007</link>\n      <description>arXiv:2406.16007v2 Announce Type: replace ",
          "matches": [
            {
              "indices": [
                117,
                122
              ],
              "text": "16007"
            },
            {
              "indices": [
                160,
                165
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/ehijano/rss_fetch/blob/f07140ea0cbc75f6d31ea6a5deba6a34d8dadcc5/rss_data/cs.CL/2025-12-23_cs.CL.xml"
    },
    {
      "path": "2-grams/s 6.txt",
      "repository": "mortlach/google_ngrams_Version-20200217",
      "score": 1,
      "stars": 3,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/529037648/contents/2-grams/s%206.txt?ref=94d2331b435b9f7f8477317abda8fad9d1c0e173",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "_start_ hitler 1998400\n_start_ signal 1996990\n_start_ houses 1996643",
          "matches": [
            {
              "indices": [
                31,
                37
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/529037648/contents/2-grams/s%206.txt?ref=94d2331b435b9f7f8477317abda8fad9d1c0e173",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "_start_ xxxvii 1435692\n_start_ handling 1435114\n_start_ carlos 1431977",
          "matches": [
            {
              "indices": [
                31,
                39
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/mortlach/google_ngrams_Version-20200217/blob/94d2331b435b9f7f8477317abda8fad9d1c0e173/2-grams/s%206.txt"
    },
    {
      "path": "a2l/model/lang_chain_4G/words.txt",
      "repository": "AltarBeastiful/ASA_ICASSP2021",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/399141247/contents/a2l/model/lang_chain_4G/words.txt?ref=d6cfda1c4cafb36f3b1581771900fac2038bc4f3",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "GEM 9610\nGEMINI 9611\nGEMS 9612",
          "matches": [
            {
              "indices": [
                9,
                15
              ],
              "text": "GEMINI"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/399141247/contents/a2l/model/lang_chain_4G/words.txt?ref=d6cfda1c4cafb36f3b1581771900fac2038bc4f3",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "SIGN 20985\nSIGNAL 20986\nSIGNALS 20987",
          "matches": [
            {
              "indices": [
                11,
                17
              ],
              "text": "SIGNAL"
            },
            {
              "indices": [
                24,
                30
              ],
              "text": "SIGNAL"
            }
          ]
        }
      ],
      "url": "https://github.com/AltarBeastiful/ASA_ICASSP2021/blob/d6cfda1c4cafb36f3b1581771900fac2038bc4f3/a2l/model/lang_chain_4G/words.txt"
    },
    {
      "path": "data/preprocessed/_claim_evi_pairs_word_mapping.txt",
      "repository": "johnnyjana730/HDAE",
      "score": 1,
      "stars": 6,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/438862071/contents/data/preprocessed/_claim_evi_pairs_word_mapping.txt?ref=d2a5131dd71008413e48f8289743fbbfde30797d",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "abroad 4032\ngemini 4033\nbusinesswoman 4034",
          "matches": [
            {
              "indices": [
                12,
                18
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/438862071/contents/data/preprocessed/_claim_evi_pairs_word_mapping.txt?ref=d2a5131dd71008413e48f8289743fbbfde30797d",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "discipline 5488\nsignal 5489\nsigint 5490",
          "matches": [
            {
              "indices": [
                16,
                22
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/johnnyjana730/HDAE/blob/d2a5131dd71008413e48f8289743fbbfde30797d/data/preprocessed/_claim_evi_pairs_word_mapping.txt"
    },
    {
      "path": "logs/norm_pmi/btc_0.2.csv",
      "repository": "lucy3/ingroup_lang",
      "score": 1,
      "stars": 13,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/206418488/contents/logs/norm_pmi/btc_0.2.csv?ref=ddae95863a16e4afd9fa2ea7cbff56c9c780b2e7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "exclusively,0.028828816341200433,44\r\nhandling,0.0288801852470921,36\r\nways,0.028886255299826018,266\r",
          "matches": [
            {
              "indices": [
                37,
                45
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/206418488/contents/logs/norm_pmi/btc_0.2.csv?ref=ddae95863a16e4afd9fa2ea7cbff56c9c780b2e7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "forming,0.052129834347269255,26\r\nsignal,0.05213219778231292,53\r\nrequest,0.05216160845954619,97\r",
          "matches": [
            {
              "indices": [
                33,
                39
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/lucy3/ingroup_lang/blob/ddae95863a16e4afd9fa2ea7cbff56c9c780b2e7/logs/norm_pmi/btc_0.2.csv"
    },
    {
      "path": "csvfolder/energy.csv",
      "repository": "williamshammond/reddit-visualizer",
      "score": 1,
      "stars": 4,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/308470151/contents/csvfolder/energy.csv?ref=cf3b91e60943a7ed52a34282566b4140219e6545",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "71,energy,\"Donald Trump’s issue with windmills might not be about birds. If the president were really worried about birds, he’d be talking about cats. “I know more about wind than you do. It’s extremely expensive. Kills all the birds. It’s very intermittent.\"\"\",1603469894.0,\"10/23/2020, 16:18:14\",103,0,338,23.548996342954297,0.93,0.3047337278106509\n72,energy,\"Panama Cancels Order For Diesel Buses, Will Purchase 195 Electric Buses Instead. MiBus will spend $35 million to purchase 195 electric buses. Trial period demonstrated the value of regenerative braking and that the buses air conditioning systems were capable of handling the hot, humid environment.\",1598803675.0,\"08/30/2020, 16:07:55\",31,0,343,5.15759296976961,0.99,0.09037900874635568\n73,energy,Revealed: legislators’ pro-pipeline letters ghostwritten by fossil fuel company,1593686050.0,\"07/02/2020, 10:34:10\",12,0,337,6.954832038583959,0.98,0.03560830860534125",
          "matches": [
            {
              "indices": [
                28,
                33
              ],
              "text": "sue w"
            },
            {
              "indices": [
                634,
                642
              ],
              "text": "he hot, "
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/308470151/contents/csvfolder/energy.csv?ref=cf3b91e60943a7ed52a34282566b4140219e6545",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "77,energy,Coal Is Now the World’s Most Expensive Fossil Fuel,1584968653.0,\"03/23/2020, 13:04:13\",37,0,336,18.35967145769348,0.95,0.11011904761904762\n78,energy,\"How Joe Biden May Have Outmaneuvered Donald Trump On Energy, Climate, and the Economic Recovery. Trump has repeatedly played to his base with various rejections of climate science. The Biden campaign has used the issue to carefully build a broad coalition. “The politics of climate have changed.”\",1604252088.0,\"11/01/2020, 17:34:48\",76,1,333,48.41660830427708,0.87,0.22822822822822822\n79,energy,\"With U.S. Fossils ‘Hurtling Toward Bankruptcy’, Execs Get a Payout While Abandoned Wells Leak Methane\",1595249321.0,\"07/20/2020, 12:48:41\",17,0,337,12.791029870258967,0.96,0.050445103857566766",
          "matches": [
            {
              "indices": [
                223,
                226
              ],
              "text": "ima"
            },
            {
              "indices": [
                326,
                329
              ],
              "text": "ima"
            },
            {
              "indices": [
                375,
                380
              ],
              "text": "sue t"
            },
            {
              "indices": [
                438,
                441
              ],
              "text": "ate"
            }
          ]
        }
      ],
      "url": "https://github.com/williamshammond/reddit-visualizer/blob/cf3b91e60943a7ed52a34282566b4140219e6545/csvfolder/energy.csv"
    },
    {
      "path": "data/preprocessed/0_claim_evi_pairs_word_mapping.txt",
      "repository": "johnnyjana730/HDAE",
      "score": 1,
      "stars": 6,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/438862071/contents/data/preprocessed/0_claim_evi_pairs_word_mapping.txt?ref=d2a5131dd71008413e48f8289743fbbfde30797d",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "abroad 4032\ngemini 4033\nbusinesswoman 4034",
          "matches": [
            {
              "indices": [
                12,
                18
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/438862071/contents/data/preprocessed/0_claim_evi_pairs_word_mapping.txt?ref=d2a5131dd71008413e48f8289743fbbfde30797d",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "discipline 5488\nsignal 5489\nsigint 5490",
          "matches": [
            {
              "indices": [
                16,
                22
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/johnnyjana730/HDAE/blob/d2a5131dd71008413e48f8289743fbbfde30797d/data/preprocessed/0_claim_evi_pairs_word_mapping.txt"
    },
    {
      "path": "data/stocks-list.csv",
      "repository": "ozencgungor/Pairs_Trading_Kalman",
      "score": 1,
      "stars": 14,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/525886204/contents/data/stocks-list.csv?ref=d665314838ca187aa78f4c8d7cfc5ae496da5702",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "FSRX,FinServ Acquisition II,Blank Check / SPAC,377638009\nFSS,Federal Signal,Machinery,2509036020\nFSSI,Fortistar Sustainable Solutions,Shell Companies,318909848",
          "matches": [
            {
              "indices": [
                69,
                75
              ],
              "text": "Signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/525886204/contents/data/stocks-list.csv?ref=d665314838ca187aa78f4c8d7cfc5ae496da5702",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "GMS,GMS Inc.,Trading Companies & Distributors,2250903223\nGMTX,Gemini Therapeutics,Biotechnology,67866704\nGMVD,G Medical Innovations Holdings,Medical Instruments & Supplies,9759535",
          "matches": [
            {
              "indices": [
                62,
                68
              ],
              "text": "Gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/ozencgungor/Pairs_Trading_Kalman/blob/d665314838ca187aa78f4c8d7cfc5ae496da5702/data/stocks-list.csv"
    },
    {
      "path": "completed.html",
      "repository": "onurkaraman/onurkaraman.github.io",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/10130865/contents/completed.html?ref=867426e59d61356872cbf8747ad398444efa36eb",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "        <li><a href=\"http://diyhpl.us/~bryan/papers2/distributed/distributed-systems/zab.totally-ordered-broadcast-protocol.2008.pdf\">zab.totally-ordered-broadcast-protocol.2008.pdf</a></li>\n        <li><a href=\"https://status.cloud.google.com/incident/compute/16007?post-mortem\">google compute engine 4-11-2016 postmortem</a></li>\n        <li><a href=\"https://engineering.linkedin.com/blog/2016/04/gobblin-gobbles-camus--looks-towards-the-future\">Gobblin Gobbles Camus, Looks Towards the Future | LinkedIn Engineering</a></li>",
          "matches": [
            {
              "indices": [
                261,
                266
              ],
              "text": "16007"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/10130865/contents/completed.html?ref=867426e59d61356872cbf8747ad398444efa36eb",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "        <li><a href=\"https://github.com/cockroachdb/cockroach/issues/13722\">rpc: 99th percentile round-trip latency numbers seem too high · Issue #13722 · cockroachdb/cockroach</a></li>\n        <li><a href=\"https://github.com/cockroachdb/cockroach/issues/17243\">perf: quantify benefit of disabling Go GC assist · Issue #17243 · cockroachdb/cockroach</a></li>\n        <li><a href=\"https://github.com/cockroachdb/cockroach/issues/18715\">perf: verify RocksDB small write performance · Issue #18715 · cockroachdb/cockroach</a></li>",
          "matches": [
            {
              "indices": [
                62,
                67
              ],
              "text": "issue"
            },
            {
              "indices": [
                141,
                146
              ],
              "text": "ssue "
            },
            {
              "indices": [
                250,
                255
              ],
              "text": "sues/"
            },
            {
              "indices": [
                316,
                321
              ],
              "text": "ue #1"
            },
            {
              "indices": [
                425,
                430
              ],
              "text": "es/18"
            },
            {
              "indices": [
                487,
                492
              ],
              "text": " #187"
            }
          ]
        }
      ],
      "url": "https://github.com/onurkaraman/onurkaraman.github.io/blob/867426e59d61356872cbf8747ad398444efa36eb/completed.html"
    },
    {
      "path": "Analysis/Tweets_Data/Data/Tweets_GNietoNieto.txt",
      "repository": "lunrongchen/Setrend",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_GNietoNieto.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "GNietoNieto\t718525362225987587\t2016-04-08T19:46:10\tCan't make it up: \"Euro zone banks should not be allowed to own too much of their own countries' debt\" \nGNietoNieto\t717779002723811328\t2016-04-06T18:20:24\tFed minutes: Raising rates \"as soon as April would signal a sense of urgency they did not think appropriate” \nGNietoNieto\t718209049448329217\t2016-04-07T22:49:15\tKKR has a 'chilling' message about the end of the credit cycle  ",
          "matches": [
            {
              "indices": [
                257,
                263
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_GNietoNieto.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "GNietoNieto\t694833462222721024\t2016-02-03T10:43:01\tOFFICIAL: Ireland goes to the polls on February 26. Here's a map of all the votes to watch in 2016 in the euro area \nGNietoNieto\t694792747648172032\t2016-02-03T08:01:14\tThis day in 1690 saw the first issue of paper money in the West   \nGNietoNieto\t694627515550285825\t2016-02-02T21:04:39\tAmerica's too-big-to-fail problem, now a lot bigger.  ",
          "matches": [
            {
              "indices": [
                250,
                255
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/lunrongchen/Setrend/blob/58fa3d0ff28dc92a83cb3f10d5c0637df51176bd/Analysis/Tweets_Data/Data/Tweets_GNietoNieto.txt"
    },
    {
      "path": "pastes/pastes_20240331060515.csv",
      "repository": "osirislab/LeakyPastes-V2",
      "score": 1,
      "stars": 29,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/706296536/contents/pastes/pastes_20240331060515.csv?ref=72f9607bd91684ca7a52815ab73215cc4101ba27",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\r\n\t-- ui creating & handling\r\n\tlocal Library = loadstring(game:HttpGet(\"\"https://raw.githubusercontent.com/violin-suzutsuki/LinoriaLib/main/Library.lua\"\"))()\r",
          "matches": [
            {
              "indices": [
                20,
                28
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/706296536/contents/pastes/pastes_20240331060515.csv?ref=72f9607bd91684ca7a52815ab73215cc4101ba27",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\r\nButton.ClickDetector.MouseClick:Connect(function()\r\n\tButton.ClickAudio.TimePosition = 0\r",
          "matches": [
            {
              "indices": [
                9,
                12
              ],
              "text": "Cli"
            },
            {
              "indices": [
                28,
                31
              ],
              "text": "Cli"
            },
            {
              "indices": [
                62,
                65
              ],
              "text": "Cli"
            }
          ]
        }
      ],
      "url": "https://github.com/osirislab/LeakyPastes-V2/blob/72f9607bd91684ca7a52815ab73215cc4101ba27/pastes/pastes_20240331060515.csv"
    },
    {
      "path": "data/ghosh/word2id.txt",
      "repository": "zeeshan0804/Sarcasm-Detection-NEW",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/886928971/contents/data/ghosh/word2id.txt?ref=69fa3ae1d9394e42b81e6f784b8d1f564531142f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "nutrition\t8954\nhandling\t8955\nimplement\t8956",
          "matches": [
            {
              "indices": [
                15,
                23
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/886928971/contents/data/ghosh/word2id.txt?ref=69fa3ae1d9394e42b81e6f784b8d1f564531142f",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "ako\t16700\ngemini\t16701\nfork\t16702",
          "matches": [
            {
              "indices": [
                10,
                16
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/zeeshan0804/Sarcasm-Detection-NEW/blob/69fa3ae1d9394e42b81e6f784b8d1f564531142f/data/ghosh/word2id.txt"
    },
    {
      "path": "data/wv/vocab/squard2_vocab.txt",
      "repository": "wangyang0922/MRC",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/296792303/contents/data/wv/vocab/squard2_vocab.txt?ref=efaeecf7acaefa43ef8dee349fb651009998e111",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "5667\tboot\n5668\thandling\n5669\tblocked",
          "matches": [
            {
              "indices": [
                15,
                23
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/296792303/contents/data/wv/vocab/squard2_vocab.txt?ref=efaeecf7acaefa43ef8dee349fb651009998e111",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "16006\tasphalte\n16007\tpavements\n16008\tsour",
          "matches": [
            {
              "indices": [
                15,
                20
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/wangyang0922/MRC/blob/efaeecf7acaefa43ef8dee349fb651009998e111/data/wv/vocab/squard2_vocab.txt"
    },
    {
      "path": "Analysis/Tweets_Data/Data/Tweets_moorehn.txt",
      "repository": "lunrongchen/Setrend",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_moorehn.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "moorehn\t719543196804653056\t2016-04-11T15:10:41\t💯 \nmoorehn\t719536400518012928\t2016-04-11T14:43:40\tmdiamondapp thank you! Gave it a signal boost. Hope you get good people.\nmoorehn\t719536295752687616\t2016-04-11T14:43:15\tReally great paper. Go for it, guys!  ",
          "matches": [
            {
              "indices": [
                133,
                139
              ],
              "text": "nal bo"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_moorehn.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "moorehn\t718896041643417600\t2016-04-09T20:19:07\ttomfgoodwin hahaha\nmoorehn\t718892559163113475\t2016-04-09T20:05:17\tIf you need me, I'll be on LinkedIn handling notifications until 2020. (Thank you for all the generous messages) \nmoorehn\t718862029264723969\t2016-04-09T18:03:58\tRamsey is the abc15 PetSmart Charities Pet Of The Week! Visit him at our TourForLife2016 event Halorsq! adoptme ",
          "matches": [
            {
              "indices": [
                149,
                157
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/lunrongchen/Setrend/blob/58fa3d0ff28dc92a83cb3f10d5c0637df51176bd/Analysis/Tweets_Data/Data/Tweets_moorehn.txt"
    },
    {
      "path": "tokenizer.csv",
      "repository": "warriorwizard/suicidal-ideation-detection",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/628701973/contents/tokenizer.csv?ref=f30817e841786e933acc3375d489eb703e189009",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "syndrome,3708\nsignal,3709\nfragile,3710",
          "matches": [
            {
              "indices": [
                14,
                20
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/628701973/contents/tokenizer.csv?ref=f30817e841786e933acc3375d489eb703e189009",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "abd,16006\ncolum,16007\nnpd,16008",
          "matches": [
            {
              "indices": [
                16,
                21
              ],
              "text": "16007"
            }
          ]
        }
      ],
      "url": "https://github.com/warriorwizard/suicidal-ideation-detection/blob/f30817e841786e933acc3375d489eb703e189009/tokenizer.csv"
    },
    {
      "path": "Python/_备选推文选题-2025-M1-M6.md",
      "repository": "arlionn/lianxhta",
      "score": 1,
      "stars": 2,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/970562647/contents/Python/_%E5%A4%87%E9%80%89%E6%8E%A8%E6%96%87%E9%80%89%E9%A2%98-2025-M1-M6.md?ref=a3810b363a064a1119112838a60e3b18d188973a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\nMachine learning has been the corner stone in analysing and extracting information from data and often a problem of missing values is encountered. Missing values occur because of various factors like missing completely at random, missing at random or missing not at random. All these may result from system malfunction during data collection or human error during data pre-processing. Nevertheless, it is important to deal with missing values before analysing data since ignoring or omitting missing values may result in biased or misinformed analysis. In literature there have been several proposals for handling missing values. In this paper, we aggregate some of the literature on missing data particularly focusing on machine learning techniques. We also give insight on how the machine learning approaches work by highlighting the key features of missing values imputation techniques, how they perform, their limitations and the kind of data they are most suitable for. We propose and evaluate two methods, the k nearest neighbor and an iterative imputation method (missForest) based on the random forest algorithm. Evaluation is performed on the Iris and novel power plant fan data with induced missing values at missingness rate of $5 \\%$ to $20 \\%$. We show that both missForest and the k nearest neighbor can successfully handle missing values and offer some possible future research direction.\n",
          "matches": [
            {
              "indices": [
                606,
                614
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/970562647/contents/Python/_%E5%A4%87%E9%80%89%E6%8E%A8%E6%96%87%E9%80%89%E9%A2%98-2025-M1-M6.md?ref=a3810b363a064a1119112838a60e3b18d188973a",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "\nAs you can see, the use\\_llm mode offers higher accuracy than marker or gemini alone.\n",
          "matches": [
            {
              "indices": [
                73,
                79
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/arlionn/lianxhta/blob/a3810b363a064a1119112838a60e3b18d188973a/Python/_%E5%A4%87%E9%80%89%E6%8E%A8%E6%96%87%E9%80%89%E9%A2%98-2025-M1-M6.md"
    },
    {
      "path": "Analysis/Tweets_Data/Data/Tweets_STJamesl.txt",
      "repository": "lunrongchen/Setrend",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_STJamesl.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "STJamesl\t710589397947781121\t2016-03-17T22:11:29\tOf course Ed Miliband was 10 points clear at this point... PartyPooper\nSTJamesl\t710588301099573249\t2016-03-17T22:07:07\tOsborne's own ratings as Chancellor now back to quite strongly negative (bad job 46% good job 23%). 51% think the gvt handling economy badly\nSTJamesl\t710559054263930880\t2016-03-17T20:10:54\tBlackthorn blossom, golden Spring evening light, milder weather, blackbirds singing.  It's definitely here Spring ",
          "matches": [
            {
              "indices": [
                285,
                293
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_STJamesl.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "STJamesl\t708061574389432320\t2016-03-10T22:46:49\tSTJamesl JeremyJHardy He did a \"can't believe you squares didn't get that\" thang.\nSTJamesl\t708061739343024128\t2016-03-10T22:47:28\tiMcKenzied Re-issue worth buying for the new Kinnock foreword - very punchy\nSTJamesl\t708061473923330052\t2016-03-10T22:46:25\tSome weeks the Crime Watch update and Question Time could come from the same studio...",
          "matches": [
            {
              "indices": [
                192,
                197
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/lunrongchen/Setrend/blob/58fa3d0ff28dc92a83cb3f10d5c0637df51176bd/Analysis/Tweets_Data/Data/Tweets_STJamesl.txt"
    },
    {
      "path": "frequencies.txt",
      "repository": "PKaracs/Homework17",
      "score": 1,
      "stars": 0,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/883223943/contents/frequencies.txt?ref=d9bb745b87d1aa3145a78e1f6b31d9d94dee0ec9",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "4683 4 handfuls\n4684 4 handling\n4685 4 happening",
          "matches": [
            {
              "indices": [
                23,
                31
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/883223943/contents/frequencies.txt?ref=d9bb745b87d1aa3145a78e1f6b31d9d94dee0ec9",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "4749 4 invariable\n4750 4 issue\n4751 4 italian",
          "matches": [
            {
              "indices": [
                25,
                30
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/PKaracs/Homework17/blob/d9bb745b87d1aa3145a78e1f6b31d9d94dee0ec9/frequencies.txt"
    },
    {
      "path": "Analysis/Tweets_Data/Data/Tweets_BrianSozzi.txt",
      "repository": "lunrongchen/Setrend",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_BrianSozzi.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "BrianSozzi\t717653009443000320\t2016-04-06T09:59:45\tVideo: Woman destroys FLGovScott in front of Starbucks baristas  \nBrianSozzi\t717033195745722369\t2016-04-04T16:56:50\tReview: UnderArmour proves that the future of smart shoes is bright w/SpeedForm Gemini 2 RE  \nBrianSozzi\t717647274936045568\t2016-04-06T09:36:58\tUpcoming UnderArmour StephenCurry30 sneak -- seen wearing last night ",
          "matches": [
            {
              "indices": [
                246,
                252
              ],
              "text": "Gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/55176322/contents/Analysis/Tweets_Data/Data/Tweets_BrianSozzi.txt?ref=58fa3d0ff28dc92a83cb3f10d5c0637df51176bd",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "BrianSozzi\t714895185499578368\t2016-03-29T19:21:08\t7-11 Slurpee donuts anyone? jlguyon carletonenglish \nBrianSozzi\t714876172937474048\t2016-03-29T18:05:36\tWhy Costco won't have any problems handling California's new $15 minimum wage  by briansozzi \nBrianSozzi\t714884741951041536\t2016-03-29T18:39:39\tThe McDonald's App Just Got The Most Amazing Update Ever  Thank you for sharing rheannabellomo  MCD",
          "matches": [
            {
              "indices": [
                188,
                196
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/lunrongchen/Setrend/blob/58fa3d0ff28dc92a83cb3f10d5c0637df51176bd/Analysis/Tweets_Data/Data/Tweets_BrianSozzi.txt"
    },
    {
      "path": "tweets/sriramvasan.csv",
      "repository": "parasg1999/Tweet-dataset",
      "score": 1,
      "stars": 6,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/262527255/contents/tweets/sriramvasan.csv?ref=cce002914c45578e34fa05ef93de73b871c0dd45",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "447923340473147400,Mon Mar 24 02:30:21 +0000 2014,@ebay @ebayindia agents speak in different voices. An agent on recorded chat assures replacement. Later, on phone, another goes back on word\n447921905505296400,Mon Mar 24 02:24:38 +0000 2014,@ebay, your India customer service is appalling. @ebayindia sends me product with leaking batteries inside and drags resolution of issue.\n439641108792897540,Sat Mar 01 05:59:43 +0000 2014,RT @ML_Hipster: A machine learning researcher, a crypto-currency expert, and an Erlang programmer walk into a bar. Facebook buys the bar fo…",
          "matches": [
            {
              "indices": [
                372,
                377
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/262527255/contents/tweets/sriramvasan.csv?ref=cce002914c45578e34fa05ef93de73b871c0dd45",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "415483064710492160,Tue Dec 24 14:04:16 +0000 2013,Big Data and the Role of Intuition - @HarvardBiz http://t.co/tizF8HHfYJ\n415427810681839600,Tue Dec 24 10:24:43 +0000 2013,Yeah, right! \"No conflict of interest in handling Petroleum, Environment ministry,\" says Moily. http://t.co/g0D9VedjeE\n415345502599671800,Tue Dec 24 04:57:39 +0000 2013,“There is still a desire for [print] magazines.\"  Hmmm. \"@tkcuny: Are we in the midst of a print magazine comeback? http://t.co/MTopduIECG\"",
          "matches": [
            {
              "indices": [
                213,
                221
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/parasg1999/Tweet-dataset/blob/cce002914c45578e34fa05ef93de73b871c0dd45/tweets/sriramvasan.csv"
    },
    {
      "path": "hm_urls.txt",
      "repository": "tech234a/annotation-urls",
      "score": 1,
      "stars": 31,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/389307289/contents/hm_urls.txt?ref=5c0300471b87c6a5f3dd8883484f8155b5ff33d7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "http://www.smarttravel.cz1.ua/\nhttps://gemini.ua\nhttps://itunes.apple.com/tr/album/iyi-ki-dogdun-perran-single/id850116468?l=tr",
          "matches": [
            {
              "indices": [
                39,
                45
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/389307289/contents/hm_urls.txt?ref=5c0300471b87c6a5f3dd8883484f8155b5ff33d7",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "https://www.beatport.com/release/awakening/2208698\nhttp://multi-com.eu/,browse,id_gr,1853,key,blocking-signal-waves-cases,smenu,gsm.html\nhttps://plus.google.com/u/0/b/111499336529826982581/111499336529826982581",
          "matches": [
            {
              "indices": [
                103,
                109
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/tech234a/annotation-urls/blob/5c0300471b87c6a5f3dd8883484f8155b5ff33d7/hm_urls.txt"
    },
    {
      "path": "src/content/issues/24-11-21-ainews-lmsys-killed-model-versioning-gpt-4o-1120-gemini-exp-1121.md",
      "repository": "smol-ai/ainews-web-2025",
      "score": 1,
      "stars": 20,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/971669456/contents/src/content/issues/24-11-21-ainews-lmsys-killed-model-versioning-gpt-4o-1120-gemini-exp-1121.md?ref=f02123e0cfadd1d72ea9ea21cb389a85333bbd17",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "   - Members suggested switching to **Mistral-Large**, **Mistral-Small**, or **Mistral-Tiny** to continue using the service.\n- **OpenRouter API Documentation Cleared Up**: Users expressed confusion regarding certain functionalities in the OpenRouter API documentation, specifically around context window capabilities.\n   - It was suggested to enhance clarity in documentation to aid understanding for users integrating OpenRouter with tools like LangChain.\n- **New Gemini Experimental Models Update**: The **Gemini Experimental 1121** model has been introduced, with claims of improved coding, reasoning, and vision capabilities.\n   - Users noted the existing quota restrictions shared with the **LearnLM** model and expressed curiosity about the model’s performance.\n- **File Upload Capability in Models**: Discussion arose regarding file upload limitations, with users questioning if any models accept non-image formats.\n   - It was clarified that image uploads are supported, and the recent infrastructure upgrades may have lifted the previous **4MB** restriction.",
          "matches": [
            {
              "indices": [
                465,
                471
              ],
              "text": "Gemini"
            },
            {
              "indices": [
                508,
                514
              ],
              "text": "Gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/smol-ai/ainews-web-2025/blob/f02123e0cfadd1d72ea9ea21cb389a85333bbd17/src/content/issues/24-11-21-ainews-lmsys-killed-model-versioning-gpt-4o-1120-gemini-exp-1121.md"
    },
    {
      "path": "sitemapf/01043133.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/01043133.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0807102148 rhetorical dimensions in criticism\nb00v0a42pi gemini found the guardians english edition\n386506745x kritzel dich durch dein leben mein leben in bildern",
          "matches": [
            {
              "indices": [
                57,
                63
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/01043133.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "3741520411 malen und entspannen into the woods\nb00anz54bc an inflight simulation of handling qualities of the sv5p pilot lifting body with various feedback gains and rudder to aileron interconnect ratios\nb01lyh5qrr mes fiches abc du bac francais 2de by anne cassounogues 20140403",
          "matches": [
            {
              "indices": [
                84,
                92
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/01043133.txt"
    },
    {
      "path": "sitemapf/99353764.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/99353764.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1358209979 spars\n0727720333 fuel management and handling\nb005qsbxde in ihrer dunkelsten stunde: ein fall fur avvocato guerrieri 4 - roman (german edition)",
          "matches": [
            {
              "indices": [
                48,
                56
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/99353764.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b004gczbiy manuel du parfait jardinier - potager, fruitier, fleuriste\n073604342x journal of sport rehabilitation: issue 2, 2004\nb01guekt14 kanye west: j.d. rockefellers book club",
          "matches": [
            {
              "indices": [
                114,
                119
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/99353764.txt"
    },
    {
      "path": "Code/Data/Sentiment data/TweetSentiment_BTC_2022-03-10_13-15.csv",
      "repository": "bprovendier/NN-for-Sentiment-Analysis",
      "score": 1,
      "stars": 35,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/505487430/contents/Code/Data/Sentiment%20data/TweetSentiment_BTC_2022-03-10_13-15.csv?ref=e7b1d2c96f7c71626db26cd0bdd0396e0cc66506",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "3)330\nStop-loss Orders:\n1) 400\n#USDT #BREAKING #cryptotrading #Bitcoin #BTC    #RussianUkrainianWar\",the100xgems upwaydao today big short signal bnb usdt exchange binance futures trade short 20x entry 378 takeprofit 1350 2340 3330 stoploss 1 400 usdt breaking cryptotrading bitcoin btc russianukrainianwar,0.08944594860076904,0.8812760710716248,0.029277991503477097\n2022-03-10,13:15,Current #Bitcoin price is 39272.340789943264$,current bitcoin price is 39272340789943264,0.02532055415213108,0.936803936958313,0.03787554055452347\n2022-03-10,13:15,\"@letecho_com TODAY BIG SHORT SIGNAL!!! \nCoin: #BNB /USDT ",
          "matches": [
            {
              "indices": [
                138,
                144
              ],
              "text": "signal"
            },
            {
              "indices": [
                577,
                583
              ],
              "text": "SIGNAL"
            }
          ]
        }
      ],
      "url": "https://github.com/bprovendier/NN-for-Sentiment-Analysis/blob/e7b1d2c96f7c71626db26cd0bdd0396e0cc66506/Code/Data/Sentiment%20data/TweetSentiment_BTC_2022-03-10_13-15.csv"
    },
    {
      "path": "sitemapf/21907852.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/21907852.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00xxep18q nature in indian philosophy and cultural traditions\nb00iwugyqy gemini (sleeping dragons book 3) (english edition)\nb009c96hde lavant-scene cinema n 56. viva maria de louis malle. suivi de un jour a paris, cours metrage de serge korber. suivi dun supplement photos: repulsion, de roman polanski. lavant-scene cinema. n 56. fevrier 1966. (cinema, periodiques, periodicals)",
          "matches": [
            {
              "indices": [
                74,
                80
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/21907852.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b0716hlw81 the revelation of the holy ghost: bible study in a year (english edition)\nb071d9xm91 safety of web applications: risks, encryption and handling vulnerabilities with php\nb01bwq1554 chejovducir: analisis de una traduccion del cuento un hombre enfundado de anton chejov (spanish edition)",
          "matches": [
            {
              "indices": [
                146,
                154
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/21907852.txt"
    },
    {
      "path": "sitemapf/48976485.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/48976485.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1349274828 african economies in transition volume 1 the changing role of the state\nb073513gpt scifan magazine issue 7 a science fantasy digital editorial english edition\nb0000e9fl6 savoir choisir et acheter son bateau",
          "matches": [
            {
              "indices": [
                110,
                115
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/48976485.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00z25ebee dead ink a karma world romance karma series book 4 english edition\nb01k27xm28 whats up gemini in 2017\nb01lzz7nhd my casino caper bonus 77 minute audio-book mp3 english edition",
          "matches": [
            {
              "indices": [
                98,
                104
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/48976485.txt"
    },
    {
      "path": "sitemapf/53051944.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/53051944.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00l18y59w critical analysis of organizations: theory, practice, revitalization\n3764351411 airways smooth muscle: neurotransmitters, amines, lipid mediators and signal transduction\nb01lzs2sp4 gedachtnis-storungen bei ms: kognitive leistungsstorungen",
          "matches": [
            {
              "indices": [
                161,
                167
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/53051944.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "1439217262 yushi and the tall man\n0071842829 world-class warehousing and material handling\nb00kwy2mnc tuhannen ja yhden vuoden tarinat: one thousand and one years",
          "matches": [
            {
              "indices": [
                82,
                90
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/53051944.txt"
    },
    {
      "path": "sitemapf/64341482.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/64341482.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "2852069121 repertoire general des aliments, tome 3 : table de composition des fruits exotiques\n3540226109 developments in spatial data handling\n2092021044 le petit chaperon rouge",
          "matches": [
            {
              "indices": [
                135,
                143
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/64341482.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "8170303680 mathematics as known to the vedic samhitas\n184072661x gemini\nb01m1madvg schlagfertig kontern: immer den besten spruch parat (german edition)",
          "matches": [
            {
              "indices": [
                65,
                71
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/64341482.txt"
    },
    {
      "path": "sitemapf/41015245.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/41015245.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0951016644 gaspard de la nuit : pour piano\nb00f8efel8 obstetric and gynecologic anesthesia, an issue of anesthesiology clinics, e-book\nb06y5tzf9k verborgene therapien: was dir der arzt nicht sagt (german edition)",
          "matches": [
            {
              "indices": [
                95,
                100
              ],
              "text": "issue"
            },
            {
              "indices": [
                119,
                122
              ],
              "text": "cli"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/41015245.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00gb5l4h6 funktion, begriff, bedeutung: funf logische studien\nb000qcqxg2 information handling in astronomy\n1601250878 pathfinder players guide: curse of the crimson throne",
          "matches": [
            {
              "indices": [
                86,
                94
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/41015245.txt"
    },
    {
      "path": "sitemapf/08281768.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/08281768.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b01fn2lvys jewish and greek communities in egypt: entrepreneurship and business before nasser\n3540147446 handling von informationssystemen mit sql\n2091721395 svt term s specialite",
          "matches": [
            {
              "indices": [
                105,
                113
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/08281768.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b007kohs8q par-dela le mekong\nb01k2jbo8k secondary headache an issue of neurologic clinics 1e (the clinics: radiology) by randolph w. evans md (2014-04-24)\n3772476023 vertikal pflanzen (kreativ.inspiration): hangende garten begrunte wande und bluhende paletten",
          "matches": [
            {
              "indices": [
                63,
                68
              ],
              "text": "issue"
            },
            {
              "indices": [
                83,
                86
              ],
              "text": "cli"
            },
            {
              "indices": [
                99,
                102
              ],
              "text": "cli"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/08281768.txt"
    },
    {
      "path": "sitemapf/40923184.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/40923184.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b01k32agiu caught in the surf by jasinda wilder (2015-07-22)\n9004228594 the conversos and moriscos in late medieval spain and beyond: the morisco issue\nb01dk3jlgc guardians: a wasteland novel",
          "matches": [
            {
              "indices": [
                146,
                151
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/40923184.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "2266093894 vu de dos\n1786391481 postharvest: an introduction to the physiology and handling of fruit and vegetables\nb00tsoz5ls die vergessene revolution oder die wiedergeburt des antiken wissens",
          "matches": [
            {
              "indices": [
                83,
                91
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/40923184.txt"
    },
    {
      "path": "sitemapf/51472389.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/51472389.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00g6tc1ew the legal framework of the constitution\n3330054360 jimat (chinga) thread: in vivo tissue reactivity and suture handling characteristics\nb00go8hfcs a flash of teenage angst (english edition)",
          "matches": [
            {
              "indices": [
                94,
                99
              ],
              "text": "issue"
            },
            {
              "indices": [
                122,
                130
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/51472389.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0761390766 army special forces: elite operations\nb01lqs9vmm lovefmd magazine issue5 (royal issue) (english edition)\nb00fadkn6s the soldier (english edition)",
          "matches": [
            {
              "indices": [
                77,
                82
              ],
              "text": "issue"
            },
            {
              "indices": [
                91,
                96
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/51472389.txt"
    },
    {
      "path": "sitemapf/96492505.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/96492505.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00mha7pb8 la seconde vie damelie\nb018flyt4i gemini-your fortune for 2016: relationship / career / health / finance (english edition)\n0978878221 the chapter 13 plan training workbook",
          "matches": [
            {
              "indices": [
                45,
                51
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/96492505.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b01d5f0e84 hajimetenonikkeinihyakunijuugosakimonominiandoraajidekasegikatamadewakaruhon: kasegutoushi (japanese edition)\n0471323519 bug proofing visual basic: a guide to error handling and prevention\nb00dyy9j56 mistiche e filosofie",
          "matches": [
            {
              "indices": [
                176,
                184
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/96492505.txt"
    },
    {
      "path": "sitemapf/21366094.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/21366094.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0736217908 avenues little language books: level a\nb007n5vjpk handling difficult customers (english edition)\n8184483791 medical biochemistry for physiotherapy students",
          "matches": [
            {
              "indices": [
                61,
                69
              ],
              "text": "handling"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/21366094.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b018r88ruq herzblut - starker als der tod (german edition)\n8883959019 astrology crystal talisman gemini\n3847906062 das traumjob-experiment: 30 jobs in einem jahr",
          "matches": [
            {
              "indices": [
                97,
                103
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/21366094.txt"
    },
    {
      "path": "sitemapf/69205599.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/69205599.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b01dhazm3o understanding suicide\n3540515593 auxiliary signal design in fault detection and diagnosis\nb01mxrczhl lincoln militia: war of 1812 (english edition)",
          "matches": [
            {
              "indices": [
                54,
                60
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/69205599.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "0117520810 kent impact study: overall assessment\nb00267iqjc how to run a store at a profit; figuring expenses and mark-up--counter and window displays--how a retailer increased business 400 in fourteen months--short cuts in handling trade--larger net profits--training your men to sell..\nb01hxjerta pro sql server internals",
          "matches": [
            {
              "indices": [
                224,
                232
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/69205599.txt"
    },
    {
      "path": "sitemapf/90416760.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/90416760.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "9992325844 beautiful bouquets\n197452759x volume 8 issue 1\nb01k9205ai understanding migraine and other headaches family doctor series by marcia wilkinson 1999-08-06",
          "matches": [
            {
              "indices": [
                50,
                55
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/90416760.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "159880829x moon spotlight providence\n1456006886 advice & tips for handling your mental illness\n3945118433 wolfi",
          "matches": [
            {
              "indices": [
                66,
                74
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/90416760.txt"
    },
    {
      "path": "sitemapf/29248178.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/29248178.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00titu6l2 pour distinguer le magnetisme de lhypnotisme analogies et differences\nb0117gaxxo white storm book 3 gemini force i english edition\nb007qbx0vw flanagans wedding english edition",
          "matches": [
            {
              "indices": [
                111,
                117
              ],
              "text": "gemini"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/29248178.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00hjelz2w 101 things you did not know about osama bin laden english edition\nb00bwv9bq0 guide to signal pathways in immune cells\nb01dq57yyu as the gods will the second series #39",
          "matches": [
            {
              "indices": [
                97,
                103
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/29248178.txt"
    },
    {
      "path": "sitemapf/54829553.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/54829553.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00z9zudzs kompetenzentwicklung als werkzeug des personalmanagements in einem kmu\nb001sn7q9q shark fear shark awareness the secret issue english edition\n2290033146 laissees pour mortes  le lynchage des femmes de hassi messaoud",
          "matches": [
            {
              "indices": [
                131,
                136
              ],
              "text": "issue"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/54829553.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b013rjnrau socken stricken leicht gemacht - hei geliebt genial fur einsteiger kreativstartup german edition\n8131800806 digital signal processing using matlab and wavelets\nb00apu3scw the quick guide to cloud computing and cyber security english edition",
          "matches": [
            {
              "indices": [
                127,
                133
              ],
              "text": "signal"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/54829553.txt"
    },
    {
      "path": "sitemapf/63806003.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/63806003.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b001fwy84g julius caesar: a brief biography (english edition)\nb008848a7u streamlining digital signal processing: a tricks of the trade guidebook\nb01nciwgpf se soigner au naturel : mode demploi: les remedes naturels, les exercices, et lalimentation : autant de cles...",
          "matches": [
            {
              "indices": [
                94,
                100
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/63806003.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b014z40cgi the wait (english edition)\n0117069159 brc global standard for food safety: interpretation guideline, issue 6\n1899649379 childrens war replica pack",
          "matches": [
            {
              "indices": [
                112,
                117
              ],
              "text": "issue"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/63806003.txt"
    },
    {
      "path": "sitemapf/90114966.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/90114966.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00crud2ce 1000 names for baby girls (english edition)\nb00f2h469c anxiety as symptom and signal\n9059114078 the increasing role of nutrition and genomics in the prevention and management of disease",
          "matches": [
            {
              "indices": [
                89,
                95
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/90114966.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "4047283843 puriken ikkyu nikyu koshiki tekisuto bukku : purizabudo furawa zenkoku kyogikai gino kentei.\nb009wusohk rein handling (when your horse rears book 6) (english edition)\n1425971253 soul-sick nation: an astrologers view of america",
          "matches": [
            {
              "indices": [
                120,
                128
              ],
              "text": "handling"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/90114966.txt"
    },
    {
      "path": "sitemapf/72682910.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/72682910.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b06wvn2sl6 sisi und ihre familie (german edition)\nb01lw6jufl bioworld mens batman bat signal t-shirt\nb00a30s24s brain power: from neurons to networks (kindle single) (english edition)",
          "matches": [
            {
              "indices": [
                86,
                92
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/72682910.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b074ggv5bg lucians true history (illustrated) (english edition)\nb014c2kd1c genie in the stars - gemini: daily predictions for 2016 (english edition)\nb01g79n8gk la rosa de sangre",
          "matches": [
            {
              "indices": [
                96,
                102
              ],
              "text": "gemini"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/72682910.txt"
    },
    {
      "path": "sitemapf/96556797.txt",
      "repository": "404-html/agcasinbook",
      "score": 1,
      "stars": 1,
      "textMatches": [
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/96556797.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "2364804930 tout lart de magic, zendikar\n0897663837 third colloquium in biological sciences: cellular signal transduction\nb01605qg56 notes pour un monde meilleur: de lautre cote du mur 2",
          "matches": [
            {
              "indices": [
                101,
                107
              ],
              "text": "signal"
            }
          ]
        },
        {
          "object_url": "https://api.github.com/repositories/180770706/contents/sitemapf/96556797.txt?ref=7df58544609830f91402d872e9600daa2689a05e",
          "object_type": "FileContent",
          "property": "content",
          "fragment": "b00fdqvn20 carping on (english edition)\nb01js6ajm0 sleep, an issue of clinics in chest medicine - e-book\nb01duebg8w computer methods in operations research",
          "matches": [
            {
              "indices": [
                61,
                66
              ],
              "text": "issue"
            },
            {
              "indices": [
                70,
                73
              ],
              "text": "cli"
            }
          ]
        }
      ],
      "url": "https://github.com/404-html/agcasinbook/blob/7df58544609830f91402d872e9600daa2689a05e/sitemapf/96556797.txt"
    }
  ]
}