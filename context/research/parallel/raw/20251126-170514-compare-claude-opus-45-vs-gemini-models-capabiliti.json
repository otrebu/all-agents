[
  {
    "domain": "www.reddit.com",
    "excerpts": [
      "[Skip to main content]() It seems opus 4.5 is just too amazing even compared to gemini 3 : r/Bard Open menu Open navigation [](/) Go to Reddit Home\n\nr/Bard A chip A close button\n\n[Log In](https://www.reddit.com/login/) Log in to Reddit\n\nExpand user menu Open settings menu\n\n[Go to Bard](/r/Bard/) [r/Bard](/r/Bard/) •\n\n[Independent-Wind4462](/user/Independent-Wind4462/) [Deutsch](https://www.reddit.com/r/Bard/comments/1p5q4eq/it_seems_opus_45_is_just_too_amazing_even/?tl=de)\n\n# It seems opus 4.5 is just too amazing even compared to gemini 3\n\nShare\n\n* * *\n\n[microsoft365](/user/microsoft365/) • Promoted\n\nMicrosoft 365 Copilot Chat helps you nail the meeting prep by analyzing, summarizing and drafting reports, so you can shine anywhere, anytime.\nLearn More\n\nm365copilot.com\n\nCollapse video player\n\n* * *\n\n[](/user/TechnologyMinute2714/)\n\n[TechnologyMinute2714](/user/TechnologyMinute2714/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqkta9p/)\n\nThey're both great SOTA models and have their use cases, i feel like Claude is better for agentic coding and Gemini is better for multimodality and it's cheaper. 165\n\n[](/user/Neurogence/)\n\n[Neurogence](/user/Neurogence/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqm6q6p/)\n\nI was testing Gemini 3 Pro and Sonnet 4.5 side by side yesterday, and to my shock, Sonnet 4.5 is a lot better on instructions following, creativity, and doesn't hallucinate as much. If even Sonnet can go toe to toe with Gemini 3 Pro, Opus 4.5 is likely way ahead of Gemini 3 in terms of intelligence and capability outside of vision based tasks.\n48\n\n[](/user/QuantityGullible4092/)\n\n[QuantityGullible4092](/user/QuantityGullible4092/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqmeapb/)\n\nLots of hallucinations from Gemini\n\n21\n\n3 more replies\n\n3 more replies [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nqmeapb/?force-legacy-sct=1) [](/user/Substantial_Big550/)\n\n[Substantial\\_Big550](/user/Substantial_Big550/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqnfwz0/)\n\nCodex and Gemini try to be too smart and don't listen to your instructions, but Claude will do what you say no matter what. There are pros and cons to both. When you're working on something and just want the AI to do what you say, Yes, Claude is right. Gemini and Codex work well for refactoring in line with modern standards.\n4\n\n1 more reply\n\n1 more reply [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nqnfwz0/?force-legacy-sct=1) 8 more replies\n\n8 more replies [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nqm6q6p/?force-legacy-sct=1) [](/user/Suitable-Opening3690/)\n\n[Suitable-Opening3690](/user/Suitable-Opening3690/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqm2nu9/)\n\nThe issue on “price” is the max plans now default to opus. So price isn’t an issue. Claude Max users get Opus all the time now they removed the opus limits. So in coding opus is probably king again. Gemini is best day to day. 9\n\n3 more replies\n\n3 more replies [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nqm2nu9/?force-legacy-sct=1) 3 more replies\n\n3 more replies [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nqkta9p/?force-legacy-sct=1) [](/user/randombsname1/)\n\n[randombsname1](/user/randombsname1/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqlfbzw/)\n\nThis is fantastic.\nClaude Code, imo, is still way way ahead of the game in terms of tooling. Skills, workflows, agent customization, hooks, mcp integration, etc...etc...\n\nCC just kind of does all of it better. So, good to see that they are keeping pace. 40 [](/user/NectarineDifferent67/)\n\n[NectarineDifferent67](/user/NectarineDifferent67/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqku491/)\n\nOpus 4.5 / 200K / In $5 / Out $25 vs Gemini 3 Pro Preview / 1.05M / In $2 / Out $12\n\n63\n\n[](/user/JoshuaJosephson/)\n\n[JoshuaJosephson](/user/JoshuaJosephson/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqm1hlr/)\n\nonly the first 200k tokens is 2-12. The rest is 4-18\n\nStill 20% lower than Opus 4.5's 5-25 though which is nice.\n ... \n7\n\n2 more replies\n\n2 more replies [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nqllyw7/?force-legacy-sct=1) 3 more replies\n\n3 more replies [Continue this thread](/r/Bard/comments/1p5q4eq/comment/nql15r1/?force-legacy-sct=1) [](/user/vanishing_grad/)\n\n[vanishing\\_grad](/user/vanishing_grad/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqlneyz/)\n\narc-agi is impressive but the other benchmarks are pretty cherrypicked for software dev, which is fair because it's where anthropic is focusing efforts but also not really representative\n\n5 [](/user/neoqueto/)\n\n[neoqueto](/user/neoqueto/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqlr9eg/)\n\nClaude has always been the goat of prompt adherence so I hope this is some healthy competition in that particular area that Google won't sleep through preparing the release of 3.5 Pro\n\n5 [](/user/Kako05/)\n\n[Kako05](/user/Kako05/)\n\n• [](/r/Bard/comments/1p5q4eq/comment/nqmkoe8/)\n\nGemini 3.0 is great for its price and handling of large contexts."
    ],
    "rank": 1,
    "title": "It seems opus 4.5 is just too amazing even compared to ...",
    "url": "https://www.reddit.com/r/Bard/comments/1p5q4eq/it_seems_opus_45_is_just_too_amazing_even/"
  },
  {
    "domain": "www.vellum.ai",
    "excerpts": [
      "Though very formidable performance, Opus 4.5 still gets beat out by Gemini 3 Pro on Humanity’s Last Exam by ~7% without search and ~2% with search enabled. * **Financial savant:** While Opus 4.5's Vending-Bench 2performance resulted in a balance of $4,967.06, a 23% increase over Sonnet 4.5, but still falling short to Gemini 3 Pro’s net wroth of $5,478.16. * **Safer than the rest:** Concerns around AI safety have been gaining prevalence in the space since bad actors and actors have slowly been penetrating the AI security measures. So on agentic safety evaluations, Anthropic emphasized Opus 4.5’s industry-leading robustness against prompt injection attacks and exhibiting ~10% less concerning behavior than GPT 5.1 and Gemini 3 Pro. ## **Coding capabilities**\n\nCoding benchmarks test a model’s ability to generate, understand, and fix code. They are crucial indicators of a model’s utility in software development workflows, from simple script generation to complex bug resolution.\nSWE-bench evaluates real-world GitHub bug fixing, while Terminal-Bench tests command-line proficiency needed for development and operations work. * Claude Opus 4.5 delivers a state-of-the-art 80.9% on SWE-bench, outperforming Gemini 3 Pro (76.2%) and GPT 5.1 (76.3%), which makes it one of the strongest models for real bug resolution. * On Terminal-Bench, Opus 4.5 scores 59.3%, ahead of Gemini 3 Pro (54.2%) and significantly outperforming GPT 5.1 (47.6%), confirming its superior capability in command-line environments. ## Reasoning capabilities\n\nReasoning benchmarks are designed to evaluate a model's ability to think logically, solve novel problems, and understand complex, abstract concepts. Strong performance here is essential for building agents that can handle multi-step, intricate workflows. The Abstract Reasoning Corpus (ARC-AGI-2) is a test of fluid intelligence, requiring the model to solve novel visual puzzles from just a few examples.\n ... \nlAST UPDATED\n\nNov 25, 2025\n\nshare post\n\n[](#) [](#) [](#) [](#)\n\nExpert verified\n\nRelated Posts\n\n[View More](#)\n\nLLM basics\n\nNovember 20, 2025\n\n•\n\n10 min\n\nGumloop vs. n8n vs. Vellum (Platform Comparison)\n\n[](/blog/gumloop-vs-n8n-vs-vellum)\n\nGuides\n\nNovember 18, 2025\n\n•\n\n8 min\n\nGoogle Gemini 3 Benchmarks\n\n[](/blog/google-gemini-3-benchmarks)\n\nNovember 11, 2025\n\n•\n\n15 min\n\nAI Agent Use Cases Guide to Unlock AI ROI\n\n[](/blog/ai-agent-use-cases-guide-to-unlock-ai-roi)\n\nLLM basics\n\nNovember 6, 2025\n\n•\n\n7 min\n\nBeginners Guide to Building AI Agents\n\n[](/blog/beginners-guide-to-building-ai-agents)\n\nProduct Updates\n\nNovember 5, 2025\n\n•\n\n7 min\n\nVellum Product Update | October\n\n[](/blog/vellum-product-update-october-2025)\n\nAll\n\nNovember 3, 2025\n\n•\n\n6 min\n\nI’m done building AI agents\n\n[](/blog/im-done-building-ai-agents)\n\nThe Best AI Tips — Direct To Your Inbox\n\nLatest AI news, tips, and techniques\n\nSpecific tips for Your AI use cases\n\nNo spam\n\nThank you! Your submission has been received! Oops!\n ... \n](/blog/claude-opus-4-5-benchmarks)\n\n[Gumloop vs. n8n vs. Vellum (Platform Comparison) A practical 2025 comparison of Gumloop, n8n, and Vellum that breaks down who each platform is for, what they do well, where they fall short to help you find the right fit for your agentic solution. ](/blog/gumloop-vs-n8n-vs-vellum)\n\n[Google Gemini 3 Benchmarks Explore this breakdown of Gemini 3 Pro’s benchmarks and performance across reasoning, math, multimodal, and agentic benchmark to learn what results actually mean for building more powerful AI agents. ](/blog/google-gemini-3-benchmarks)\n\n[AI Agent Use Cases Guide to Unlock AI ROI Explore AI agent use cases by industry with real examples, ROI benchmarks, and a simple plan to begin automating workflows today to unlock AI nativity and ROI](/blog/ai-agent-use-cases-guide-to-unlock-ai-roi)\n\n[Beginners Guide to Building AI Agents A practical beginner guide to buildAI agents.\n ... \n](/blog/how-gravitystack-cut-credit-agreement-review-time-by-200-with-agentic-ai)\n\n[How the Best Product and Engineering Teams Ship AI Solutions Four core practices that enable teams to move 100x faster, without sacrificing reliability. ](/blog/how-the-best-product-and-eng-teams-ship-ai-solutions)\n\n[Evaluation: Claude 4 Sonnet vs OpenAI o4-mini vs Gemini 2.5 Pro Analyzing the difference in performance, cost and speed between the world's best reasoning models. ](/blog/evaluation-claude-4-sonnet-vs-openai-o4-mini-vs-gemini-2-5-pro)\n\n[Document Data Extraction in 2025: LLMs vs OCRs A choice dependent on specific needs, document types and business requirements. ](/blog/document-data-extraction-in-2025-llms-vs-ocrs)\n\n[How to continuously improve your AI Assistant using Vellum Capture edge cases in production and fix them in couple of minutes without redeploying you application."
    ],
    "rank": 2,
    "title": "Claude Opus 4.5 Benchmarks (Explained)",
    "url": "https://www.vellum.ai/blog/claude-opus-4-5-benchmarks"
  },
  {
    "domain": "www.glbgpt.com",
    "excerpts": [
      "**\n\n### **Core improvements in** **Opus** **4\\.5**\n\nClaude Opus 4.5 is [Anthropic’s most intelligent flagship model to date,](https://www.glbgpt.com/hub/claude-sonnet-4-5-the-most-powerful-ai-for-30-hours-of-nonstop-coding/) combining extended reasoning, improved coding reliability, and advanced computer-use capabilities. It introduces enhanced zoom-level inspection for UI elements, more stable multi-step reasoning, better tool-use orchestration, and fully preserved thinking blocks across long sessions. Compared to Opus 4.1, it delivers stronger performance in logic-heavy tasks, complex planning, and agent workflows. ### **Strengths and ideal use cases**\n\nOpus 4.5 is designed for deep reasoning, structured analysis, and tasks requiring precision over flair. It performs exceptionally well in multi-step tool workflows, long-form problem-solving, security engineering reviews, and detailed UI inspection through its improved computer-use interface.\n ... \nIt builds on the [agent-first foundations of Gemini 2.5 Pro](https://www.glbgpt.com/hub/gpt-5-vs-gemini-2-5-pro-a-detailed-ai-model-review/) but adds dynamic generative interfaces, richer spatial understanding, high-frame-rate video reasoning, and complex web UI generation. It is also deeply integrated into Google Search, Android, and Antigravity-based developer tools. ### **Gemini 3 Deep Think mode**\n\nDeep Think amplifies Gemini 3’s already strong reasoning abilities, improving benchmark scores on ARC-AGI-2, Humanity’s Last Exam, and other abstract reasoning tasks. It enables deeper chain-of-thought planning, interprets nuanced mathematical or scientific concepts, and supports more deliberate multi-step logic. ### **Ideal use cases and model strengths**\n\nGemini 3 excels at multimodal understanding—images, videos, screen content, spatial layouts, and long-context cross-media reasoning.\n ... \nIn official benchmarks, Opus 4.5 shows significant jumps in complex problem-solving and coding reasoning compared to Opus 4.1. Gemini 3, however, achieves frontier-level performance in conceptual reasoning through its Deep Think mode and consistently leads on academic-style benchmarks like Humanity’s Last Exam, ARC-AGI-2, and GPQA. It also displays stronger intuition with abstract patterns and high-level conceptual interpretation, especially in science and mathematics. ## **How do Claude** **Opus** **4\\.5 and Gemini 3 compare in multimodal understanding? **\n\nGemini 3 sets a new bar for multimodal intelligence with best-in-class performance on MMMU-Pro, Video-MMMU, document QA, and spatial reasoning. It handles complex visual instructions, 3D understanding, time-dependent video analysis, and UI comprehension in a way that is far more fluid than previous versions.\n ... \nOpus 4.5 inherits much of this improved coding stability, especially in long-context architectures, security reasoning, and systematic refactoring. Gemini 3, especially in Google Antigravity, excels at **agentic coding** , enabling multiple agents to operate simultaneously across editors, terminals, and browser contexts. It also leads the WebDev Arena leaderboard with 1487 Elo and performs exceptionally well in Terminal-Bench 2.0, making it strong for full-stack interactive development. ## **Which model is better for creative tasks, planning, and** **UI** **generation? **\n\nGemini 3 is the stronger model for [vivid creative ideation,](https://www.glbgpt.com/hub/chatgpt-vs-gemini-3-pro-for-blog-writing/) 3D visualization, UI layout coding, and interactive content generation. Its “vibe coding” paradigm allows a single prompt to generate fully functional web apps, interactive tutorials, or immersive 3D experiences.\nClaude Opus 4.5 produces polished writing, high-consistency story structures, and detailed professional documents. It is less focused on visual creativity but excels at producing coherent, logically consistent content over very long documents. ## **Pricing Comparison: Claude Opus 4.5 vs Gemini 3**\n\n### Key Takeaways\n\n**Claude Opus 4.5** has the highest per-token cost, reflecting its focus on deep reasoning and long-context planning. **Gemini 3 Pro** offers significantly lower pricing with strong multimodal and UI-generation capabilities. **GlobalGPT** removes per-token billing entirely—its ~$5.75 Basic plan gives access to 100+ models, offering the best value for users who switch between multiple AI systems. ### **Which model is more cost-efficient? **\n\nGemini 3 is generally more cost-effective for multimodal, creative, or video-rich tasks, while Claude Opus 4.5 becomes more efficient for deep reasoning tasks where output size is smaller relative to the complexity of the reasoning."
    ],
    "rank": 3,
    "title": "Claude Opus 4.5 vs Gemini 3: Which AI Model Is Better in ...",
    "url": "https://www.glbgpt.com/kr/hub/claude-opus-4-5-vs-gemini-3/"
  },
  {
    "domain": "blog.google",
    "excerpts": [
      "[Google DeepMind](https://blog.google/technology/google-deepmind/)\n\n# Gemini 2.5: Our most intelligent AI model\n\nMar 25, 2025\n\n·\n\nShare\n\n[Twitter](https://twitter.com/intent/tweet?text=Gemini%202.5%3A%20Our%20most%20intelligent%20AI%20model%20%40google&url=https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) [Facebook](https://www.facebook.com/sharer/sharer.php?caption=Gemini%202.5%3A%20Our%20most%20intelligent%20AI%20model&u=https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) [LinkedIn](https://www.linkedin.com/shareArticle?mini=true&url=https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/&title=Gemini%202.5%3A%20Our%20most%20intelligent%20AI%20model) [Mail](mailto:?subject=Gemini%202.5%3A%20Our%20most%20intelligent%20AI%20model&body=Check out this article on the Keyword:%0A%0AGemini%202.5%3A%20Our%20most%20intelligent%20AI%20model%0A%0AGemini 2.5 is our most intelligent AI\nmodel, now with thinking.%0A%0Ahttps://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\nCopy link\n\nGemini 2.5 is a thinking model, designed to tackle increasingly complex problems. Our first 2.5 model, Gemini 2.5 Pro Experimental, leads common benchmarks by meaningful margins and showcases strong reasoning and code capabilities.\n ... \nthinking.%0A%0Ahttps://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\nCopy link\n\nIn this story\n\nIn this story\n\n* * *\n\n* [Introducing Gemini 2.5]()\n* [Gemini 2.5 Pro]()\n* [Enhanced reasoning]()\n* [Advanced coding]()\n* [The best of Gemini]()\n\n_Last updated March 26_\n\nToday we’re introducing Gemini 2.5, our most intelligent AI model. Our first 2.5 release is an experimental version of 2.5 Pro, which is state-of-the-art on a wide range of benchmarks and debuts at #1 on [LMArena](https://lmarena.ai/?leaderboard) by a significant margin. [Gemini 2.5 models](https://deepmind.google/technologies/gemini) are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. In the field of AI, a system’s capacity for “reasoning” refers to more than just classification and prediction.\n ... \nIt tops the [LMArena](https://lmarena.ai/?leaderboard) leaderboard — which measures human preferences — by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks. Gemini 2.5 Pro is available now in [Google AI Studio](http://aistudio.google.com/app/prompts/new_chat?model=gemini-2.5-pro-exp-03-25) and in the [Gemini app](https://gemini.google.com/) for Gemini Advanced users, and will be coming to [Vertex AI](https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio) soon. We’ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use. Updated March 26 with new MRCR (Multi Round Coreference Resolution) evaluations\n\n## Enhanced reasoning\n\nGemini 2.5 Pro is state-of-the-art across a range of benchmarks requiring advanced reasoning.\nWithout test-time techniques that increase cost, like majority voting, 2.5 Pro leads in math and science benchmarks like GPQA and AIME 2025. It also scores a state-of-the-art 18.8% across models without tool use on Humanity’s Last Exam, a dataset designed by hundreds of subject matter experts to capture the human frontier of knowledge and reasoning. ## Advanced coding\n\nWe’ve been focused on coding performance, and with Gemini 2.5 we’ve achieved a big leap over 2.0 — with more improvements to come. 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing. On SWE-Bench Verified, the industry standard for agentic code evals, Gemini 2.5 Pro scores 63.8% with a custom agent setup. Here’s an example of how 2.5 Pro can use its reasoning capabilities to create a video game by producing the executable code from a single line prompt."
    ],
    "rank": 4,
    "title": "Gemini 2.5: Our most intelligent AI model - Google Blog",
    "url": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"
  },
  {
    "domain": "thenewstack.io",
    "excerpts": [
      "Dunlop](/adam-jacob-on-why-scaling-is-the-funnest-game/)\n\n[Data Locality vs. Independence: Which Should Your Database Prioritize?\n ... \nPython](/build-your-first-http-server-in-python/)\n\nNov 4th 2025 3:00pm, by Jessica Wachtel\n\n[The Rise of JavaScript in Machine Learning](/the-rise-of-javascript-in-machine-learning/)\n\nOct 23rd 2025 9:03am, by Loraine Lawson\n\n[SQL vs. Python: Frenemies of the Data World](/sql-vs-python-frenemies-of-the-data-world/)\n\nOct 14th 2025 7:00am, by Ivan Novick\n\n[Debian Mandates Rust for APT, Reshaping Ubuntu and Other Linux Distros](/debian-mandates-rust-for-apt-reshaping-ubuntu-and-other-linux-distros/)\n\nNov 11th 2025 12:00pm, by Steven J. Vaughan-Nichols\n\n[Moving From C++ to Rust?\n ... \nCassel\n\n[Go Power: Microsoft's Bold Bet on Faster TypeScript Tools](/go-power-microsofts-bold-bet-on-faster-typescript-tools/)\n\nMar 12th 2025 1:00pm, by Darryl K. Taft and Loraine Lawson\n\n[Oracle Won’t Release ‘JavaScript’ Without a Fight](/oracle-wont-release-javascript-without-a-fight/)\n\nJan 11th 2025 5:00am, by Loraine Lawson\n\n[The Year in JavaScript: Top JS News Stories of 2024](/the-year-in-javascript-top-js-news-stories-of-2024/)\n\nDec 27th 2024 6:30am, by Loraine Lawson\n\n2025-11-24 11:00:23\n\nAnthropic's New Claude Opus 4.5 Reclaims the Coding Crown\n\n[AI](/category/ai/) / [AI Agents](/category/ai-agents/)\n\n# Anthropic’s New Claude Opus 4.5 Reclaims the Coding Crown\n\nIn addition to the new model, Anthropic is also announcing two updates to the Claude Developer Platform that go hand-in-hand with the Opus 4.5 release. Nov 24th, 2025 11:00am by [Frederic Lardinois](https://thenewstack.io/author/frederic-lardinois/ \"Posts by Frederic Lardinois\")\n\nFeatured image credit: Anthropic.\nAnthropic today launched the latest version of its flagship Opus model: Opus 4.5. The company calls it its most intelligent model yet and notes that it is especially strong in solving coding tasks, taking the crown from OpenAI’s [GPT-5.1-Codex-Max](https://thenewstack.io/openai-says-its-new-codex-max-model-is-better-faster-and-cheaper/) and Google’s week-old [Gemini 3](https://thenewstack.io/google-launches-gemini-3-pro/) model with an SWE-Bench Verified accuracy score of 80.9%. The company is also making Opus 4.5 significantly more affordable to use, with API pricing of $5 per million input tokens and $25 per million output tokens, down from $15/$75 per million input/output tokens. Users on Anthropic’s subscription plans will also now see a bit more headroom to use Opus 4.5.\n## Benchmarks\n\nWith the launches of OpenAI’s GPT-5.1 and 5.1-Codex-Max, Google’s [Gemini 3](https://thenewstack.io/google-launches-gemini-3-pro/) (and its hit Nano Banana Pro image model), it’s been a very active November for the large model builders. Gemini 3, especially, received a very positive reception. Unlike Google, Anthropic has never focused on image manipulation or video creation, but has stuck squarely to its strength in coding and productivity use cases. This latest Opus is no different and Anthropic stresses that the model can now produce documents, spreadsheets and presentations “with consistency, professional polish, and domain awareness.”\n\nBut as usual, it’s coding where the Claude models shine. That’s reflected on the benchmarks, where Opus 4.5 bests the competition across the board, but benchmarks don’t always reflect real-world use cases, of course. Credit: Anthropic.\n ... \nI still am reviewing it and things, but I really could have just been hands-off here.”\n\n## Low, Medium, High Effort\n\nOne new feature of Opus 4.5 is that it features an “effort” parameter (low, medium, high), similar to some of its competitors’ models, which allows developers to control how much time (and how many tokens) the model will use to solve a given problem. Set to medium, the model is on par with Sonnet 4.5 on the SWE-bench Verified benchmark but uses 76% fewer tokens, and even at the high setting, where it beats Sonnet 4.5, it uses only about half the tokens of the Sonnet model. That’s a trend we’ve been seeing and this efficiency is something OpenAI also stressed when it launched its latest [Codex-Max](https://thenewstack.io/openai-says-its-new-codex-max-model-is-better-faster-and-cheaper/) model last week. Overall, the model also improved upon the rest of the Opus family (and Opus 4.1) in other areas, including visual reasoning and math. Credit: Anthropic."
    ],
    "rank": 5,
    "title": "Anthropic's New Claude Opus 4.5 Reclaims the Coding ...",
    "url": "https://thenewstack.io/anthropics-new-claude-opus-4-5-reclaims-the-coding-crown-from-gemini-3/"
  },
  {
    "domain": "medium.com",
    "excerpts": [
      "-e3887df3ed04&source=---header_actions--e3887df3ed04---------------------post_audio_button------------------)\n\nShare\n\nPress enter or click to view image in full size\n\nAnthropic just released the Claude Opus 4.5 model, and the timing is perfect, _just days after the Gemini 3 Pro release._\n\n> **We’re in the middle of what feels like an AI arms race. Every week brings a new model, a new capability, a new benchmark shattered. **\n> \n> \n\nAnthropic claims this is the best coding model in the world. They say it scored higher on their internal engineering exam than any human candidate ever has. > They’re talking about a model that “just gets it” without hand-holding. Those are bold claims in a market crowded with frontier models. > \n> \n\n> **So I took it for a quick test to see if it lives up to this claim, and as you are going to see in the last testing section, it surprised me."
    ],
    "rank": 6,
    "title": "Claude Opus 4.5 Is Here (And Beats Gemini 3 Pro SWE by ...",
    "url": "https://medium.com/ai-software-engineer/claude-opus-4-5-is-here-and-beats-gemini-3-pro-swe-by-4-7-i-tested-it-e3887df3ed04"
  },
  {
    "domain": "natesnewsletter.substack.com",
    "excerpts": [
      "[](/)\n\n# [](/)\n\nSubscribe Sign in\n\nPlayback speed\n\n×\n\nShare post\n\nShare post at current time\n\nShare from 0:00\n\n0:00\n\n/\n\n0:00\n\nPlayback speed\n\n×\n\nShare post\n\n0:00\n\n/\n\n0:00\n\nPreview\n\n7\n\n1\n\n## I Tested Opus 4.5 Early—Here's Where It Can Save You HOURS on Complex Workflows + a Comparison vs. Gemini 3 and ChatGPT 5.1 + a Model-Picker Prompt + 15 Workflows to Get Started Now\n\nI tested Opus 4.5 vs. Gemini 3 vs. ChatGPT 5.1 on real-world business tasks: here's what I found, plus a complete breakdown of which model I'd use for complex workflows plus a custom model-picker! [](https://substack.com/@natesnewsletter)\n\n[Nate](https://substack.com/@natesnewsletter)\n\nNov 25, 2025\n\n∙ Paid\n\n7\n\n1\n\nShare\n\nI’ll cut to it: Opus 4.5 IS a big deal. Even during a week that has felt like one endless model release after another. But I’m not here to tell you this is a big deal because of benchmarks."
    ],
    "rank": 7,
    "title": "I tested Opus 4.5 vs. Gemini 3 vs. ChatGPT 5.1 on real- ...",
    "url": "https://natesnewsletter.substack.com/p/claude-opus-45-loves-messy-real-world"
  },
  {
    "domain": "www.anthropic.com",
    "excerpts": [
      "We’re updating usage limits to make sure you’re able to use Opus 4.5 for daily work. These limits are specific to Opus 4.5. As future models surpass it, we expect to update limits as needed. #### Footnotes\n\n_1: This result was using parallel test-time compute, a method that aggregates multiple “tries” from the model and selects from among them. Without a time limit, the model (used within Claude Code) matched the best-ever human candidate._\n\n_2: We improved the hosting environment to reduce infrastructure failures. This change improved Gemini 3 to 56.7% and GPT-5.1 to 48.6% from the values reported by their developers, using the Terminus-2 harness._\n\n_3: Note that these evaluations were run on an in-progress upgrade to [Petri](https://www.anthropic.com/research/petri-open-source-auditing) , our open-source, automated evaluation tool. They were run on an earlier snapshot of Claude Opus 4.5."
    ],
    "rank": 8,
    "title": "Introducing Claude Opus 4.5",
    "url": "https://www.anthropic.com/news/claude-opus-4-5"
  },
  {
    "domain": "www.glbgpt.com",
    "excerpts": [
      "**\n\nClaude Opus 4.5 pushes Anthropic’s reasoning capabilities forward with extended thinking, more stable chain-of-thought execution, and highly reliable tool use. It excels in tasks requiring multi-step logic, structured decomposition, and precise decision-making across long agent workflows. In official benchmarks, Opus 4.5 shows significant jumps in complex problem-solving and coding reasoning compared to Opus 4.1. Gemini 3, however, achieves frontier-level performance in conceptual reasoning through its Deep Think mode and consistently leads on academic-style benchmarks like Humanity’s Last Exam, ARC-AGI-2, and GPQA. It also displays stronger intuition with abstract patterns and high-level conceptual interpretation, especially in science and mathematics. ## **How do Claude** **Opus** **4\\.5 and Gemini 3 compare in multimodal understanding?"
    ],
    "rank": 9,
    "title": "Claude Opus 4.5 vs Gemini 3: Which AI Model Is Better in ...",
    "url": "https://www.glbgpt.com/hub/claude-opus-4-5-vs-gemini-3/"
  },
  {
    "domain": "www.thealgorithmicbridge.com",
    "excerpts": [
      "Gemini 3 is great, much better than the alternatives—including [GPT-5.1](https://openai.com/index/gpt-5-1/) (recently released by OpenAI) and [Claude Sonnet 4.5](https://www.anthropic.com/news/claude-sonnet-4-5) (from Anthropic)—but I wouldn’t update much on benchmark scores (they’re mostly noise!). However, there’s one achievement that stands out to me as not only impressive but _genuinely surprising_ . But before I go into that, let’s do a quick review of just how good Gemini 3 is compared to the competition. Google [says](https://x.com/GoogleDeepMind/status/1990812966074376261) Gemini 3 has “state-of-the-art reasoning capabilities, world-leading multimodal understanding, and enables new agentic coding experiences,” but when every new model from a frontier AI company is accompanied by the same kind of description, I believe the differences are better understood with images (and, of course, firsthand experience with the models)."
    ],
    "rank": 10,
    "title": "Google Gemini 3 Is the Best Model Ever. One Score Stands Out ...",
    "url": "https://www.thealgorithmicbridge.com/p/google-gemini-3-just-killed-every"
  },
  {
    "domain": "www.reddit.com",
    "excerpts": [
      "[Skip to main content]() Open menu Open navigation [](/) Go to Reddit Home\n\nr/singularity A chip A close button\n\n[Log In](https://www.reddit.com/login/) Log in to Reddit\n\nExpand user menu Open settings menu\n\n[Go to singularity](/r/singularity/) [r/singularity](/r/singularity/) •\n\n[zero0\\_one1](/user/zero0_one1/)\n\n# Claude Opus 4.5 Thinking 16K scores 63.8 on the Extended NYT Connections benchmark (Opus 4.1 Thinking 16K: 58.8, Sonnet 4.5 Thinking: 48.2). * \n*\n\n<https://github.com/lechmazur/nyt-connections/>\n\nBy far the best non-reasoning model, but reasoning adds little."
    ],
    "rank": 11,
    "title": "Claude Opus 4.5 Thinking 16K scores 63.8 on the ...",
    "url": "https://www.reddit.com/r/singularity/comments/1p683x1/claude_opus_45_thinking_16k_scores_638_on_the/"
  },
  {
    "domain": "ai.google.dev",
    "excerpts": [
      "patterns](/gemini-api/docs/models/gemini) for more details. * `Preview: gemini-3-pro-image-preview` |\n|calendar\\_month Latest update |November 2025 |\n|cognition\\_2 Knowledge cutoff |January 2025 |\n\nOUR ADVANCED THINKING MODEL\n\n## Gemini 2.5 Pro\n\nOur state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.\n ... \ndetails. * `gemini-2.5-pro-preview-tts` |\n|calendar\\_month Latest update |May 2025 |\n\nFAST AND INTELLIGENT\n\n## Gemini 2.5 Flash\n\nOur best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases."
    ],
    "rank": 12,
    "title": "Gemini models | Gemini API | Google AI for Developers",
    "url": "https://ai.google.dev/gemini-api/docs/models"
  },
  {
    "domain": "blog.getbind.co",
    "excerpts": [
      "12 hours ago — Claude Opus 4.5 writes better code, leading across 7 out of 8 ... While Anthropic's Opus 4.5 outperforms it in raw coding benchmarks ..."
    ],
    "rank": 13,
    "title": "Claude Opus 4.5 vs Gemini 3.0 Pro vs GPT-5.1",
    "url": "https://blog.getbind.co/2025/11/26/claude-opus-4-5-vs-gemini-3-0-pro-vs-gpt-5-1-which-is-best-for-coding/"
  },
  {
    "domain": "www.datastudios.org",
    "excerpts": [
      "Both models perform exceptionally well here, each hovering around roughly **90% accuracy** . Essentially, they both demonstrate mastery across disciplines, turning in performances that would correspond to top-percentile human test-takers. Neither has a decisive edge – they’re effectively tied on MMLU, indicating that in broad knowledge and reasoning, they are equally strong. * **Advanced Reasoning Tests:** On newer, extremely difficult evaluations (for instance, _“Humanity’s Last Exam”_ or ARC-Advanced challenges, which are designed to stump AI with counterintuitive logic or require creative problem solving), **the results vary by task** . Generally, **Google’s Gemini 3 Pro leads on many of these** , with ChatGPT 5.1 often coming second and Claude 4.5 close behind. For example, on a PhD-level scientific QA benchmark ( _GPQA Diamond_ ), Gemini might score in the low 90s, ChatGPT just a hair behind that, and Claude in the high 80s."
    ],
    "rank": 14,
    "title": "Claude Opus 4.5 vs. ChatGPT 5.1 - Data Studios",
    "url": "https://www.datastudios.org/post/claude-opus-4-5-vs-chatgpt-5-1-full-report-and-comparison-of-models-features-performance-pricin"
  },
  {
    "domain": "deepmind.google",
    "excerpts": [
      "Slide 1 of 3\n\nchevron\\_left chevron\\_right\n\n|Benchmark |Notes |Gemini 3 Pro |Gemini 2.5 Pro |Claude Sonnet 4.5 |GPT-5.1 |\n| --- | --- | --- | --- | --- | --- |\n|Academic reasoning Humanity's Last Exam |No tools |37\\.5% |21\\.6% |13\\.7% |26\\.5% |\n|With search and code execution |45\\.8% |— |— |— |\n|Visual reasoning puzzles ARC-AGI-2 |ARC Prize Verified |31\\.1% |4\\.9% |13\\.6% |17\\.6% |\n|Scientific knowledge GPQA Diamond |No tools |91\\.9% |86\\.4% |83\\.4% |88\\.1% |\n|Mathematics AIME 2025 |No tools |95\\.0% |88\\.0% |87\\.0% |94\\.0% |\n|With code execution |100\\.0% |— |100\\.0% |— |\n|Challenging Math Contest problems MathArena Apex | |23\\.4% |0\\.5% |1\\.6% |1\\.0% |\n|Multimodal understanding and reasoning MMMU-Pro | |81\\.0% |68\\.0% |68\\.0% |76\\.0% |\n|Screen understanding ScreenSpot-Pro | |72\\.7% |11\\.4% |36\\.2% |3\\.5% |\n|Information synthesis from complex charts CharXiv Reasoning | |81\\.4% |69\\.6% |68\\.5% |69\\.5% |\n|OCR OmniDocBench 1.5 |Overall Edit Distance, lower is better |0\\.115 |0\\.145"
    ],
    "rank": 15,
    "title": "Gemini 3 Pro - Google DeepMind",
    "url": "https://deepmind.google/models/gemini/pro/"
  }
]